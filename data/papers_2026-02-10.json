[
  {
    "title": "DynFOA: Generating First-Order Ambisonics with Conditional Diffusion for Dynamic and Acoustically Complex 360-Degree Videos",
    "authors": [
      "Ziyu Luo",
      "Lin Chen",
      "Qiang Qu",
      "Xiaoming Chen",
      "Yiran Shen"
    ],
    "abstract": "Spatial audio is crucial for creating compelling immersive 360-degree video experiences. However, generating realistic spatial audio, such as first-order ambisonics (FOA), from 360-degree videos in complex acoustic scenes remains challenging. Existing methods often overlook the dynamic nature and acoustic complexity of 360-degree scenes, fail to fully account for dynamic sound sources, and neglect complex environmental effects such as occlusion, reflections, and reverberation, which are influenced by scene geometries and materials. We propose DynFOA, a framework based on dynamic acoustic perception and conditional diffusion, for generating high-fidelity FOA from 360-degree videos. DynFOA first performs visual processing via a video encoder, which detects and localizes multiple dynamic sound sources, estimates their depth and semantics, and reconstructs the scene geometry and materials using a 3D Gaussian Splatting. This reconstruction technique accurately models occlusion, reflections, and reverberation based on the geometries and materials of the reconstructed 3D scene and the listener's viewpoint. The audio encoder then captures the spatial motion and temporal 4D sound source trajectories to fine-tune the diffusion-based FOA generator. The fine-tuned FOA generator adjusts spatial cues in real time, ensuring consistent directional fidelity during listener head rotation and complex environmental changes. Extensive evaluations demonstrate that DynFOA consistently outperforms existing methods across metrics such as spatial accuracy, acoustic fidelity, and distribution matching, while also improving the user experience. Therefore, DynFOA provides a robust and scalable approach to rendering realistic dynamic spatial audio for VR and immersive media applications.",
    "arxiv_url": "https://arxiv.org/abs/2602.06846v1",
    "pdf_url": "https://arxiv.org/pdf/2602.06846v1",
    "published_date": "2026-02-06",
    "categories": [
      "cs.SD"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "head",
      "dynamic",
      "vr",
      "high-fidelity",
      "motion",
      "geometry",
      "ar",
      "semantic",
      "reflection",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.06846v1",
      "pdf": "https://arxiv.org/pdf/2602.06846v1"
    },
    "bibtex": ""
  },
  {
    "title": "GaussianPOP: Principled Simplification Framework for Compact 3D Gaussian Splatting via Error Quantification",
    "authors": [
      "Soonbin Lee",
      "Yeong-Gyu Kim",
      "Simon Sasse",
      "Tomas M. Borges",
      "Yago Sanchez",
      "Eun-Seok Ryu",
      "Thomas Schierl",
      "Cornelius Hellge"
    ],
    "abstract": "Existing 3D Gaussian Splatting simplification methods commonly use importance scores, such as blending weights or sensitivity, to identify redundant Gaussians. However, these scores are not driven by visual error metrics, often leading to suboptimal trade-offs between compactness and rendering fidelity. We present GaussianPOP, a principled simplification framework based on analytical Gaussian error quantification. Our key contribution is a novel error criterion, derived directly from the 3DGS rendering equation, that precisely measures each Gaussian's contribution to the rendered image. By introducing a highly efficient algorithm, our framework enables practical error calculation in a single forward pass. The framework is both accurate and flexible, supporting on-training pruning as well as post-training simplification via iterative error re-quantification for improved stability. Experimental results show that our method consistently outperforms existing state-of-the-art pruning methods across both application scenarios, achieving a superior trade-off between model compactness and high rendering quality.",
    "arxiv_url": "https://arxiv.org/abs/2602.06830v1",
    "pdf_url": "https://arxiv.org/pdf/2602.06830v1",
    "published_date": "2026-02-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.06830v1",
      "pdf": "https://arxiv.org/pdf/2602.06830v1"
    },
    "bibtex": ""
  },
  {
    "title": "TFusionOcc: Student's t-Distribution Based Object-Centric Multi-Sensor Fusion Framework for 3D Occupancy Prediction",
    "authors": [
      "Zhenxing Ming",
      "Julie Stephany Berrio",
      "Mao Shan",
      "Stewart Worrall"
    ],
    "abstract": "3D semantic occupancy prediction enables autonomous vehicles (AVs) to perceive fine-grained geometric and semantic structure of their surroundings from onboard sensors, which is essential for safe decision-making and navigation. Recent models for 3D semantic occupancy prediction have successfully addressed the challenge of describing real-world objects with varied shapes and classes. However, the intermediate representations used by existing methods for 3D semantic occupancy prediction rely heavily on 3D voxel volumes or a set of 3D Gaussians, hindering the model's ability to efficiently and effectively capture fine-grained geometric details in the 3D driving environment. This paper introduces TFusionOcc, a novel object-centric multi-sensor fusion framework for predicting 3D semantic occupancy. By leveraging multi-stage multi-sensor fusion, Student's t-distribution, and the T-Mixture model (TMM), together with more geometrically flexible primitives, such as the deformable superquadric (superquadric with inverse warp), the proposed method achieved state-of-the-art (SOTA) performance on the nuScenes benchmark. In addition, extensive experiments were conducted on the nuScenes-C dataset to demonstrate the robustness of the proposed method in different camera and lidar corruption scenarios. The code will be available at: https://github.com/DanielMing123/TFusionOcc",
    "arxiv_url": "https://arxiv.org/abs/2602.06400v1",
    "pdf_url": "https://arxiv.org/pdf/2602.06400v1",
    "published_date": "2026-02-06",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "https://github.com/DanielMing123/TFusionOcc",
    "keywords": [
      "3d gaussian",
      "efficient",
      "semantic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.06400v1",
      "pdf": "https://arxiv.org/pdf/2602.06400v1",
      "github": "https://github.com/DanielMing123/TFusionOcc"
    },
    "bibtex": ""
  },
  {
    "title": "Uncertainty-Aware 4D Gaussian Splatting for Monocular Occluded Human Rendering",
    "authors": [
      "Weiquan Wang",
      "Feifei Shao",
      "Lin Li",
      "Zhen Wang",
      "Jun Xiao",
      "Long Chen"
    ],
    "abstract": "High-fidelity rendering of dynamic humans from monocular videos typically degrades catastrophically under occlusions. Existing solutions incorporate external priors-either hallucinating missing content via generative models, which induces severe temporal flickering, or imposing rigid geometric heuristics that fail to capture diverse appearances. To this end, we reformulate the task as a Maximum A Posteriori estimation problem under heteroscedastic observation noise. In this paper, we propose U-4DGS, a framework integrating a Probabilistic Deformation Network and a Double Rasterization pipeline. This architecture renders pixel-aligned uncertainty maps that act as an adaptive gradient modulator, automatically attenuating artifacts from unreliable observations. Furthermore, to prevent geometric drift in regions lacking reliable visual cues, we enforce Confidence-Aware Regularizations, which leverage the learned uncertainty to selectively propagate spatial-temporal validity. Extensive experiments on ZJU-MoCap and OcMotion demonstrate that U-4DGS achieves SOTA rendering fidelity and robustness.",
    "arxiv_url": "https://arxiv.org/abs/2602.06343v1",
    "pdf_url": "https://arxiv.org/pdf/2602.06343v1",
    "published_date": "2026-02-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "high-fidelity",
      "deformation",
      "gaussian splatting",
      "ar",
      "human",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.06343v1",
      "pdf": "https://arxiv.org/pdf/2602.06343v1"
    },
    "bibtex": ""
  },
  {
    "title": "From Blurry to Believable: Enhancing Low-quality Talking Heads with 3D Generative Priors",
    "authors": [
      "Ding-Jiun Huang",
      "Yuanhao Wang",
      "Shao-Ji Yuan",
      "Albert Mosella-Montoro",
      "Francisco Vicente Carrasco",
      "Cheng Zhang",
      "Fernando De la Torre"
    ],
    "abstract": "Creating high-fidelity, animatable 3D talking heads is crucial for immersive applications, yet often hindered by the prevalence of low-quality image or video sources, which yield poor 3D reconstructions. In this paper, we introduce SuperHead, a novel framework for enhancing low-resolution, animatable 3D head avatars. The core challenge lies in synthesizing high-quality geometry and textures, while ensuring both 3D and temporal consistency during animation and preserving subject identity. Despite recent progress in image, video and 3D-based super-resolution (SR), existing SR techniques are ill-equipped to handle dynamic 3D inputs. To address this, SuperHead leverages the rich priors from pre-trained 3D generative models via a novel dynamics-aware 3D inversion scheme. This process optimizes the latent representation of the generative model to produce a super-resolved 3D Gaussian Splatting (3DGS) head model, which is subsequently rigged to an underlying parametric head model (e.g., FLAME) for animation. The inversion is jointly supervised using a sparse collection of upscaled 2D face renderings and corresponding depth maps, captured from diverse facial expressions and camera viewpoints, to ensure realism under dynamic facial motions. Experiments demonstrate that SuperHead generates avatars with fine-grained facial details under dynamic motions, significantly outperforming baseline methods in visual quality.",
    "arxiv_url": "https://arxiv.org/abs/2602.06122v1",
    "pdf_url": "https://arxiv.org/pdf/2602.06122v1",
    "published_date": "2026-02-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "dynamic",
      "high-fidelity",
      "face",
      "animation",
      "motion",
      "3d reconstruction",
      "geometry",
      "avatar",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.06122v1",
      "pdf": "https://arxiv.org/pdf/2602.06122v1"
    },
    "bibtex": ""
  },
  {
    "title": "Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation",
    "authors": [
      "David Shavin",
      "Sagie Benaim"
    ],
    "abstract": "Vision Foundation Models (VFMs) have achieved remarkable success when applied to various downstream 2D tasks. Despite their effectiveness, they often exhibit a critical lack of 3D awareness. To this end, we introduce Splat and Distill, a framework that instills robust 3D awareness into 2D VFMs by augmenting the teacher model with a fast, feed-forward 3D reconstruction pipeline. Given 2D features produced by a teacher model, our method first lifts these features into an explicit 3D Gaussian representation, in a feedforward manner. These 3D features are then ``splatted\" onto novel viewpoints, producing a set of novel 2D feature maps used to supervise the student model, ``distilling\" geometrically grounded knowledge. By replacing slow per-scene optimization of prior work with our feed-forward lifting approach, our framework avoids feature-averaging artifacts, creating a dynamic learning process where the teacher's consistency improves alongside that of the student. We conduct a comprehensive evaluation on a suite of downstream tasks, including monocular depth estimation, surface normal estimation, multi-view correspondence, and semantic segmentation. Our method significantly outperforms prior works, not only achieving substantial gains in 3D awareness but also enhancing the underlying semantic richness of 2D features. Project page is available at https://davidshavin4.github.io/Splat-and-Distill/",
    "arxiv_url": "https://arxiv.org/abs/2602.06032v1",
    "pdf_url": "https://arxiv.org/pdf/2602.06032v1",
    "published_date": "2026-02-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "face",
      "3d reconstruction",
      "segmentation",
      "ar",
      "semantic",
      "3d gaussian",
      "fast"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.06032v1",
      "pdf": "https://arxiv.org/pdf/2602.06032v1",
      "project": "https://davidshavin4.github.io/Splat-and-Distill"
    },
    "bibtex": ""
  },
  {
    "title": "NVS-HO: A Benchmark for Novel View Synthesis of Handheld Objects",
    "authors": [
      "Musawar Ali",
      "Manuel Carranza-García",
      "Nicola Fioraio",
      "Samuele Salti",
      "Luigi Di Stefano"
    ],
    "abstract": "We propose NVS-HO, the first benchmark designed for novel view synthesis of handheld objects in real-world environments using only RGB inputs. Each object is recorded in two complementary RGB sequences: (1) a handheld sequence, where the object is manipulated in front of a static camera, and (2) a board sequence, where the object is fixed on a ChArUco board to provide accurate camera poses via marker detection. The goal of NVS-HO is to learn a NVS model that captures the full appearance of an object from (1), whereas (2) provides the ground-truth images used for evaluation. To establish baselines, we consider both a classical SfM pipeline and a state-of-the-art pre-trained feed-forward neural network (VGGT) as pose estimators, and train NVS models based on NeRF and Gaussian Splatting. Our experiments reveal significant performance gaps in current methods under unconstrained handheld conditions, highlighting the need for more robust approaches. NVS-HO thus offers a challenging real-world benchmark to drive progress in RGB-based novel view synthesis of handheld objects.",
    "arxiv_url": "https://arxiv.org/abs/2602.05822v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05822v1",
    "published_date": "2026-02-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lighting",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.05822v1",
      "pdf": "https://arxiv.org/pdf/2602.05822v1"
    },
    "bibtex": ""
  },
  {
    "title": "Unified Sensor Simulation for Autonomous Driving",
    "authors": [
      "Nikolay Patakin",
      "Arsenii Shirokov",
      "Anton Konushin",
      "Dmitry Senushkin"
    ],
    "abstract": "In this work, we introduce \\textbf{XSIM}, a sensor simulation framework for autonomous driving. XSIM extends 3DGUT splatting with a generalized rolling-shutter modeling tailored for autonomous driving applications. Our framework provides a unified and flexible formulation for appearance and geometric sensor modeling, enabling rendering of complex sensor distortions in dynamic environments. We identify spherical cameras, such as LiDARs, as a critical edge case for existing 3DGUT splatting due to cyclic projection and time discontinuities at azimuth boundaries leading to incorrect particle projection. To address this issue, we propose a phase modeling mechanism that explicitly accounts temporal and shape discontinuities of Gaussians projected by the Unscented Transform at azimuth borders. In addition, we introduce an extended 3D Gaussian representation that incorporates two distinct opacity parameters to resolve mismatches between geometry and color distributions. As a result, our framework provides enhanced scene representations with improved geometric consistency and photorealistic appearance. We evaluate our framework extensively on multiple autonomous driving datasets, including Waymo Open Dataset, Argoverse 2, and PandaSet. Our framework consistently outperforms strong recent baselines and achieves state-of-the-art performance across all datasets. The source code is publicly available at \\href{https://github.com/whesense/XSIM}{https://github.com/whesense/XSIM}.",
    "arxiv_url": "https://arxiv.org/abs/2602.05617v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05617v1",
    "published_date": "2026-02-05",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "https://github.com/whesense/XSIM",
    "keywords": [
      "dynamic",
      "geometry",
      "autonomous driving",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.05617v1",
      "pdf": "https://arxiv.org/pdf/2602.05617v1",
      "github": "https://github.com/whesense/XSIM"
    },
    "bibtex": ""
  },
  {
    "title": "PoseGaussian: Pose-Driven Novel View Synthesis for Robust 3D Human Reconstruction",
    "authors": [
      "Ju Shen",
      "Chen Chen",
      "Tam V. Nguyen",
      "Vijayan K. Asari"
    ],
    "abstract": "We propose PoseGaussian, a pose-guided Gaussian Splatting framework for high-fidelity human novel view synthesis. Human body pose serves a dual purpose in our design: as a structural prior, it is fused with a color encoder to refine depth estimation; as a temporal cue, it is processed by a dedicated pose encoder to enhance temporal consistency across frames. These components are integrated into a fully differentiable, end-to-end trainable pipeline. Unlike prior works that use pose only as a condition or for warping, PoseGaussian embeds pose signals into both geometric and temporal stages to improve robustness and generalization. It is specifically designed to address challenges inherent in dynamic human scenes, such as articulated motion and severe self-occlusion. Notably, our framework achieves real-time rendering at 100 FPS, maintaining the efficiency of standard Gaussian Splatting pipelines. We validate our approach on ZJU-MoCap, THuman2.0, and in-house datasets, demonstrating state-of-the-art performance in perceptual quality and structural accuracy (PSNR 30.86, SSIM 0.979, LPIPS 0.028).",
    "arxiv_url": "https://arxiv.org/abs/2602.05190v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05190v1",
    "published_date": "2026-02-05",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "high-fidelity",
      "body",
      "real-time rendering",
      "gaussian splatting",
      "ar",
      "human",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.05190v1",
      "pdf": "https://arxiv.org/pdf/2602.05190v1"
    },
    "bibtex": ""
  },
  {
    "title": "QuantumGS: Quantum Encoding Framework for Gaussian Splatting",
    "authors": [
      "Grzegorz Wilczyński",
      "Rafał Tobiasz",
      "Paweł Gora",
      "Marcin Mazur",
      "Przemysław Spurek"
    ],
    "abstract": "Recent advances in neural rendering, particularly 3D Gaussian Splatting (3DGS), have enabled real-time rendering of complex scenes. However, standard 3DGS relies on spherical harmonics, which often struggle to accurately capture high-frequency view-dependent effects such as sharp reflections and transparency. While hybrid approaches like Viewing Direction Gaussian Splatting (VDGS) mitigate this limitation using classical Multi-Layer Perceptrons (MLPs), they remain limited by the expressivity of classical networks in low-parameter regimes. In this paper, we introduce QuantumGS, a novel hybrid framework that integrates Variational Quantum Circuits (VQC) into the Gaussian Splatting pipeline. We propose a unique encoding strategy that maps the viewing direction directly onto the Bloch sphere, leveraging the natural geometry of qubits to represent 3D directional data. By replacing classical color-modulating networks with quantum circuits generated via a hypernetwork or conditioning mechanism, we achieve higher expressivity and better generalization. Source code is available in the supplementary material. Code is available at https://github.com/gwilczynski95/QuantumGS",
    "arxiv_url": "https://arxiv.org/abs/2602.05047v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05047v1",
    "published_date": "2026-02-04",
    "categories": [
      "quant-ph",
      "cs.CV"
    ],
    "github_url": "https://github.com/gwilczynski95/QuantumGS",
    "keywords": [
      "real-time rendering",
      "neural rendering",
      "geometry",
      "ar",
      "reflection",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.05047v1",
      "pdf": "https://arxiv.org/pdf/2602.05047v1",
      "github": "https://github.com/gwilczynski95/QuantumGS"
    },
    "bibtex": ""
  },
  {
    "title": "Nix and Fix: Targeting 1000x Compression of 3D Gaussian Splatting with Diffusion Models",
    "authors": [
      "Cem Eteke",
      "Enzo Tartaglione"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) revolutionized novel view rendering. Instead of inferring from dense spatial points, as implicit representations do, 3DGS uses sparse Gaussians. This enables real-time performance but increases space requirements, hindering applications such as immersive communication. 3DGS compression emerged as a field aimed at alleviating this issue. While impressive progress has been made, at low rates, compression introduces artifacts that degrade visual quality significantly. We introduce NiFi, a method for extreme 3DGS compression through restoration via artifact-aware, diffusion-based one-step distillation. We show that our method achieves state-of-the-art perceptual quality at extremely low rates, down to 0.1 MB, and towards 1000x rate improvement over 3DGS at comparable perceptual performance. The code will be open-sourced upon acceptance.",
    "arxiv_url": "https://arxiv.org/abs/2602.04549v1",
    "pdf_url": "https://arxiv.org/pdf/2602.04549v1",
    "published_date": "2026-02-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.04549v1",
      "pdf": "https://arxiv.org/pdf/2602.04549v1"
    },
    "bibtex": ""
  },
  {
    "title": "VecSet-Edit: Unleashing Pre-trained LRM for Mesh Editing from Single Image",
    "authors": [
      "Teng-Fang Hsiao",
      "Bo-Kai Ruan",
      "Yu-Lun Liu",
      "Hong-Han Shuai"
    ],
    "abstract": "3D editing has emerged as a critical research area to provide users with flexible control over 3D assets. While current editing approaches predominantly focus on 3D Gaussian Splatting or multi-view images, the direct editing of 3D meshes remains underexplored. Prior attempts, such as VoxHammer, rely on voxel-based representations that suffer from limited resolution and necessitate labor-intensive 3D mask. To address these limitations, we propose \\textbf{VecSet-Edit}, the first pipeline that leverages the high-fidelity VecSet Large Reconstruction Model (LRM) as a backbone for mesh editing. Our approach is grounded on a analysis of the spatial properties in VecSet tokens, revealing that token subsets govern distinct geometric regions. Based on this insight, we introduce Mask-guided Token Seeding and Attention-aligned Token Gating strategies to precisely localize target regions using only 2D image conditions. Also, considering the difference between VecSet diffusion process versus voxel we design a Drift-aware Token Pruning to reject geometric outliers during the denoising process. Finally, our Detail-preserving Texture Baking module ensures that we not only preserve the geometric details of original mesh but also the textural information. More details can be found in our project page: https://github.com/BlueDyee/VecSet-Edit/tree/main",
    "arxiv_url": "https://arxiv.org/abs/2602.04349v1",
    "pdf_url": "https://arxiv.org/pdf/2602.04349v1",
    "published_date": "2026-02-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/BlueDyee/VecSet-Edit",
    "keywords": [
      "3d gaussian",
      "high-fidelity",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.04349v1",
      "pdf": "https://arxiv.org/pdf/2602.04349v1",
      "github": "https://github.com/BlueDyee/VecSet-Edit"
    },
    "bibtex": ""
  },
  {
    "title": "JOintGS: Joint Optimization of Cameras, Bodies and 3D Gaussians for In-the-Wild Monocular Reconstruction",
    "authors": [
      "Zihan Lou",
      "Jinlong Fan",
      "Sihan Ma",
      "Yuxiang Yang",
      "Jing Zhang"
    ],
    "abstract": "Reconstructing high-fidelity animatable 3D human avatars from monocular RGB videos remains challenging, particularly in unconstrained in-the-wild scenarios where camera parameters and human poses from off-the-shelf methods (e.g., COLMAP, HMR2.0) are often inaccurate. Splatting (3DGS) advances demonstrate impressive rendering quality and real-time performance, they critically depend on precise camera calibration and pose annotations, limiting their applicability in real-world settings. We present JOintGS, a unified framework that jointly optimizes camera extrinsics, human poses, and 3D Gaussian representations from coarse initialization through a synergistic refinement mechanism. Our key insight is that explicit foreground-background disentanglement enables mutual reinforcement: static background Gaussians anchor camera estimation via multi-view consistency; refined cameras improve human body alignment through accurate temporal correspondence; optimized human poses enhance scene reconstruction by removing dynamic artifacts from static constraints. We further introduce a temporal dynamics module to capture fine-grained pose-dependent deformations and a residual color field to model illumination variations. Extensive experiments on NeuMan and EMDB datasets demonstrate that JOintGS achieves superior reconstruction quality, with 2.1~dB PSNR improvement over state-of-the-art methods on NeuMan dataset, while maintaining real-time rendering. Notably, our method shows significantly enhanced robustness to noisy initialization compared to the baseline.Our source code is available at https://github.com/MiliLab/JOintGS.",
    "arxiv_url": "https://arxiv.org/abs/2602.04317v1",
    "pdf_url": "https://arxiv.org/pdf/2602.04317v1",
    "published_date": "2026-02-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/MiliLab/JOintGS",
    "keywords": [
      "illumination",
      "dynamic",
      "high-fidelity",
      "body",
      "deformation",
      "real-time rendering",
      "avatar",
      "ar",
      "human",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.04317v1",
      "pdf": "https://arxiv.org/pdf/2602.04317v1",
      "github": "https://github.com/MiliLab/JOintGS"
    },
    "bibtex": ""
  },
  {
    "title": "SkeletonGaussian: Editable 4D Generation through Gaussian Skeletonization",
    "authors": [
      "Lifan Wu",
      "Ruijie Zhu",
      "Yubo Ai",
      "Tianzhu Zhang"
    ],
    "abstract": "4D generation has made remarkable progress in synthesizing dynamic 3D objects from input text, images, or videos. However, existing methods often represent motion as an implicit deformation field, which limits direct control and editability. To address this issue, we propose SkeletonGaussian, a novel framework for generating editable dynamic 3D Gaussians from monocular video input. Our approach introduces a hierarchical articulated representation that decomposes motion into sparse rigid motion explicitly driven by a skeleton and fine-grained non-rigid motion. Concretely, we extract a robust skeleton and drive rigid motion via linear blend skinning, followed by a hexplane-based refinement for non-rigid deformations, enhancing interpretability and editability. Experimental results demonstrate that SkeletonGaussian surpasses existing methods in generation quality while enabling intuitive motion editing, establishing a new paradigm for editable 4D generation. Project page: https://wusar.github.io/projects/skeletongaussian/",
    "arxiv_url": "https://arxiv.org/abs/2602.04271v1",
    "pdf_url": "https://arxiv.org/pdf/2602.04271v1",
    "published_date": "2026-02-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "deformation",
      "ar",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.04271v1",
      "pdf": "https://arxiv.org/pdf/2602.04271v1",
      "project": "https://wusar.github.io/projects/skeletongaussian"
    },
    "bibtex": ""
  },
  {
    "title": "Towards Next-Generation SLAM: A Survey on 3DGS-SLAM Focusing on Performance, Robustness, and Future Directions",
    "authors": [
      "Li Wang",
      "Ruixuan Gong",
      "Yumo Han",
      "Lei Yang",
      "Lu Yang",
      "Ying Li",
      "Bin Xu",
      "Huaping Liu",
      "Rong Fu"
    ],
    "abstract": "Traditional Simultaneous Localization and Mapping (SLAM) systems often face limitations including coarse rendering quality, insufficient recovery of scene details, and poor robustness in dynamic environments. 3D Gaussian Splatting (3DGS), with its efficient explicit representation and high-quality rendering capabilities, offers a new reconstruction paradigm for SLAM. This survey comprehensively reviews key technical approaches for integrating 3DGS with SLAM. We analyze performance optimization of representative methods across four critical dimensions: rendering quality, tracking accuracy, reconstruction speed, and memory consumption, delving into their design principles and breakthroughs. Furthermore, we examine methods for enhancing the robustness of 3DGS-SLAM in complex environments such as motion blur and dynamic environments. Finally, we discuss future challenges and development trends in this area. This survey aims to provide a technical reference for researchers and foster the development of next-generation SLAM systems characterized by high fidelity, efficiency, and robustness.",
    "arxiv_url": "https://arxiv.org/abs/2602.04251v1",
    "pdf_url": "https://arxiv.org/pdf/2602.04251v1",
    "published_date": "2026-02-04",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "dynamic",
      "face",
      "tracking",
      "survey",
      "gaussian splatting",
      "ar",
      "mapping",
      "3d gaussian",
      "slam",
      "efficient",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.04251v1",
      "pdf": "https://arxiv.org/pdf/2602.04251v1"
    },
    "bibtex": ""
  },
  {
    "title": "Seeing Through Clutter: Structured 3D Scene Reconstruction via Iterative Object Removal",
    "authors": [
      "Rio Aguina-Kang",
      "Kevin James Blackburn-Matzen",
      "Thibault Groueix",
      "Vladimir Kim",
      "Matheus Gadelha"
    ],
    "abstract": "We present SeeingThroughClutter, a method for reconstructing structured 3D representations from single images by segmenting and modeling objects individually. Prior approaches rely on intermediate tasks such as semantic segmentation and depth estimation, which often underperform in complex scenes, particularly in the presence of occlusion and clutter. We address this by introducing an iterative object removal and reconstruction pipeline that decomposes complex scenes into a sequence of simpler subtasks. Using VLMs as orchestrators, foreground objects are removed one at a time via detection, segmentation, object removal, and 3D fitting. We show that removing objects allows for cleaner segmentations of subsequent objects, even in highly occluded scenes. Our method requires no task-specific training and benefits directly from ongoing advances in foundation models. We demonstrate stateof-the-art robustness on 3D-Front and ADE20K datasets. Project Page: https://rioak.github.io/seeingthroughclutter/",
    "arxiv_url": "https://arxiv.org/abs/2602.04053v1",
    "pdf_url": "https://arxiv.org/pdf/2602.04053v1",
    "published_date": "2026-02-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "ar",
      "semantic"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.04053v1",
      "pdf": "https://arxiv.org/pdf/2602.04053v1",
      "project": "https://rioak.github.io/seeingthroughclutter"
    },
    "bibtex": ""
  },
  {
    "title": "AnyStyle: Single-Pass Multimodal Stylization for 3D Gaussian Splatting",
    "authors": [
      "Joanna Kaleta",
      "Bartosz Świrta",
      "Kacper Kania",
      "Przemysław Spurek",
      "Marek Kowalski"
    ],
    "abstract": "The growing demand for rapid and scalable 3D asset creation has driven interest in feed-forward 3D reconstruction methods, with 3D Gaussian Splatting (3DGS) emerging as an effective scene representation. While recent approaches have demonstrated pose-free reconstruction from unposed image collections, integrating stylization or appearance control into such pipelines remains underexplored. Existing attempts largely rely on image-based conditioning, which limits both controllability and flexibility. In this work, we introduce AnyStyle, a feed-forward 3D reconstruction and stylization framework that enables pose-free, zero-shot stylization through multimodal conditioning. Our method supports both textual and visual style inputs, allowing users to control the scene appearance using natural language descriptions or reference images. We propose a modular stylization architecture that requires only minimal architectural modifications and can be integrated into existing feed-forward 3D reconstruction backbones. Experiments demonstrate that AnyStyle improves style controllability over prior feed-forward stylization methods while preserving high-quality geometric reconstruction. A user study further confirms that AnyStyle achieves superior stylization quality compared to an existing state-of-the-art approach. Repository: https://github.com/joaxkal/AnyStyle.",
    "arxiv_url": "https://arxiv.org/abs/2602.04043v1",
    "pdf_url": "https://arxiv.org/pdf/2602.04043v1",
    "published_date": "2026-02-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/joaxkal/AnyStyle",
    "keywords": [
      "3d reconstruction",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.04043v1",
      "pdf": "https://arxiv.org/pdf/2602.04043v1",
      "github": "https://github.com/joaxkal/AnyStyle"
    },
    "bibtex": ""
  },
  {
    "title": "Constrained Dynamic Gaussian Splatting",
    "authors": [
      "Zihan Zheng",
      "Zhenglong Wu",
      "Xuanxuan Wang",
      "Houqiang Zhong",
      "Xiaoyun Zhang",
      "Qiang Hu",
      "Guangtao Zhai",
      "Wenjun Zhang"
    ],
    "abstract": "While Dynamic Gaussian Splatting enables high-fidelity 4D reconstruction, its deployment is severely hindered by a fundamental dilemma: unconstrained densification leads to excessive memory consumption incompatible with edge devices, whereas heuristic pruning fails to achieve optimal rendering quality under preset Gaussian budgets. In this work, we propose Constrained Dynamic Gaussian Splatting (CDGS), a novel framework that formulates dynamic scene reconstruction as a budget-constrained optimization problem to enforce a strict, user-defined Gaussian budget during training. Our key insight is to introduce a differentiable budget controller as the core optimization driver. Guided by a multi-modal unified importance score, this controller fuses geometric, motion, and perceptual cues for precise capacity regulation. To maximize the utility of this fixed budget, we further decouple the optimization of static and dynamic elements, employing an adaptive allocation mechanism that dynamically distributes capacity based on motion complexity. Furthermore, we implement a three-phase training strategy to seamlessly integrate these constraints, ensuring precise adherence to the target count. Coupled with a dual-mode hybrid compression scheme, CDGS not only strictly adheres to hardware constraints (error < 2%}) but also pushes the Pareto frontier of rate-distortion performance. Extensive experiments demonstrate that CDGS delivers optimal rendering quality under varying capacity limits, achieving over 3x compression compared to state-of-the-art methods.",
    "arxiv_url": "https://arxiv.org/abs/2602.03538v1",
    "pdf_url": "https://arxiv.org/pdf/2602.03538v1",
    "published_date": "2026-02-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "high-fidelity",
      "compression",
      "gaussian splatting",
      "ar",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.03538v1",
      "pdf": "https://arxiv.org/pdf/2602.03538v1"
    },
    "bibtex": ""
  },
  {
    "title": "Pi-GS: Sparse-View Gaussian Splatting with Dense π^3 Initialization",
    "authors": [
      "Manuel Hofer",
      "Markus Steinberger",
      "Thomas Köhler"
    ],
    "abstract": "Novel view synthesis has evolved rapidly, advancing from Neural Radiance Fields to 3D Gaussian Splatting (3DGS), which offers real-time rendering and rapid training without compromising visual fidelity. However, 3DGS relies heavily on accurate camera poses and high-quality point cloud initialization, which are difficult to obtain in sparse-view scenarios. While traditional Structure from Motion (SfM) pipelines often fail in these settings, existing learning-based point estimation alternatives typically require reliable reference views and remain sensitive to pose or depth errors. In this work, we propose a robust method utilizing π^3, a reference-free point cloud estimation network. We integrate dense initialization from π^3 with a regularization scheme designed to mitigate geometric inaccuracies. Specifically, we employ uncertainty-guided depth supervision, normal consistency loss, and depth warping. Experimental results demonstrate that our approach achieves state-of-the-art performance on the Tanks and Temples, LLFF, DTU, and MipNeRF360 datasets.",
    "arxiv_url": "https://arxiv.org/abs/2602.03327v1",
    "pdf_url": "https://arxiv.org/pdf/2602.03327v1",
    "published_date": "2026-02-03",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "real-time rendering",
      "gaussian splatting",
      "ar",
      "nerf",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.03327v1",
      "pdf": "https://arxiv.org/pdf/2602.03327v1"
    },
    "bibtex": ""
  },
  {
    "title": "WebSplatter: Enabling Cross-Device Efficient Gaussian Splatting in Web Browsers via WebGPU",
    "authors": [
      "Yudong Han",
      "Chao Xu",
      "Xiaodan Ye",
      "Weichen Bi",
      "Zilong Dong",
      "Yun Ma"
    ],
    "abstract": "We present WebSplatter, an end-to-end GPU rendering pipeline for the heterogeneous web ecosystem. Unlike naive ports, WebSplatter introduces a wait-free hierarchical radix sort that circumvents the lack of global atomics in WebGPU, ensuring deterministic execution across diverse hardware. Furthermore, we propose an opacity-aware geometry culling stage that dynamically prunes splats before rasterization, significantly reducing overdraw and peak memory footprint. Evaluation demonstrates that WebSplatter consistently achieves 1.2$\\times$ to 4.5$\\times$ speedups over state-of-the-art web viewers.",
    "arxiv_url": "https://arxiv.org/abs/2602.03207v1",
    "pdf_url": "https://arxiv.org/pdf/2602.03207v1",
    "published_date": "2026-02-03",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.PF"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "geometry",
      "ar",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.03207v1",
      "pdf": "https://arxiv.org/pdf/2602.03207v1"
    },
    "bibtex": ""
  },
  {
    "title": "SharpTimeGS: Sharp and Stable Dynamic Gaussian Splatting via Lifespan Modulation",
    "authors": [
      "Zhanfeng Liao",
      "Jiajun Zhang",
      "Hanzhang Tu",
      "Zhixi Wang",
      "Yunqi Gao",
      "Hongwen Zhang",
      "Yebin Liu"
    ],
    "abstract": "Novel view synthesis of dynamic scenes is fundamental to achieving photorealistic 4D reconstruction and immersive visual experiences. Recent progress in Gaussian-based representations has significantly improved real-time rendering quality, yet existing methods still struggle to maintain a balance between long-term static and short-term dynamic regions in both representation and optimization. To address this, we present SharpTimeGS, a lifespan-aware 4D Gaussian framework that achieves temporally adaptive modeling of both static and dynamic regions under a unified representation. Specifically, we introduce a learnable lifespan parameter that reformulates temporal visibility from a Gaussian-shaped decay into a flat-top profile, allowing primitives to remain consistently active over their intended duration and avoiding redundant densification. In addition, the learned lifespan modulates each primitives' motion, reducing drift in long-lived static points while retaining unrestricted motion for short-lived dynamic ones. This effectively decouples motion magnitude from temporal duration, improving long-term stability without compromising dynamic fidelity. Moreover, we design a lifespan-velocity-aware densification strategy that mitigates optimization imbalance between static and dynamic regions by allocating more capacity to regions with pronounced motion while keeping static areas compact and stable. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art performance while supporting real-time rendering up to 4K resolution at 100 FPS on one RTX 4090.",
    "arxiv_url": "https://arxiv.org/abs/2602.02989v2",
    "pdf_url": "https://arxiv.org/pdf/2602.02989v2",
    "published_date": "2026-02-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "compact",
      "dynamic",
      "real-time rendering",
      "gaussian splatting",
      "ar",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.02989v2",
      "pdf": "https://arxiv.org/pdf/2602.02989v2"
    },
    "bibtex": ""
  },
  {
    "title": "SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation",
    "authors": [
      "Mu Huang",
      "Hui Wang",
      "Kerui Ren",
      "Linning Xu",
      "Yunsong Zhou",
      "Mulin Yu",
      "Bo Dai",
      "Jiangmiao Pang"
    ],
    "abstract": "Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.",
    "arxiv_url": "https://arxiv.org/abs/2602.02402v1",
    "pdf_url": "https://arxiv.org/pdf/2602.02402v1",
    "published_date": "2026-02-02",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "physics.app-ph"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "body",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.02402v1",
      "pdf": "https://arxiv.org/pdf/2602.02402v1"
    },
    "bibtex": ""
  },
  {
    "title": "Intellectual Property Protection for 3D Gaussian Splatting Assets: A Survey",
    "authors": [
      "Longjie Zhao",
      "Ziming Hong",
      "Jiaxin Huang",
      "Runnan Chen",
      "Mingming Gong",
      "Tongliang Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become a mainstream representation for real-time 3D scene synthesis, enabling applications in virtual and augmented reality, robotics, and 3D content creation. Its rising commercial value and explicit parametric structure raise emerging intellectual property (IP) protection concerns, prompting a surge of research on 3DGS IP protection. However, current progress remains fragmented, lacking a unified view of the underlying mechanisms, protection paradigms, and robustness challenges. To address this gap, we present the first systematic survey on 3DGS IP protection and introduce a bottom-up framework that examines (i) underlying Gaussian-based perturbation mechanisms, (ii) passive and active protection paradigms, and (iii) robustness threats under emerging generative AI era, revealing gaps in technical foundations and robustness characterization and indicating opportunities for deeper investigation. Finally, we outline six research directions across robustness, efficiency, and protection paradigms, offering a roadmap toward reliable and trustworthy IP protection for 3DGS assets.",
    "arxiv_url": "https://arxiv.org/abs/2602.03878v1",
    "pdf_url": "https://arxiv.org/pdf/2602.03878v1",
    "published_date": "2026-02-02",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "survey",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.03878v1",
      "pdf": "https://arxiv.org/pdf/2602.03878v1"
    },
    "bibtex": ""
  },
  {
    "title": "UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction",
    "authors": [
      "Changbai Li",
      "Haodong Zhu",
      "Hanlin Chen",
      "Xiuping Liang",
      "Tongfei Chen",
      "Shuwei Shao",
      "Linlin Yang",
      "Huobin Tan",
      "Baochang Zhang"
    ],
    "abstract": "While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2602.02089v1",
    "pdf_url": "https://arxiv.org/pdf/2602.02089v1",
    "published_date": "2026-02-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "high-fidelity",
      "real-time rendering",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.02089v1",
      "pdf": "https://arxiv.org/pdf/2602.02089v1"
    },
    "bibtex": ""
  },
  {
    "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
    "authors": [
      "Bing He",
      "Jingnan Gao",
      "Yunuo Chen",
      "Ning Cao",
      "Gang Chen",
      "Zhengxue Cheng",
      "Li Song",
      "Wenjun Zhang"
    ],
    "abstract": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: https://hebing-sjtu.github.io/SurfSplat-website/",
    "arxiv_url": "https://arxiv.org/abs/2602.02000v2",
    "pdf_url": "https://arxiv.org/pdf/2602.02000v2",
    "published_date": "2026-02-02",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "3d reconstruction",
      "geometry",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.02000v2",
      "pdf": "https://arxiv.org/pdf/2602.02000v2",
      "project": "https://hebing-sjtu.github.io/SurfSplat-website"
    },
    "bibtex": ""
  },
  {
    "title": "CloDS: Visual-Only Unsupervised Cloth Dynamics Learning in Unknown Conditions",
    "authors": [
      "Yuliang Zhan",
      "Jian Li",
      "Wenbing Huang",
      "Wenbing Huang",
      "Yang Liu",
      "Hao Sun"
    ],
    "abstract": "Deep learning has demonstrated remarkable capabilities in simulating complex dynamic systems. However, existing methods require known physical properties as supervision or inputs, limiting their applicability under unknown conditions. To explore this challenge, we introduce Cloth Dynamics Grounding (CDG), a novel scenario for unsupervised learning of cloth dynamics from multi-view visual observations. We further propose Cloth Dynamics Splatting (CloDS), an unsupervised dynamic learning framework designed for CDG. CloDS adopts a three-stage pipeline that first performs video-to-geometry grounding and then trains a dynamics model on the grounded meshes. To cope with large non-linear deformations and severe self-occlusions during grounding, we introduce a dual-position opacity modulation that supports bidirectional mapping between 2D observations and 3D geometry via mesh-based Gaussian splatting in video-to-geometry grounding stage. It jointly considers the absolute and relative position of Gaussian components. Comprehensive experimental evaluations demonstrate that CloDS effectively learns cloth dynamics from visual data while maintaining strong generalization capabilities for unseen configurations. Our code is available at https://github.com/whynot-zyl/CloDS. Visualization results are available at https://github.com/whynot-zyl/CloDS_video}.%\\footnote{As in this example.",
    "arxiv_url": "https://arxiv.org/abs/2602.01844v1",
    "pdf_url": "https://arxiv.org/pdf/2602.01844v1",
    "published_date": "2026-02-02",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/whynot-zyl/CloDS",
    "keywords": [
      "dynamic",
      "deformation",
      "geometry",
      "ar",
      "mapping",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.01844v1",
      "pdf": "https://arxiv.org/pdf/2602.01844v1",
      "github": "https://github.com/whynot-zyl/CloDS"
    },
    "bibtex": ""
  },
  {
    "title": "OFERA: Blendshape-driven 3D Gaussian Control for Occluded Facial Expression to Realistic Avatars in VR",
    "authors": [
      "Seokhwan Yang",
      "Boram Yoon",
      "Seoyoung Kang",
      "Hail Song",
      "Woontack Woo"
    ],
    "abstract": "We propose OFERA, a novel framework for real-time expression control of photorealistic Gaussian head avatars for VR headset users. Existing approaches attempt to recover occluded facial expressions using additional sensors or internal cameras, but sensor-based methods increase device weight and discomfort, while camera-based methods raise privacy concerns and suffer from limited access to raw data. To overcome these limitations, we leverage the blendshape signals provided by commercial VR headsets as expression inputs. Our framework consists of three key components: (1) Blendshape Distribution Alignment (BDA), which applies linear regression to align the headset-provided blendshape distribution to a canonical input space; (2) an Expression Parameter Mapper (EPM) that maps the aligned blendshape signals into an expression parameter space for controlling Gaussian head avatars; and (3) a Mapper-integrated Avatar (MiA) that incorporates EPM into the avatar learning process to ensure distributional consistency. Furthermore, OFERA establishes an end-to-end pipeline that senses and maps expressions, updates Gaussian avatars, and renders them in real-time within VR environments. We show that EPM outperforms existing mapping methods on quantitative metrics, and we demonstrate through a user study that the full OFERA framework enhances expression fidelity while preserving avatar realism. By enabling real-time and photorealistic avatar expression control, OFERA significantly improves telepresence in VR communication. A project page is available at https://ysshwan147.github.io/projects/ofera/.",
    "arxiv_url": "https://arxiv.org/abs/2602.01748v1",
    "pdf_url": "https://arxiv.org/pdf/2602.01748v1",
    "published_date": "2026-02-02",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "vr",
      "avatar",
      "ar",
      "3d gaussian",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.01748v1",
      "pdf": "https://arxiv.org/pdf/2602.01748v1",
      "project": "https://ysshwan147.github.io/projects/ofera"
    },
    "bibtex": ""
  },
  {
    "title": "FastPhysGS: Accelerating Physics-based Dynamic 3DGS Simulation via Interior Completion and Adaptive Optimization",
    "authors": [
      "Yikun Ma",
      "Yiqing Li",
      "Jingwen Ye",
      "Zhongkai Wu",
      "Weidong Zhang",
      "Lin Gao",
      "Zhi Jin"
    ],
    "abstract": "Extending 3D Gaussian Splatting (3DGS) to 4D physical simulation remains challenging. Based on the Material Point Method (MPM), existing methods either rely on manual parameter tuning or distill dynamics from video diffusion models, limiting the generalization and optimization efficiency. Recent attempts using LLMs/VLMs suffer from a text/image-to-3D perceptual gap, yielding unstable physics behavior. In addition, they often ignore the surface structure of 3DGS, leading to implausible motion. We propose FastPhysGS, a fast and robust framework for physics-based dynamic 3DGS simulation:(1) Instance-aware Particle Filling (IPF) with Monte Carlo Importance Sampling (MCIS) to efficiently populate interior particles while preserving geometric fidelity; (2) Bidirectional Graph Decoupling Optimization (BGDO), an adaptive strategy that rapidly optimizes material parameters predicted from a VLM. Experiments show FastPhysGS achieves high-fidelity physical simulation in 1 minute using only 7 GB runtime memory, outperforming prior works with broad potential applications.",
    "arxiv_url": "https://arxiv.org/abs/2602.01723v1",
    "pdf_url": "https://arxiv.org/pdf/2602.01723v1",
    "published_date": "2026-02-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "high-fidelity",
      "face",
      "motion",
      "ar",
      "3d gaussian",
      "efficient",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.01723v1",
      "pdf": "https://arxiv.org/pdf/2602.01723v1"
    },
    "bibtex": ""
  },
  {
    "title": "VRGaussianAvatar: Integrating 3D Gaussian Avatars into VR",
    "authors": [
      "Hail Song",
      "Boram Yoon",
      "Seokhwan Yang",
      "Seoyoung Kang",
      "Hyunjeong Kim",
      "Henning Metzmacher",
      "Woontack Woo"
    ],
    "abstract": "We present VRGaussianAvatar, an integrated system that enables real-time full-body 3D Gaussian Splatting (3DGS) avatars in virtual reality using only head-mounted display (HMD) tracking signals. The system adopts a parallel pipeline with a VR Frontend and a GA Backend. The VR Frontend uses inverse kinematics to estimate full-body pose and streams the resulting pose along with stereo camera parameters to the backend. The GA Backend stereoscopically renders a 3DGS avatar reconstructed from a single image. To improve stereo rendering efficiency, we introduce Binocular Batching, which jointly processes left and right eye views in a single batched pass to reduce redundant computation and support high-resolution VR displays. We evaluate VRGaussianAvatar with quantitative performance tests and a within-subject user study against image- and video-based mesh avatar baselines. Results show that VRGaussianAvatar sustains interactive VR performance and yields higher perceived appearance similarity, embodiment, and plausibility. Project page and source code are available at https://vrgaussianavatar.github.io.",
    "arxiv_url": "https://arxiv.org/abs/2602.01674v1",
    "pdf_url": "https://arxiv.org/pdf/2602.01674v1",
    "published_date": "2026-02-02",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "vr",
      "body",
      "tracking",
      "avatar",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.01674v1",
      "pdf": "https://arxiv.org/pdf/2602.01674v1",
      "project": "https://vrgaussianavatar.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "MarkCleaner: High-Fidelity Watermark Removal via Imperceptible Micro-Geometric Perturbation",
    "authors": [
      "Xiaoxi Kong",
      "Jieyu Yuan",
      "Pengdi Chen",
      "Yuanlin Zhang",
      "Chongyi Li",
      "Bin Li"
    ],
    "abstract": "Semantic watermarks exhibit strong robustness against conventional image-space attacks. In this work, we show that such robustness does not survive under micro-geometric perturbations: spatial displacements can remove watermarks by breaking the phase alignment. Motivated by this observation, we introduce MarkCleaner, a watermark removal framework that avoids semantic drift caused by regeneration-based watermark removal. Specifically, MarkCleaner is trained with micro-geometry-perturbed supervision, which encourages the model to separate semantic content from strict spatial alignment and enables robust reconstruction under subtle geometric displacements. The framework adopts a mask-guided encoder that learns explicit spatial representations and a 2D Gaussian Splatting-based decoder that explicitly parameterizes geometric perturbations while preserving semantic content. Extensive experiments demonstrate that MarkCleaner achieves superior performance in both watermark removal effectiveness and visual fidelity, while enabling efficient real-time inference. Our code will be made available upon acceptance.",
    "arxiv_url": "https://arxiv.org/abs/2602.01513v1",
    "pdf_url": "https://arxiv.org/pdf/2602.01513v1",
    "published_date": "2026-02-02",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "geometry",
      "ar",
      "semantic",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.01513v1",
      "pdf": "https://arxiv.org/pdf/2602.01513v1"
    },
    "bibtex": ""
  },
  {
    "title": "Position: 3D Gaussian Splatting Watermarking Should Be Scenario-Driven and Threat-Model Explicit",
    "authors": [
      "Yangfan Deng",
      "Anirudh Nakra",
      "Min Wu"
    ],
    "abstract": "3D content acquisition and creation are expanding rapidly in the new era of machine learning and AI. 3D Gaussian Splatting (3DGS) has become a promising high-fidelity and real-time representation for 3D content. Similar to the initial wave of digital audio-visual content at the turn of the millennium, the demand for intellectual property protection is also increasing, since explicit and editable 3D parameterization makes unauthorized use and dissemination easier. In this position paper, we argue that effective progress in watermarking 3D assets requires articulated security objectives and realistic threat models, incorporating the lessons learned from digital audio-visual asset protection over the past decades. To address this gap in security specification and evaluation, we advocate a scenario-driven formulation, in which adversarial capabilities are formalized through a security model. Based on this formulation, we construct a reference framework that organizes existing methods and clarifies how specific design choices map to corresponding adversarial assumptions. Within this framework, we also examine a legacy spread-spectrum embedding scheme, characterizing its advantages and limitations and highlighting the important trade-offs it entails. Overall, this work aims to foster effective intellectual property protection for 3D assets.",
    "arxiv_url": "https://arxiv.org/abs/2602.02602v1",
    "pdf_url": "https://arxiv.org/pdf/2602.02602v1",
    "published_date": "2026-02-01",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "3d gaussian",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.02602v1",
      "pdf": "https://arxiv.org/pdf/2602.02602v1"
    },
    "bibtex": ""
  },
  {
    "title": "Split&Splat: Zero-Shot Panoptic Segmentation via Explicit Instance Modeling and 3D Gaussian Splatting",
    "authors": [
      "Leonardo Monchieri",
      "Elena Camuffo",
      "Francesco Barbato",
      "Pietro Zanuttigh",
      "Simone Milani"
    ],
    "abstract": "3D Gaussian Splatting (GS) enables fast and high-quality scene reconstruction, but it lacks an object-consistent and semantically aware structure. We propose Split&Splat, a framework for panoptic scene reconstruction using 3DGS. Our approach explicitly models object instances. It first propagates instance masks across views using depth, thus producing view-consistent 2D masks. Each object is then reconstructed independently and merged back into the scene while refining its boundaries. Finally, instance-level semantic descriptors are embedded in the reconstructed objects, supporting various applications, including panoptic segmentation, object retrieval, and 3D editing. Unlike existing methods, Split&Splat tackles the problem by first segmenting the scene and then reconstructing each object individually. This design naturally supports downstream tasks and allows Split&Splat to achieve state-of-the-art performance on the ScanNetv2 segmentation benchmark.",
    "arxiv_url": "https://arxiv.org/abs/2602.03809v1",
    "pdf_url": "https://arxiv.org/pdf/2602.03809v1",
    "published_date": "2026-02-01",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "ar",
      "semantic",
      "3d gaussian",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.03809v1",
      "pdf": "https://arxiv.org/pdf/2602.03809v1"
    },
    "bibtex": ""
  },
  {
    "title": "Radioactive 3D Gaussian Ray Tracing for Tomographic Reconstruction",
    "authors": [
      "Ling Chen",
      "Bao Yang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged in computer vision as a promising rendering technique. By adapting the principles of Elliptical Weighted Average (EWA) splatting to a modern differentiable pipeline, 3DGS enables real-time, high-quality novel view synthesis. Building upon this, R2-Gaussian extended the 3DGS paradigm to tomographic reconstruction by rectifying integration bias, achieving state-of-the-art performance in computed tomography (CT). To enable differentiability, R2-Gaussian adopts a local affine approximation: each 3D Gaussian is locally mapped to a 2D Gaussian on the detector and composed via alpha blending to form projections. However, the affine approximation can degrade reconstruction quantitative accuracy and complicate the incorporation of nonlinear geometric corrections. To address these limitations, we propose a tomographic reconstruction framework based on 3D Gaussian ray tracing. Our approach provides two key advantages over splatting-based models: (i) it computes the line integral through 3D Gaussian primitives analytically, avoiding the local affine collapse and thus yielding a more physically consistent forward projection model; and (ii) the ray-tracing formulation gives explicit control over ray origins and directions, which facilitates the precise application of nonlinear geometric corrections, e.g., arc-correction used in positron emission tomography (PET). These properties extend the applicability of Gaussian-based reconstruction to a wider range of realistic tomography systems while improving projection accuracy.",
    "arxiv_url": "https://arxiv.org/abs/2602.01057v1",
    "pdf_url": "https://arxiv.org/pdf/2602.01057v1",
    "published_date": "2026-02-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "ray tracing",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.01057v1",
      "pdf": "https://arxiv.org/pdf/2602.01057v1"
    },
    "bibtex": ""
  },
  {
    "title": "HPC: Hierarchical Point-based Latent Representation for Streaming Dynamic Gaussian Splatting Compression",
    "authors": [
      "Yangzhi Ma",
      "Bojun Liu",
      "Wenting Liao",
      "Dong Liu",
      "Zhu Li",
      "Li Li"
    ],
    "abstract": "While dynamic Gaussian Splatting has driven significant advances in free-viewpoint video, maintaining its rendering quality with a small memory footprint for efficient streaming transmission still presents an ongoing challenge. Existing streaming dynamic Gaussian Splatting compression methods typically leverage a latent representation to drive the neural network for predicting Gaussian residuals between frames. Their core latent representations can be categorized into structured grid-based and unstructured point-based paradigms. However, the former incurs significant parameter redundancy by inevitably modeling unoccupied space, while the latter suffers from limited compactness as it fails to exploit local correlations. To relieve these limitations, we propose HPC, a novel streaming dynamic Gaussian Splatting compression framework. It employs a hierarchical point-based latent representation that operates on a per-Gaussian basis to avoid parameter redundancy in unoccupied space. Guided by a tailored aggregation scheme, these latent points achieve high compactness with low spatial redundancy. To improve compression efficiency, we further undertake the first investigation to compress neural networks for streaming dynamic Gaussian Splatting through mining and exploiting the inter-frame correlation of parameters. Combined with latent compression, this forms a fully end-to-end compression framework. Comprehensive experimental evaluations demonstrate that HPC substantially outperforms state-of-the-art methods. It achieves a storage reduction of 67% against its baseline while maintaining high reconstruction fidelity.",
    "arxiv_url": "https://arxiv.org/abs/2602.00671v1",
    "pdf_url": "https://arxiv.org/pdf/2602.00671v1",
    "published_date": "2026-01-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "dynamic",
      "compression",
      "ar",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.00671v1",
      "pdf": "https://arxiv.org/pdf/2602.00671v1"
    },
    "bibtex": ""
  },
  {
    "title": "Tune-Your-Style: Intensity-tunable 3D Style Transfer with Gaussian Splatting",
    "authors": [
      "Yian Zhao",
      "Rushi Ye",
      "Ruochong Zheng",
      "Zesen Cheng",
      "Chaoran Feng",
      "Jiashu Yang",
      "Pengchong Qiao",
      "Chang Liu",
      "Jie Chen"
    ],
    "abstract": "3D style transfer refers to the artistic stylization of 3D assets based on reference style images. Recently, 3DGS-based stylization methods have drawn considerable attention, primarily due to their markedly enhanced training and rendering speeds. However, a vital challenge for 3D style transfer is to strike a balance between the content and the patterns and colors of the style. Although the existing methods strive to achieve relatively balanced outcomes, the fixed-output paradigm struggles to adapt to the diverse content-style balance requirements from different users. In this work, we introduce a creative intensity-tunable 3D style transfer paradigm, dubbed \\textbf{Tune-Your-Style}, which allows users to flexibly adjust the style intensity injected into the scene to match their desired content-style balance, thus enhancing the customizability of 3D style transfer. To achieve this goal, we first introduce Gaussian neurons to explicitly model the style intensity and parameterize a learnable style tuner to achieve intensity-tunable style injection. To facilitate the learning of tunable stylization, we further propose the tunable stylization guidance, which obtains multi-view consistent stylized views from diffusion models through cross-view style alignment, and then employs a two-stage optimization strategy to provide stable and efficient guidance by modulating the balance between full-style guidance from the stylized views and zero-style guidance from the initial rendering. Extensive experiments demonstrate that our method not only delivers visually appealing results, but also exhibits flexible customizability for 3D style transfer. Project page is available at https://zhao-yian.github.io/TuneStyle.",
    "arxiv_url": "https://arxiv.org/abs/2602.00618v1",
    "pdf_url": "https://arxiv.org/pdf/2602.00618v1",
    "published_date": "2026-01-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.00618v1",
      "pdf": "https://arxiv.org/pdf/2602.00618v1",
      "project": "https://zhao-yian.github.io/TuneStyle"
    },
    "bibtex": ""
  },
  {
    "title": "PSGS: Text-driven Panorama Sliding Scene Generation via Gaussian Splatting",
    "authors": [
      "Xin Zhang",
      "Shen Chen",
      "Jiale Zhou",
      "Lei Li"
    ],
    "abstract": "Generating realistic 3D scenes from text is crucial for immersive applications like VR, AR, and gaming. While text-driven approaches promise efficiency, existing methods suffer from limited 3D-text data and inconsistent multi-view stitching, resulting in overly simplistic scenes. To address this, we propose PSGS, a two-stage framework for high-fidelity panoramic scene generation. First, a novel two-layer optimization architecture generates semantically coherent panoramas: a layout reasoning layer parses text into structured spatial relationships, while a self-optimization layer refines visual details via iterative MLLM feedback. Second, our panorama sliding mechanism initializes globally consistent 3D Gaussian Splatting point clouds by strategically sampling overlapping perspectives. By incorporating depth and semantic coherence losses during training, we greatly improve the quality and detail fidelity of rendered scenes. Our experiments demonstrate that PSGS outperforms existing methods in panorama generation and produces more appealing 3D scenes, offering a robust solution for scalable immersive content creation.",
    "arxiv_url": "https://arxiv.org/abs/2602.00463v1",
    "pdf_url": "https://arxiv.org/pdf/2602.00463v1",
    "published_date": "2026-01-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "high-fidelity",
      "ar",
      "semantic",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.00463v1",
      "pdf": "https://arxiv.org/pdf/2602.00463v1"
    },
    "bibtex": ""
  },
  {
    "title": "3DGS$^2$-TR: Scalable Second-Order Trust-Region Method for 3D Gaussian Splatting",
    "authors": [
      "Roger Hsiao",
      "Yuchen Fang",
      "Xiangru Huang",
      "Ruilong Li",
      "Hesam Rabeti",
      "Zan Gojcic",
      "Javad Lavaei",
      "James Demmel",
      "Sophia Shao"
    ],
    "abstract": "We propose 3DGS$^2$-TR,a second-order optimizer for accelerating the scene training problem in 3D Gaussian Splatting (3DGS). Unlike existing second-order approaches that rely on explicit or dense curvature representations, such as 3DGS-LM (Höllein et al., 2025) or 3DGS2 (Lan et al., 2025), our method approximates curvature using only the diagonal of the Hessian matrix, efficiently via Hutchinson's method. Our approach is fully matrix-free and has the same complexity as ADAM (Kingma, 2024), $O(n)$ in both computation and memory costs. To ensure stable optimization in the presence of strong nonlinearity in the 3DGS rasterization process, we introduce a parameter-wise trust-region technique based on the squared Hellinger distance, regularizing updates to Gaussian parameters. Under identical parameter initialization and without densification, 3DGS$^2$-TR is able to achieve better reconstruction quality on standard datasets, using 50% fewer training iterations compared to ADAM, while incurring less than 1GB of peak GPU memory overhead (17% more than ADAM and 85% less than 3DGS-LM), enabling scalability to very large scenes and potentially to distributed training settings.",
    "arxiv_url": "https://arxiv.org/abs/2602.00395v1",
    "pdf_url": "https://arxiv.org/pdf/2602.00395v1",
    "published_date": "2026-01-30",
    "categories": [
      "cs.CV",
      "cs.LG",
      "math.OC"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "ar",
      "3d gaussian",
      "large scene",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.00395v1",
      "pdf": "https://arxiv.org/pdf/2602.00395v1"
    },
    "bibtex": ""
  },
  {
    "title": "EAG-PT: Emission-Aware Gaussians and Path Tracing for Indoor Scene Reconstruction and Editing",
    "authors": [
      "Xijie Yang",
      "Mulin Yu",
      "Changjian Jiang",
      "Kerui Ren",
      "Tao Lu",
      "Jiangmiao Pang",
      "Dahua Lin",
      "Bo Dai",
      "Linning Xu"
    ],
    "abstract": "Recent reconstruction methods based on radiance field such as NeRF and 3DGS reproduce indoor scenes with high visual fidelity, but break down under scene editing due to baked illumination and the lack of explicit light transport. In contrast, physically based inverse rendering relies on mesh representations and path tracing, which enforce correct light transport but place strong requirements on geometric fidelity, becoming a practical bottleneck for real indoor scenes. In this work, we propose Emission-Aware Gaussians and Path Tracing (EAG-PT), aiming for physically based light transport with a unified 2D Gaussian representation. Our design is based on three cores: (1) using 2D Gaussians as a unified scene representation and transport-friendly geometry proxy that avoids reconstructed mesh, (2) explicitly separating emissive and non-emissive components during reconstruction for further scene editing, and (3) decoupling reconstruction from final rendering by using efficient single-bounce optimization and high-quality multi-bounce path tracing after scene editing. Experiments on synthetic and real indoor scenes show that EAG-PT produces more natural and physically consistent renders after editing than radiant scene reconstructions, while preserving finer geometric detail and avoiding mesh-induced artifacts compared to mesh-based inverse path tracing. These results suggest promising directions for future use in interior design, XR content creation, and embodied AI.",
    "arxiv_url": "https://arxiv.org/abs/2601.23065v1",
    "pdf_url": "https://arxiv.org/pdf/2601.23065v1",
    "published_date": "2026-01-30",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "light transport",
      "geometry",
      "ar",
      "nerf",
      "efficient",
      "path tracing"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.23065v1",
      "pdf": "https://arxiv.org/pdf/2601.23065v1"
    },
    "bibtex": ""
  },
  {
    "title": "Self-Supervised Slice-to-Volume Reconstruction with Gaussian Representations for Fetal MRI",
    "authors": [
      "Yinsong Wang",
      "Thomas Fletcher",
      "Xinzhe Luo",
      "Aine Travers Dineen",
      "Rhodri Cusack",
      "Chen Qin"
    ],
    "abstract": "Reconstructing 3D fetal MR volumes from motion-corrupted stacks of 2D slices is a crucial and challenging task. Conventional slice-to-volume reconstruction (SVR) methods are time-consuming and require multiple orthogonal stacks for reconstruction. While learning-based SVR approaches have significantly reduced the time required at the inference stage, they heavily rely on ground truth information for training, which is inaccessible in practice. To address these challenges, we propose GaussianSVR, a self-supervised framework for slice-to-volume reconstruction. GaussianSVR represents the target volume using 3D Gaussian representations to achieve high-fidelity reconstruction. It leverages a simulated forward slice acquisition model to enable self-supervised training, alleviating the need for ground-truth volumes. Furthermore, to enhance both accuracy and efficiency, we introduce a multi-resolution training strategy that jointly optimizes Gaussian parameters and spatial transformations across different resolution levels. Experiments show that GaussianSVR outperforms the baseline methods on fetal MR volumetric reconstruction. Code will be available upon acceptance.",
    "arxiv_url": "https://arxiv.org/abs/2601.22990v1",
    "pdf_url": "https://arxiv.org/pdf/2601.22990v1",
    "published_date": "2026-01-30",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "high-fidelity",
      "ar",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.22990v1",
      "pdf": "https://arxiv.org/pdf/2601.22990v1"
    },
    "bibtex": ""
  },
  {
    "title": "Learning Geometrically-Grounded 3D Visual Representations for View-Generalizable Robotic Manipulation",
    "authors": [
      "Di Zhang",
      "Weicheng Duan",
      "Dasen Gu",
      "Hongye Lu",
      "Hai Zhang",
      "Hang Yu",
      "Junqiao Zhao",
      "Guang Chen"
    ],
    "abstract": "Real-world robotic manipulation demands visuomotor policies capable of robust spatial scene understanding and strong generalization across diverse camera viewpoints. While recent advances in 3D-aware visual representations have shown promise, they still suffer from several key limitations, including reliance on multi-view observations during inference which is impractical in single-view restricted scenarios, incomplete scene modeling that fails to capture holistic and fine-grained geometric structures essential for precise manipulation, and lack of effective policy training strategies to retain and exploit the acquired 3D knowledge. To address these challenges, we present MethodName, a unified representation-policy learning framework for view-generalizable robotic manipulation. MethodName introduces a single-view 3D pretraining paradigm that leverages point cloud reconstruction and feed-forward gaussian splatting under multi-view supervision to learn holistic geometric representations. During policy learning, MethodName performs multi-step distillation to preserve the pretrained geometric understanding and effectively transfer it to manipulation skills. We conduct experiments on 12 RLBench tasks, where our approach outperforms the previous state-of-the-art method by 12.7% in average success rate. Further evaluation on six representative tasks demonstrates strong zero-shot view generalization, with success rate drops of only 22.0% and 29.7% under moderate and large viewpoint shifts respectively, whereas the state-of-the-art method suffers larger decreases of 41.6% and 51.5%.",
    "arxiv_url": "https://arxiv.org/abs/2601.22988v1",
    "pdf_url": "https://arxiv.org/pdf/2601.22988v1",
    "published_date": "2026-01-30",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.22988v1",
      "pdf": "https://arxiv.org/pdf/2601.22988v1"
    },
    "bibtex": ""
  },
  {
    "title": "Diachronic Stereo Matching for Multi-Date Satellite Imagery",
    "authors": [
      "Elías Masquil",
      "Luca Savant Aira",
      "Roger Marí",
      "Thibaud Ehret",
      "Pablo Musé",
      "Gabriele Facciolo"
    ],
    "abstract": "Recent advances in image-based satellite 3D reconstruction have progressed along two complementary directions. On one hand, multi-date approaches using NeRF or Gaussian-splatting jointly model appearance and geometry across many acquisitions, achieving accurate reconstructions on opportunistic imagery with numerous observations. On the other hand, classical stereoscopic reconstruction pipelines deliver robust and scalable results for simultaneous or quasi-simultaneous image pairs. However, when the two images are captured months apart, strong seasonal, illumination, and shadow changes violate standard stereoscopic assumptions, causing existing pipelines to fail. This work presents the first Diachronic Stereo Matching method for satellite imagery, enabling reliable 3D reconstruction from temporally distant pairs. Two advances make this possible: (1) fine-tuning a state-of-the-art deep stereo network that leverages monocular depth priors, and (2) exposing it to a dataset specifically curated to include a diverse set of diachronic image pairs. In particular, we start from a pretrained MonSter model, trained initially on a mix of synthetic and real datasets such as SceneFlow and KITTI, and fine-tune it on a set of stereo pairs derived from the DFC2019 remote sensing challenge. This dataset contains both synchronic and diachronic pairs under diverse seasonal and illumination conditions. Experiments on multi-date WorldView-3 imagery demonstrate that our approach consistently surpasses classical pipelines and unadapted deep stereo models on both synchronic and diachronic settings. Fine-tuning on temporally diverse images, together with monocular priors, proves essential for enabling 3D reconstruction from previously incompatible acquisition dates. Left image (winter) Right image (autumn) DSM geometry Ours (1.23 m) Zero-shot (3.99 m) LiDAR GT Figure 1. Output geometry for a winter-autumn image pair from Omaha (OMA 331 test scene). Our method recovers accurate geometry despite the diachronic nature of the pair, exhibiting strong appearance changes, which cause existing zero-shot methods to fail. Missing values due to perspective shown in black.  Mean altitude error in parentheses; lower is better.",
    "arxiv_url": "https://arxiv.org/abs/2601.22808v1",
    "pdf_url": "https://arxiv.org/pdf/2601.22808v1",
    "published_date": "2026-01-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "shadow",
      "3d reconstruction",
      "geometry",
      "ar",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.22808v1",
      "pdf": "https://arxiv.org/pdf/2601.22808v1"
    },
    "bibtex": ""
  },
  {
    "title": "GaussianOcc3D: A Gaussian-Based Adaptive Multi-modal 3D Occupancy Prediction",
    "authors": [
      "A. Enes Doruk",
      "Hasan F. Ates"
    ],
    "abstract": "3D semantic occupancy prediction is a pivotal task in autonomous driving, providing a dense and fine-grained understanding of the surrounding environment, yet single-modality methods face trade-offs between camera semantics and LiDAR geometry. Existing multi-modal frameworks often struggle with modality heterogeneity, spatial misalignment, and the representation crisis--where voxels are computationally heavy and BEV alternatives are lossy. We present GaussianOcc3D, a multi-modal framework bridging camera and LiDAR through a memory-efficient, continuous 3D Gaussian representation. We introduce four modules: (1) LiDAR Depth Feature Aggregation (LDFA), using depth-wise deformable sampling to lift sparse signals onto Gaussian primitives; (2) Entropy-Based Feature Smoothing (EBFS) to mitigate domain noise; (3) Adaptive Camera-LiDAR Fusion (ACLF) with uncertainty-aware reweighting for sensor reliability; and (4) a Gauss-Mamba Head leveraging Selective State Space Models for global context with linear complexity. Evaluations on Occ3D, SurroundOcc, and SemanticKITTI benchmarks demonstrate state-of-the-art performance, achieving mIoU scores of 49.4%, 28.9%, and 25.2% respectively. GaussianOcc3D exhibits superior robustness across challenging rainy and nighttime conditions.",
    "arxiv_url": "https://arxiv.org/abs/2601.22729v1",
    "pdf_url": "https://arxiv.org/pdf/2601.22729v1",
    "published_date": "2026-01-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "face",
      "geometry",
      "autonomous driving",
      "ar",
      "semantic",
      "3d gaussian",
      "efficient",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.22729v1",
      "pdf": "https://arxiv.org/pdf/2601.22729v1"
    },
    "bibtex": ""
  },
  {
    "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
    "authors": [
      "Changjian Jiang",
      "Kerui Ren",
      "Xudong Li",
      "Kaiwen Song",
      "Linning Xu",
      "Tao Lu",
      "Junting Dong",
      "Yu Zhang",
      "Bo Dai",
      "Mulin Yu"
    ],
    "abstract": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of PLANING make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: https://city-super.github.io/PLANING/ .",
    "arxiv_url": "https://arxiv.org/abs/2601.22046v2",
    "pdf_url": "https://arxiv.org/pdf/2601.22046v2",
    "published_date": "2026-01-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "3d reconstruction",
      "ar",
      "efficient",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.22046v2",
      "pdf": "https://arxiv.org/pdf/2601.22046v2",
      "project": "https://city-super.github.io/PLANING"
    },
    "bibtex": ""
  },
  {
    "title": "Hybrid Foveated Path Tracing with Peripheral Gaussians for Immersive Anatomy",
    "authors": [
      "Constantin Kleinbeck",
      "Luisa Theelke",
      "Hannah Schieber",
      "Ulrich Eck",
      "Rüdiger von Eisenhart-Rothe",
      "Daniel Roth"
    ],
    "abstract": "Volumetric medical imaging offers great potential for understanding complex pathologies. Yet, traditional 2D slices provide little support for interpreting spatial relationships, forcing users to mentally reconstruct anatomy into three dimensions. Direct volumetric path tracing and VR rendering can improve perception but are computationally expensive, while precomputed representations, like Gaussian Splatting, require planning ahead. Both approaches limit interactive use.   We propose a hybrid rendering approach for high-quality, interactive, and immersive anatomical visualization. Our method combines streamed foveated path tracing with a lightweight Gaussian Splatting approximation of the periphery. The peripheral model generation is optimized with volume data and continuously refined using foveal renderings, enabling interactive updates. Depth-guided reprojection further improves robustness to latency and allows users to balance fidelity with refresh rate.   We compare our method against direct path tracing and Gaussian Splatting. Our results highlight how their combination can preserve strengths in visual quality while re-generating the peripheral model in under a second, eliminating extensive preprocessing and approximations. This opens new options for interactive medical visualization.",
    "arxiv_url": "https://arxiv.org/abs/2601.22026v1",
    "pdf_url": "https://arxiv.org/pdf/2601.22026v1",
    "published_date": "2026-01-29",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "understanding",
      "vr",
      "lightweight",
      "medical",
      "gaussian splatting",
      "ar",
      "path tracing"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.22026v1",
      "pdf": "https://arxiv.org/pdf/2601.22026v1"
    },
    "bibtex": ""
  },
  {
    "title": "Synthetic-to-Real Domain Bridging for Single-View 3D Reconstruction of Ships for Maritime Monitoring",
    "authors": [
      "Borja Carrillo-Perez",
      "Felix Sattler",
      "Angel Bueno Rodriguez",
      "Maurice Stephan",
      "Sarah Barnes"
    ],
    "abstract": "Three-dimensional (3D) reconstruction of ships is an important part of maritime monitoring, allowing improved visualization, inspection, and decision-making in real-world monitoring environments. However, most state-ofthe-art 3D reconstruction methods require multi-view supervision, annotated 3D ground truth, or are computationally intensive, making them impractical for real-time maritime deployment. In this work, we present an efficient pipeline for single-view 3D reconstruction of real ships by training entirely on synthetic data and requiring only a single view at inference. Our approach uses the Splatter Image network, which represents objects as sparse sets of 3D Gaussians for rapid and accurate reconstruction from single images. The model is first fine-tuned on synthetic ShapeNet vessels and further refined with a diverse custom dataset of 3D ships, bridging the domain gap between synthetic and real-world imagery. We integrate a state-of-the-art segmentation module based on YOLOv8 and custom preprocessing to ensure compatibility with the reconstruction network. Postprocessing steps include real-world scaling, centering, and orientation alignment, followed by georeferenced placement on an interactive web map using AIS metadata and homography-based mapping. Quantitative evaluation on synthetic validation data demonstrates strong reconstruction fidelity, while qualitative results on real maritime images from the ShipSG dataset confirm the potential for transfer to operational maritime settings. The final system provides interactive 3D inspection of real ships without requiring real-world 3D annotations. This pipeline provides an efficient, scalable solution for maritime monitoring and highlights a path toward real-time 3D ship visualization in practical applications. Interactive demo: https://dlr-mi.github.io/ship3d-demo/.",
    "arxiv_url": "https://arxiv.org/abs/2601.21786v1",
    "pdf_url": "https://arxiv.org/pdf/2601.21786v1",
    "published_date": "2026-01-29",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "segmentation",
      "ar",
      "mapping",
      "3d gaussian",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.21786v1",
      "pdf": "https://arxiv.org/pdf/2601.21786v1",
      "demo": "https://dlr-mi.github.io/ship3d-demo"
    },
    "bibtex": ""
  },
  {
    "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields",
    "authors": [
      "Shiqian Li",
      "Ruihong Shen",
      "Junfeng Ni",
      "Chang Pan",
      "Chi Zhang",
      "Yixin Zhu"
    ],
    "abstract": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF's strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models.",
    "arxiv_url": "https://arxiv.org/abs/2602.00148v1",
    "pdf_url": "https://arxiv.org/pdf/2602.00148v1",
    "published_date": "2026-01-29",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "ar",
      "3d gaussian",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.00148v1",
      "pdf": "https://arxiv.org/pdf/2602.00148v1"
    },
    "bibtex": ""
  },
  {
    "title": "Lightweight High-Fidelity Low-Bitrate Talking Face Compression for 3D Video Conference",
    "authors": [
      "Jianglong Li",
      "Jun Xu",
      "Bingcong Lu",
      "Zhengxue Cheng",
      "Hongwei Hu",
      "Ronghua Wu",
      "Li Song"
    ],
    "abstract": "The demand for immersive and interactive communication has driven advancements in 3D video conferencing, yet achieving high-fidelity 3D talking face representation at low bitrates remains a challenge. Traditional 2D video compression techniques fail to preserve fine-grained geometric and appearance details, while implicit neural rendering methods like NeRF suffer from prohibitive computational costs. To address these challenges, we propose a lightweight, high-fidelity, low-bitrate 3D talking face compression framework that integrates FLAME-based parametric modeling with 3DGS neural rendering. Our approach transmits only essential facial metadata in real time, enabling efficient reconstruction with a Gaussian-based head model. Additionally, we introduce a compact representation and compression scheme, including Gaussian attribute compression and MLP optimization, to enhance transmission efficiency. Experimental results demonstrate that our method achieves superior rate-distortion performance, delivering high-quality facial rendering at extremely low bitrates, making it well-suited for real-time 3D video conferencing applications.",
    "arxiv_url": "https://arxiv.org/abs/2601.21269v1",
    "pdf_url": "https://arxiv.org/pdf/2601.21269v1",
    "published_date": "2026-01-29",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "compact",
      "high-fidelity",
      "lightweight",
      "face",
      "compression",
      "neural rendering",
      "ar",
      "nerf",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.21269v1",
      "pdf": "https://arxiv.org/pdf/2601.21269v1"
    },
    "bibtex": ""
  },
  {
    "title": "FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models",
    "authors": [
      "Hongyu Zhou",
      "Zisen Shao",
      "Sheng Miao",
      "Pan Wang",
      "Dongfeng Bai",
      "Bingbing Liu",
      "Yiyi Liao"
    ],
    "abstract": "Neural Radiance Fields and 3D Gaussian Splatting have advanced novel view synthesis, yet still rely on dense inputs and often degrade at extrapolated views. Recent approaches leverage generative models, such as diffusion models, to provide additional supervision, but face a trade-off between generalization and fidelity: fine-tuning diffusion models for artifact removal improves fidelity but risks overfitting, while fine-tuning-free methods preserve generalization but often yield lower fidelity. We introduce FreeFix, a fine-tuning-free approach that pushes the boundary of this trade-off by enhancing extrapolated rendering with pretrained image diffusion models. We present an interleaved 2D-3D refinement strategy, showing that image diffusion models can be leveraged for consistent refinement without relying on costly video diffusion models. Furthermore, we take a closer look at the guidance signal for 2D refinement and propose a per-pixel confidence mask to identify uncertain regions for targeted improvement. Experiments across multiple datasets show that FreeFix improves multi-frame consistency and achieves performance comparable to or surpassing fine-tuning-based methods, while retaining strong generalization ability.",
    "arxiv_url": "https://arxiv.org/abs/2601.20857v1",
    "pdf_url": "https://arxiv.org/pdf/2601.20857v1",
    "published_date": "2026-01-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "face",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.20857v1",
      "pdf": "https://arxiv.org/pdf/2601.20857v1"
    },
    "bibtex": ""
  },
  {
    "title": "GRTX: Efficient Ray Tracing for 3D Gaussian-Based Rendering",
    "authors": [
      "Junseo Lee",
      "Sangyun Jeon",
      "Jungi Lee",
      "Junyong Park",
      "Jaewoong Sim"
    ],
    "abstract": "3D Gaussian Splatting has gained widespread adoption across diverse applications due to its exceptional rendering performance and visual quality. While most existing methods rely on rasterization to render Gaussians, recent research has started investigating ray tracing approaches to overcome the fundamental limitations inherent in rasterization. However, current Gaussian ray tracing methods suffer from inefficiencies such as bloated acceleration structures and redundant node traversals, which greatly degrade ray tracing performance.   In this work, we present GRTX, a set of software and hardware optimizations that enable efficient ray tracing for 3D Gaussian-based rendering. First, we introduce a novel approach for constructing streamlined acceleration structures for Gaussian primitives. Our key insight is that anisotropic Gaussians can be treated as unit spheres through ray space transformations, which substantially reduces BVH size and traversal overhead. Second, we propose dedicated hardware support for traversal checkpointing within ray tracing units. This eliminates redundant node visits during multi-round tracing by resuming traversal from checkpointed nodes rather than restarting from the root node in each subsequent round. Our evaluation shows that GRTX significantly improves ray tracing performance compared to the baseline ray tracing method with a negligible hardware cost.",
    "arxiv_url": "https://arxiv.org/abs/2601.20429v1",
    "pdf_url": "https://arxiv.org/pdf/2601.20429v1",
    "published_date": "2026-01-28",
    "categories": [
      "cs.GR",
      "cs.AR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "ray tracing",
      "acceleration",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.20429v1",
      "pdf": "https://arxiv.org/pdf/2601.20429v1"
    },
    "bibtex": ""
  },
  {
    "title": "GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction",
    "authors": [
      "Mai Su",
      "Qihan Yu",
      "Zhongtao Wang",
      "Yilong Li",
      "Chengwei Pan",
      "Yisong Chen",
      "Guoping Wang"
    ],
    "abstract": "3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: https://github.com/GVGScode/GVGS.",
    "arxiv_url": "https://arxiv.org/abs/2601.20331v1",
    "pdf_url": "https://arxiv.org/pdf/2601.20331v1",
    "published_date": "2026-01-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/GVGScode/GVGS",
    "keywords": [
      "face",
      "geometry",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.20331v1",
      "pdf": "https://arxiv.org/pdf/2601.20331v1",
      "github": "https://github.com/GVGScode/GVGS"
    },
    "bibtex": ""
  },
  {
    "title": "Graphical X Splatting (GraphiXS): A Graphical Model for 4D Gaussian Splatting under Uncertainty",
    "authors": [
      "Doga Yilmaz",
      "Jialin Zhu",
      "Deshan Gong",
      "He Wang"
    ],
    "abstract": "We propose a new framework to systematically incorporate data uncertainty in Gaussian Splatting. Being the new paradigm of neural rendering, Gaussian Splatting has been investigated in many applications, with the main effort in extending its representation, improving its optimization process, and accelerating its speed. However, one orthogonal, much needed, but under-explored area is data uncertainty. In standard 4D Gaussian Splatting, data uncertainty can manifest as view sparsity, missing frames, camera asynchronization, etc. So far, there has been little research to holistically incorporating various types of data uncertainty under a single framework. To this end, we propose Graphical X Splatting, or GraphiXS, a new probabilistic framework that considers multiple types of data uncertainty, aiming for a fundamental augmentation of the current 4D Gaussian Splatting paradigm into a probabilistic setting. GraphiXS is general and can be instantiated with a range of primitives, e.g. Gaussians, Student's-t. Furthermore, GraphiXS can be used to `upgrade' existing methods to accommodate data uncertainty. Through exhaustive evaluation and comparison, we demonstrate that GraphiXS can systematically model various uncertainties in data, outperform existing methods in many settings where data are missing or polluted in space and time, and therefore is a major generalization of the current 4D Gaussian Splatting research.",
    "arxiv_url": "https://arxiv.org/abs/2601.19843v1",
    "pdf_url": "https://arxiv.org/pdf/2601.19843v1",
    "published_date": "2026-01-27",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "neural rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.19843v1",
      "pdf": "https://arxiv.org/pdf/2601.19843v1"
    },
    "bibtex": ""
  },
  {
    "title": "WaterClear-GS: Optical-Aware Gaussian Splatting for Underwater Reconstruction and Restoration",
    "authors": [
      "Xinrui Zhang",
      "Yufeng Wang",
      "Shuangkang Fang",
      "Zesheng Wang",
      "Dacheng Qi",
      "Wenrui Ding"
    ],
    "abstract": "Underwater 3D reconstruction and appearance restoration are hindered by the complex optical properties of water, such as wavelength-dependent attenuation and scattering. Existing Neural Radiance Fields (NeRF)-based methods struggle with slow rendering speeds and suboptimal color restoration, while 3D Gaussian Splatting (3DGS) inherently lacks the capability to model complex volumetric scattering effects. To address these issues, we introduce WaterClear-GS, the first pure 3DGS-based framework that explicitly integrates underwater optical properties of local attenuation and scattering into Gaussian primitives, eliminating the need for an auxiliary medium network. Our method employs a dual-branch optimization strategy to ensure underwater photometric consistency while naturally recovering water-free appearances. This strategy is enhanced by depth-guided geometry regularization and perception-driven image loss, together with exposure constraints, spatially-adaptive regularization, and physically guided spectral regularization, which collectively enforce local 3D coherence and maintain natural visual perception. Experiments on standard benchmarks and our newly collected dataset demonstrate that WaterClear-GS achieves outstanding performance on both novel view synthesis (NVS) and underwater image restoration (UIR) tasks, while maintaining real-time rendering. The code will be available at https://buaaxrzhang.github.io/WaterClear-GS/.",
    "arxiv_url": "https://arxiv.org/abs/2601.19753v1",
    "pdf_url": "https://arxiv.org/pdf/2601.19753v1",
    "published_date": "2026-01-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "real-time rendering",
      "3d reconstruction",
      "geometry",
      "ar",
      "nerf",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.19753v1",
      "pdf": "https://arxiv.org/pdf/2601.19753v1",
      "project": "https://buaaxrzhang.github.io/WaterClear-GS"
    },
    "bibtex": ""
  },
  {
    "title": "DiffStyle3D: Consistent 3D Gaussian Stylization via Attention Optimization",
    "authors": [
      "Yitong Yang",
      "Xuexin Liu",
      "Yinglin Wang",
      "Jing Wang",
      "Hao Dou",
      "Changshuo Wang",
      "Shuting He"
    ],
    "abstract": "3D style transfer enables the creation of visually expressive 3D content, enriching the visual appearance of 3D scenes and objects. However, existing VGG- and CLIP-based methods struggle to model multi-view consistency within the model itself, while diffusion-based approaches can capture such consistency but rely on denoising directions, leading to unstable training. To address these limitations, we propose DiffStyle3D, a novel diffusion-based paradigm for 3DGS style transfer that directly optimizes in the latent space. Specifically, we introduce an Attention-Aware Loss that performs style transfer by aligning style features in the self-attention space, while preserving original content through content feature alignment. Inspired by the geometric invariance of 3D stylization, we propose a Geometry-Guided Multi-View Consistency method that integrates geometric information into self-attention to enable cross-view correspondence modeling. Based on geometric information, we additionally construct a geometry-aware mask to prevent redundant optimization in overlapping regions across views, which further improves multi-view consistency. Extensive experiments show that DiffStyle3D outperforms state-of-the-art methods, achieving higher stylization quality and visual realism.",
    "arxiv_url": "https://arxiv.org/abs/2601.19717v1",
    "pdf_url": "https://arxiv.org/pdf/2601.19717v1",
    "published_date": "2026-01-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.19717v1",
      "pdf": "https://arxiv.org/pdf/2601.19717v1"
    },
    "bibtex": ""
  },
  {
    "title": "Fast Converging 3D Gaussian Splatting for 1-Minute Reconstruction",
    "authors": [
      "Ziyu Zhang",
      "Tianle Liu",
      "Diantao Tu",
      "Shuhan Shen"
    ],
    "abstract": "We present a fast 3DGS reconstruction pipeline designed to converge within one minute, developed for the SIGGRAPH Asia 3DGS Fast Reconstruction Challenge. The challenge consists of an initial round using SLAM-generated camera poses (with noisy trajectories) and a final round using COLMAP poses (highly accurate). To robustly handle these heterogeneous settings, we develop a two-stage solution. In the first round, we use reverse per-Gaussian parallel optimization and compact forward splatting based on Taming-GS and Speedy-splat, load-balanced tiling, an anchor-based Neural-Gaussian representation enabling rapid convergence with fewer learnable parameters, initialization from monocular depth and partially from feed-forward 3DGS models, and a global pose refinement module for noisy SLAM trajectories. In the final round, the accurate COLMAP poses change the optimization landscape; we disable pose refinement, revert from Neural-Gaussians back to standard 3DGS to eliminate MLP inference overhead, introduce multi-view consistency-guided Gaussian splitting inspired by Fast-GS, and introduce a depth estimator to supervise the rendered depth. Together, these techniques enable high-fidelity reconstruction under a strict one-minute budget. Our method achieved the top performance with a PSNR of 28.43 and ranked first in the competition.",
    "arxiv_url": "https://arxiv.org/abs/2601.19489v2",
    "pdf_url": "https://arxiv.org/pdf/2601.19489v2",
    "published_date": "2026-01-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "compact",
      "high-fidelity",
      "ar",
      "3d gaussian",
      "slam",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.19489v2",
      "pdf": "https://arxiv.org/pdf/2601.19489v2"
    },
    "bibtex": ""
  },
  {
    "title": "ClipGS-VR: Immersive and Interactive Cinematic Visualization of Volumetric Medical Data in Mobile Virtual Reality",
    "authors": [
      "Yuqi Tong",
      "Ruiyang Li",
      "Chengkun Li",
      "Qixuan Liu",
      "Shi Qiu",
      "Pheng-Ann Heng"
    ],
    "abstract": "High-fidelity cinematic medical visualization on mobile virtual reality (VR) remains challenging. Although ClipGS enables cross-sectional exploration via 3D Gaussian Splatting, it lacks arbitrary-angle slicing on consumer-grade VR headsets. To achieve real-time interactive performance, we introduce ClipGS-VR and restructure ClipGS's neural inference into a consolidated dataset, integrating high-fidelity layers from multiple pre-computed slicing states into a unified rendering structure. Our framework further supports arbitrary-angle slicing via gradient-based opacity modulation for smooth, visually coherent rendering. Evaluations confirm our approach maintains visual fidelity comparable to offline results while offering superior usability and interaction efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2601.19310v1",
    "pdf_url": "https://arxiv.org/pdf/2601.19310v1",
    "published_date": "2026-01-27",
    "categories": [
      "cs.GR",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "vr",
      "high-fidelity",
      "medical",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.19310v1",
      "pdf": "https://arxiv.org/pdf/2601.19310v1"
    },
    "bibtex": ""
  },
  {
    "title": "TIGaussian: Disentangle Gaussians for Spatial-Awared Text-Image-3D Alignment",
    "authors": [
      "Jiarun Liu",
      "Qifeng Chen",
      "Yiru Zhao",
      "Minghua Liu",
      "Baorui Ma",
      "Sheng Yang"
    ],
    "abstract": "While visual-language models have profoundly linked features between texts and images, the incorporation of 3D modality data, such as point clouds and 3D Gaussians, further enables pretraining for 3D-related tasks, e.g., cross-modal retrieval, zero-shot classification, and scene recognition. As challenges remain in extracting 3D modal features and bridging the gap between different modalities, we propose TIGaussian, a framework that harnesses 3D Gaussian Splatting (3DGS) characteristics to strengthen cross-modality alignment through multi-branch 3DGS tokenizer and modality-specific 3D feature alignment strategies. Specifically, our multi-branch 3DGS tokenizer decouples the intrinsic properties of 3DGS structures into compact latent representations, enabling more generalizable feature extraction. To further bridge the modality gap, we develop a bidirectional cross-modal alignment strategies: a multi-view feature fusion mechanism that leverages diffusion priors to resolve perspective ambiguity in image-3D alignment, while a text-3D projection module adaptively maps 3D features to text embedding space for better text-3D alignment. Extensive experiments on various datasets demonstrate the state-of-the-art performance of TIGaussian in multiple tasks.",
    "arxiv_url": "https://arxiv.org/abs/2601.19247v1",
    "pdf_url": "https://arxiv.org/pdf/2601.19247v1",
    "published_date": "2026-01-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "recognition",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.19247v1",
      "pdf": "https://arxiv.org/pdf/2601.19247v1"
    },
    "bibtex": ""
  },
  {
    "title": "UniMGS: Unifying Mesh and 3D Gaussian Splatting with Single-Pass Rasterization and Proxy-Based Deformation",
    "authors": [
      "Zeyu Xiao",
      "Mingyang Sun",
      "Yimin Cong",
      "Lintao Wang",
      "Dongliang Kou",
      "Zhenyi Wu",
      "Dingkang Yang",
      "Peng Zhai",
      "Zeyu Wang",
      "Lihua Zhang"
    ],
    "abstract": "Joint rendering and deformation of mesh and 3D Gaussian Splatting (3DGS) have significant value as both representa tions offer complementary advantages for graphics applica tions. However, due to differences in representation and ren dering pipelines, existing studies render meshes and 3DGS separately, making it difficult to accurately handle occlusions and transparency. Moreover, the deformed 3DGS still suffers from visual artifacts due to the sensitivity to the topology quality of the proxy mesh. These issues pose serious obsta cles to the joint use of 3DGS and meshes, making it diffi cult to adapt 3DGS to conventional mesh-oriented graphics pipelines. We propose UniMGS, the first unified framework for rasterizing mesh and 3DGS in a single-pass anti-aliased manner, with a novel binding strategy for 3DGS deformation based on proxy mesh. Our key insight is to blend the col ors of both triangle and Gaussian fragments by anti-aliased α-blending in a single pass, achieving visually coherent re sults with precise handling of occlusion and transparency. To improve the visual appearance of the deformed 3DGS, our Gaussian-centric binding strategy employs a proxy mesh and spatially associates Gaussians with the mesh faces, signifi cantly reducing rendering artifacts. With these two compo nents, UniMGS enables the visualization and manipulation of 3D objects represented by mesh or 3DGS within a unified framework, opening up new possibilities in embodied AI, vir tual reality, and gaming. We will release our source code to facilitate future research.",
    "arxiv_url": "https://arxiv.org/abs/2601.19233v1",
    "pdf_url": "https://arxiv.org/pdf/2601.19233v1",
    "published_date": "2026-01-27",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "deformation",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.19233v1",
      "pdf": "https://arxiv.org/pdf/2601.19233v1"
    },
    "bibtex": ""
  },
  {
    "title": "Bridging Visual and Wireless Sensing: A Unified Radiation Field for 3D Radio Map Construction",
    "authors": [
      "Chaozheng Wen",
      "Jingwen Tong",
      "Zehong Lin",
      "Chenghong Bian",
      "Jun Zhang"
    ],
    "abstract": "The emerging applications of next-generation wireless networks (e.g., immersive 3D communication, low-altitude networks, and integrated sensing and communication) necessitate high-fidelity environmental intelligence. 3D radio maps have emerged as a critical tool for this purpose, enabling spectrum-aware planning and environment-aware sensing by bridging the gap between physical environments and electromagnetic signal propagation. However, constructing accurate 3D radio maps requires fine-grained 3D geometric information and a profound understanding of electromagnetic wave propagation. Existing approaches typically treat optical and wireless knowledge as distinct modalities, failing to exploit the fundamental physical principles governing both light and electromagnetic propagation. To bridge this gap, we propose URF-GS, a unified radio-optical radiation field representation framework for accurate and generalizable 3D radio map construction based on 3D Gaussian splatting (3D-GS) and inverse rendering. By fusing visual and wireless sensing observations, URF-GS recovers scene geometry and material properties while accurately predicting radio signal behavior at arbitrary transmitter-receiver (Tx-Rx) configurations. Experimental results demonstrate that URF-GS achieves up to a 24.7% improvement in spatial spectrum prediction accuracy and a 10x increase in sample efficiency for 3D radio map construction compared with neural radiance field (NeRF)-based methods. This work establishes a foundation for next-generation wireless networks by integrating perception, interaction, and communication through holistic radiation field reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2601.19216v1",
    "pdf_url": "https://arxiv.org/pdf/2601.19216v1",
    "published_date": "2026-01-27",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "high-fidelity",
      "geometry",
      "ar",
      "nerf",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.19216v1",
      "pdf": "https://arxiv.org/pdf/2601.19216v1"
    },
    "bibtex": ""
  },
  {
    "title": "Uncertainty-Aware 3D Emotional Talking Face Synthesis with Emotion Prior Distillation",
    "authors": [
      "Nanhan Shen",
      "Zhilei Liu"
    ],
    "abstract": "Emotional Talking Face synthesis is pivotal in multimedia and signal processing, yet existing 3D methods suffer from two critical challenges: poor audio-vision emotion alignment, manifested as difficult audio emotion extraction and inadequate control over emotional micro-expressions; and a one-size-fits-all multi-view fusion strategy that overlooks uncertainty and feature quality differences, undermining rendering quality. We propose UA-3DTalk, Uncertainty-Aware 3D Emotional Talking Face Synthesis with emotion prior distillation, which has three core modules: the Prior Extraction module disentangles audio into content-synchronized features for alignment and person-specific complementary features for individualization; the Emotion Distillation module introduces a multi-modal attention-weighted fusion mechanism and 4D Gaussian encoding with multi-resolution code-books, enabling fine-grained audio emotion extraction and precise control of emotional micro-expressions; the Uncertainty-based Deformation deploys uncertainty blocks to estimate view-specific aleatoric (input noise) and epistemic (model parameters) uncertainty, realizing adaptive multi-view fusion and incorporating a multi-head decoder for Gaussian primitive optimization to mitigate the limitations of uniform-weight fusion. Extensive experiments on regular and emotional datasets show UA-3DTalk outperforms state-of-the-art methods like DEGSTalk and EDTalk by 5.2% in E-FID for emotion alignment, 3.1% in SyncC for lip synchronization, and 0.015 in LPIPS for rendering quality. Project page: https://mrask999.github.io/UA-3DTalk",
    "arxiv_url": "https://arxiv.org/abs/2601.19112v1",
    "pdf_url": "https://arxiv.org/pdf/2601.19112v1",
    "published_date": "2026-01-27",
    "categories": [
      "cs.AI",
      "cs.MM",
      "cs.SD"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "head",
      "face",
      "deformation",
      "ar",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.19112v1",
      "pdf": "https://arxiv.org/pdf/2601.19112v1",
      "project": "https://mrask999.github.io/UA-3DTalk"
    },
    "bibtex": ""
  },
  {
    "title": "Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting",
    "authors": [
      "Tong Shi",
      "Melonie de Almeida",
      "Daniela Ivanova",
      "Nicolas Pugeault",
      "Paul Henderson"
    ],
    "abstract": "Talking Head Generation aims at synthesizing natural-looking talking videos from speech and a single portrait image. Previous 3D talking head generation methods have relied on domain-specific heuristics such as warping-based facial motion representation priors to animate talking motions, yet still produce inaccurate 3D avatar reconstructions, thus undermining the realism of generated animations. We introduce Splat-Portrait, a Gaussian-splatting-based method that addresses the challenges of 3D head reconstruction and lip motion synthesis. Our approach automatically learns to disentangle a single portrait image into a static 3D reconstruction represented as static Gaussian Splatting, and a predicted whole-image 2D background. It then generates natural lip motion conditioned on input audio, without any motion driven priors. Training is driven purely by 2D reconstruction and score-distillation losses, without 3D supervision nor landmarks. Experimental results demonstrate that Splat-Portrait exhibits superior performance on talking head generation and novel view synthesis, achieving better visual quality compared to previous works. Our project code and supplementary documents are public available at https://github.com/stonewalking/Splat-portrait.",
    "arxiv_url": "https://arxiv.org/abs/2601.18633v1",
    "pdf_url": "https://arxiv.org/pdf/2601.18633v1",
    "published_date": "2026-01-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/stonewalking/Splat-portrait",
    "keywords": [
      "head",
      "animation",
      "avatar",
      "3d reconstruction",
      "gaussian splatting",
      "ar",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.18633v1",
      "pdf": "https://arxiv.org/pdf/2601.18633v1",
      "github": "https://github.com/stonewalking/Splat-portrait"
    },
    "bibtex": ""
  },
  {
    "title": "ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection",
    "authors": [
      "Yiming Wang",
      "Ruogu Zhang",
      "Minyang Li",
      "Hao Shi",
      "Junbo Wang",
      "Deyi Li",
      "Jieji Ren",
      "Wenhai Liu",
      "Weiming Wang",
      "Hao-Shu Fang"
    ],
    "abstract": "Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on https://github.com/zaixiabalala/ExoGS.",
    "arxiv_url": "https://arxiv.org/abs/2601.18629v1",
    "pdf_url": "https://arxiv.org/pdf/2601.18629v1",
    "published_date": "2026-01-26",
    "categories": [
      "cs.RO"
    ],
    "github_url": "https://github.com/zaixiabalala/ExoGS",
    "keywords": [
      "4d",
      "dynamic",
      "lightweight",
      "geometry",
      "ar",
      "semantic",
      "human",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.18629v1",
      "pdf": "https://arxiv.org/pdf/2601.18629v1",
      "github": "https://github.com/zaixiabalala/ExoGS"
    },
    "bibtex": ""
  },
  {
    "title": "LoD-Structured 3D Gaussian Splatting for Streaming Video Reconstruction",
    "authors": [
      "Xinhui Liu",
      "Can Wang",
      "Lei Liu",
      "Zhenghao Chen",
      "Wei Jiang",
      "Wei Wang",
      "Dong Xu"
    ],
    "abstract": "Free-Viewpoint Video (FVV) reconstruction enables photorealistic and interactive 3D scene visualization; however, real-time streaming is often bottlenecked by sparse-view inputs, prohibitive training costs, and bandwidth constraints. While recent 3D Gaussian Splatting (3DGS) has advanced FVV due to its superior rendering speed, Streaming Free-Viewpoint Video (SFVV) introduces additional demands for rapid optimization, high-fidelity reconstruction under sparse constraints, and minimal storage footprints. To bridge this gap, we propose StreamLoD-GS, an LoD-based Gaussian Splatting framework designed specifically for SFVV. Our approach integrates three core innovations: 1) an Anchor- and Octree-based LoD-structured 3DGS with a hierarchical Gaussian dropout technique to ensure efficient and stable optimization while maintaining high-quality rendering; 2) a GMM-based motion partitioning mechanism that separates dynamic and static content, refining dynamic regions while preserving background stability; and 3) a quantized residual refinement framework that significantly reduces storage requirements without compromising visual fidelity. Extensive experiments demonstrate that StreamLoD-GS achieves competitive or state-of-the-art performance in terms of quality, efficiency, and storage.",
    "arxiv_url": "https://arxiv.org/abs/2601.18475v1",
    "pdf_url": "https://arxiv.org/pdf/2601.18475v1",
    "published_date": "2026-01-26",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "dynamic",
      "high-fidelity",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "efficient",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.18475v1",
      "pdf": "https://arxiv.org/pdf/2601.18475v1"
    },
    "bibtex": ""
  },
  {
    "title": "Geometry-Grounded Gaussian Splatting",
    "authors": [
      "Baowen Zhang",
      "Chenxing Jiang",
      "Heng Li",
      "Shaojie Shen",
      "Ping Tan"
    ],
    "abstract": "Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets.",
    "arxiv_url": "https://arxiv.org/abs/2601.17835v2",
    "pdf_url": "https://arxiv.org/pdf/2601.17835v2",
    "published_date": "2026-01-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "shape reconstruction",
      "ar",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.17835v2",
      "pdf": "https://arxiv.org/pdf/2601.17835v2"
    },
    "bibtex": ""
  },
  {
    "title": "Advancing Structured Priors for Sparse-Voxel Surface Reconstruction",
    "authors": [
      "Ting-Hsun Chi",
      "Chu-Rong Chen",
      "Chi-Tun Hsu",
      "Hsuan-Ting Lin",
      "Sheng-Yu Huang",
      "Cheng Sun",
      "Yu-Chiang Frank Wang"
    ],
    "abstract": "Reconstructing accurate surfaces with radiance fields has progressed rapidly, yet two promising explicit representations, 3D Gaussian Splatting and sparse-voxel rasterization, exhibit complementary strengths and weaknesses. 3D Gaussian Splatting converges quickly and carries useful geometric priors, but surface fidelity is limited by its point-like parameterization. Sparse-voxel rasterization provides continuous opacity fields and crisp geometry, but its typical uniform dense-grid initialization slows convergence and underutilizes scene structure. We combine the advantages of both by introducing a voxel initialization method that places voxels at plausible locations and with appropriate levels of detail, yielding a strong starting point for per-scene optimization. To further enhance depth consistency without blurring edges, we propose refined depth geometry supervision that converts multi-view cues into direct per-ray depth regularization. Experiments on standard benchmarks demonstrate improvements over prior methods in geometric accuracy, better fine-structure recovery, and more complete surfaces, while maintaining fast convergence.",
    "arxiv_url": "https://arxiv.org/abs/2601.17720v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17720v1",
    "published_date": "2026-01-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "ar",
      "3d gaussian",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.17720v1",
      "pdf": "https://arxiv.org/pdf/2601.17720v1"
    },
    "bibtex": ""
  },
  {
    "title": "PocketGS: On-Device Training of 3D Gaussian Splatting for High Perceptual Modeling",
    "authors": [
      "Wenzhi Guo",
      "Guangchi Fang",
      "Shu Yang",
      "Bing Wang"
    ],
    "abstract": "Efficient and high-fidelity 3D scene modeling is a long-standing pursuit in computer graphics. While recent 3D Gaussian Splatting (3DGS) methods achieve impressive real-time modeling performance, they rely on resource-unconstrained training assumptions that fail on mobile devices, which are limited by minute-scale training budgets and hardware-available peak-memory. We present PocketGS, a mobile scene modeling paradigm that enables on-device 3DGS training under these tightly coupled constraints while preserving high perceptual fidelity. Our method resolves the fundamental contradictions of standard 3DGS through three co-designed operators: G builds geometry-faithful point-cloud priors; I injects local surface statistics to seed anisotropic Gaussians, thereby reducing early conditioning gaps; and T unrolls alpha compositing with cached intermediates and index-mapped gradient scattering for stable mobile backpropagation. Collectively, these operators satisfy the competing requirements of training efficiency, memory compactness, and modeling fidelity. Extensive experiments demonstrate that PocketGS is able to outperform the powerful mainstream workstation 3DGS baseline to deliver high-quality reconstructions, enabling a fully on-device, practical capture-to-rendering workflow.",
    "arxiv_url": "https://arxiv.org/abs/2601.17354v2",
    "pdf_url": "https://arxiv.org/pdf/2601.17354v2",
    "published_date": "2026-01-24",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "high-fidelity",
      "face",
      "geometry",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.17354v2",
      "pdf": "https://arxiv.org/pdf/2601.17354v2"
    },
    "bibtex": ""
  },
  {
    "title": "Mirage2Matter: A Physically Grounded Gaussian World Model from Video",
    "authors": [
      "Zhengqing Gao",
      "Ziwen Li",
      "Xin Wang",
      "Jiaxin Huang",
      "Zhenyang Ren",
      "Mingkai Shao",
      "Hanlue Zhang",
      "Tianyu Huang",
      "Yongkang Cheng",
      "Yandong Guo",
      "Runqi Lin",
      "Yuanyuan Wang",
      "Tongliang Liu",
      "Kun Zhang",
      "Mingming Gong"
    ],
    "abstract": "The scalability of embodied intelligence is fundamentally constrained by the scarcity of real-world interaction data. While simulation platforms provide a promising alternative, existing approaches often suffer from a substantial visual and physical gap to real environments and rely on expensive sensors, precise robot calibration, or depth measurements, limiting their practicality at scale. We present Simulate Anything, a graphics-driven world modeling and simulation framework that enables efficient generation of high-fidelity embodied training data using only multi-view environment videos and off-the-shelf assets. Our approach reconstructs real-world environments into a photorealistic scene representation using 3D Gaussian Splatting (3DGS), seamlessly capturing fine-grained geometry and appearance from video. We then leverage generative models to recover a physically realistic representation and integrate it into a simulation environment via a precision calibration target, enabling accurate scale alignment between the reconstructed scene and the real world. Together, these components provide a unified, editable, and physically grounded world model. Vision Language Action (VLA) models trained on our simulated data achieve strong zero-shot performance on downstream tasks, matching or even surpassing results obtained with real-world data, highlighting the potential of reconstruction-driven world modeling for scalable and practical embodied intelligence training.",
    "arxiv_url": "https://arxiv.org/abs/2602.00096v1",
    "pdf_url": "https://arxiv.org/pdf/2602.00096v1",
    "published_date": "2026-01-24",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "geometry",
      "ar",
      "efficient",
      "3d gaussian",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.00096v1",
      "pdf": "https://arxiv.org/pdf/2602.00096v1"
    },
    "bibtex": ""
  },
  {
    "title": "LGDWT-GS: Local and Global Discrete Wavelet-Regularized 3D Gaussian Splatting for Sparse-View Scene Reconstruction",
    "authors": [
      "Shima Salehi",
      "Atharva Agashe",
      "Andrew J. McFarland",
      "Joshua Peeples"
    ],
    "abstract": "We propose a new method for few-shot 3D reconstruction that integrates global and local frequency regularization to stabilize geometry and preserve fine details under sparse-view conditions, addressing a key limitation of existing 3D Gaussian Splatting (3DGS) models. We also introduce a new multispectral greenhouse dataset containing four spectral bands captured from diverse plant species under controlled conditions. Alongside the dataset, we release an open-source benchmarking package that defines standardized few-shot reconstruction protocols for evaluating 3DGS-based methods. Experiments on our multispectral dataset, as well as standard benchmarks, demonstrate that the proposed method achieves sharper, more stable, and spectrally consistent reconstructions than existing baselines. The dataset and code for this work are publicly available",
    "arxiv_url": "https://arxiv.org/abs/2601.17185v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17185v1",
    "published_date": "2026-01-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "few-shot",
      "3d reconstruction",
      "geometry",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.17185v1",
      "pdf": "https://arxiv.org/pdf/2601.17185v1"
    },
    "bibtex": ""
  },
  {
    "title": "A Step to Decouple Optimization in 3DGS",
    "authors": [
      "Renjie Ding",
      "Yaonan Wang",
      "Min Liu",
      "Jialin Zhu",
      "Jiazheng Wang",
      "Jiahao Zhao",
      "Wenting Shen",
      "Feixiang He",
      "Xiang Chen"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time novel view synthesis. As an explicit representation optimized through gradient propagation among primitives, optimization widely accepted in deep neural networks (DNNs) is actually adopted in 3DGS, such as synchronous weight updating and Adam with the adaptive gradient. However, considering the physical significance and specific design in 3DGS, there are two overlooked details in the optimization of 3DGS: (i) update step coupling, which induces optimizer state rescaling and costly attribute updates outside the viewpoints, and (ii) gradient coupling in the moment, which may lead to under- or over-effective regularization. Nevertheless, such a complex coupling is under-explored. After revisiting the optimization of 3DGS, we take a step to decouple it and recompose the process into: Sparse Adam, Re-State Regularization and Decoupled Attribute Regularization. Taking a large number of experiments under the 3DGS and 3DGS-MCMC frameworks, our work provides a deeper understanding of these components. Finally, based on the empirical analysis, we re-design the optimization and propose AdamW-GS by re-coupling the beneficial components, under which better optimization efficiency and representation effectiveness are achieved simultaneously.",
    "arxiv_url": "https://arxiv.org/abs/2601.16736v2",
    "pdf_url": "https://arxiv.org/pdf/2601.16736v2",
    "published_date": "2026-01-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "understanding",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.16736v2",
      "pdf": "https://arxiv.org/pdf/2601.16736v2"
    },
    "bibtex": ""
  },
  {
    "title": "ReWeaver: Towards Simulation-Ready and Topology-Accurate Garment Reconstruction",
    "authors": [
      "Ming Li",
      "Hui Shan",
      "Kai Zheng",
      "Chentao Shen",
      "Siyu Liu",
      "Yanwei Fu",
      "Zhen Chen",
      "Xiangru Huang"
    ],
    "abstract": "High-quality 3D garment reconstruction plays a crucial role in mitigating the sim-to-real gap in applications such as digital avatars, virtual try-on and robotic manipulation. However, existing garment reconstruction methods typically rely on unstructured representations, such as 3D Gaussian Splats, struggling to provide accurate reconstructions of garment topology and sewing structures. As a result, the reconstructed outputs are often unsuitable for high-fidelity physical simulation. We propose ReWeaver, a novel framework for topology-accurate 3D garment and sewing pattern reconstruction from sparse multi-view RGB images. Given as few as four input views, ReWeaver predicts seams and panels as well as their connectivities in both the 2D UV space and the 3D space. The predicted seams and panels align precisely with the multi-view images, yielding structured 2D--3D garment representations suitable for 3D perception, high-fidelity physical simulation, and robotic manipulation. To enable effective training, we construct a large-scale dataset GCD-TS, comprising multi-view RGB images, 3D garment geometries, textured human body meshes and annotated sewing patterns. The dataset contains over 100,000 synthetic samples covering a wide range of complex geometries and topologies. Extensive experiments show that ReWeaver consistently outperforms existing methods in terms of topology accuracy, geometry alignment and seam-panel consistency.",
    "arxiv_url": "https://arxiv.org/abs/2601.16672v1",
    "pdf_url": "https://arxiv.org/pdf/2601.16672v1",
    "published_date": "2026-01-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "body",
      "avatar",
      "geometry",
      "ar",
      "human",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.16672v1",
      "pdf": "https://arxiv.org/pdf/2601.16672v1"
    },
    "bibtex": ""
  },
  {
    "title": "GlassesGB: Controllable 2D GAN-Based Eyewear Personalization for 3D Gaussian Blendshapes Head Avatars",
    "authors": [
      "Rui-Yang Ju",
      "Jen-Shiun Chiang"
    ],
    "abstract": "Virtual try-on systems allow users to interactively try different products within VR scenarios. However, most existing VTON methods operate only on predefined eyewear templates and lack support for fine-grained, user-driven customization. While GlassesGAN enables personalized 2D eyewear design, its capability remains limited to 2D image generation. Motivated by the success of 3D Gaussian Blendshapes in head reconstruction, we integrate these two techniques and propose GlassesGB, a framework that supports customizable eyewear generation for 3D head avatars. GlassesGB effectively bridges 2D generative customization with 3D head avatar rendering, addressing the challenge in achieving personalized eyewear design for VR applications. The implementation code is available at https://ruiyangju.github.io/GlassesGB.",
    "arxiv_url": "https://arxiv.org/abs/2601.17088v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17088v1",
    "published_date": "2026-01-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "vr",
      "avatar",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.17088v1",
      "pdf": "https://arxiv.org/pdf/2601.17088v1",
      "project": "https://ruiyangju.github.io/GlassesGB"
    },
    "bibtex": ""
  },
  {
    "title": "CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback",
    "authors": [
      "Wenhang Ge",
      "Guibao Shen",
      "Jiawei Feng",
      "Luozhou Wang",
      "Hao Lu",
      "Xingye Tian",
      "Xin Tao",
      "Ying-Cong Chen"
    ],
    "abstract": "Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \\href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.",
    "arxiv_url": "https://arxiv.org/abs/2601.16214v1",
    "pdf_url": "https://arxiv.org/pdf/2601.16214v1",
    "published_date": "2026-01-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "face",
      "ar",
      "3d gaussian",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.16214v1",
      "pdf": "https://arxiv.org/pdf/2601.16214v1",
      "project": "https://a-bigbao.github.io/CamPilot"
    },
    "bibtex": ""
  },
  {
    "title": "EVolSplat4D: Efficient Volume-based Gaussian Splatting for 4D Urban Scene Synthesis",
    "authors": [
      "Sheng Miao",
      "Sijin Li",
      "Pan Wang",
      "Dongfeng Bai",
      "Bingbing Liu",
      "Yue Wang",
      "Andreas Geiger",
      "Yiyi Liao"
    ],
    "abstract": "Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality. While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches achieve photorealism, they often rely on time-consuming per-scene optimization. Conversely, emerging feed-forward methods frequently adopt per-pixel Gaussian representations, which lead to 3D inconsistencies when aggregating multi-view predictions in complex, dynamic environments. We propose EvolSplat4D, a feed-forward framework that moves beyond existing per-pixel paradigms by unifying volume-based and pixel-based Gaussian prediction across three specialized branches. For close-range static regions, we predict consistent geometry of 3D Gaussians over multiple frames directly from a 3D feature volume, complemented by a semantically-enhanced image-based rendering module for predicting their appearance. For dynamic actors, we utilize object-centric canonical spaces and a motion-adjusted rendering module to aggregate temporal features, ensuring stable 4D reconstruction despite noisy motion priors. Far-Field scenery is handled by an efficient per-pixel Gaussian branch to ensure full-scene coverage. Experimental results on the KITTI-360, KITTI, Waymo, and PandaSet datasets show that EvolSplat4D reconstructs both static and dynamic environments with superior accuracy and consistency, outperforming both per-scene optimization and state-of-the-art feed-forward baselines.",
    "arxiv_url": "https://arxiv.org/abs/2601.15951v1",
    "pdf_url": "https://arxiv.org/pdf/2601.15951v1",
    "published_date": "2026-01-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "urban scene",
      "dynamic",
      "motion",
      "geometry",
      "autonomous driving",
      "ar",
      "semantic",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.15951v1",
      "pdf": "https://arxiv.org/pdf/2601.15951v1"
    },
    "bibtex": ""
  },
  {
    "title": "ThermoSplat: Cross-Modal 3D Gaussian Splatting with Feature Modulation and Geometry Decoupling",
    "authors": [
      "Zhaoqi Su",
      "Shihai Chen",
      "Xinyan Lin",
      "Liqin Huang",
      "Zhipeng Su",
      "Xiaoqiang Lu"
    ],
    "abstract": "Multi-modal scene reconstruction integrating RGB and thermal infrared data is essential for robust environmental perception across diverse lighting and weather conditions. However, extending 3D Gaussian Splatting (3DGS) to multi-spectral scenarios remains challenging. Current approaches often struggle to fully leverage the complementary information of multi-modal data, typically relying on mechanisms that either tend to neglect cross-modal correlations or leverage shared representations that fail to adaptively handle the complex structural correlations and physical discrepancies between spectrums. To address these limitations, we propose ThermoSplat, a novel framework that enables deep spectral-aware reconstruction through active feature modulation and adaptive geometry decoupling. First, we introduce a Cross-Modal FiLM Modulation mechanism that dynamically conditions shared latent features on thermal structural priors, effectively guiding visible texture synthesis with reliable cross-modal geometric cues. Second, to accommodate modality-specific geometric inconsistencies, we propose a Modality-Adaptive Geometric Decoupling scheme that learns independent opacity offsets and executes an independent rasterization pass for the thermal branch. Additionally, a hybrid rendering pipeline is employed to integrate explicit Spherical Harmonics with implicit neural decoding, ensuring both semantic consistency and high-frequency detail preservation. Extensive experiments on the RGBT-Scenes dataset demonstrate that ThermoSplat achieves state-of-the-art rendering quality across both visible and thermal spectrums.",
    "arxiv_url": "https://arxiv.org/abs/2601.15897v1",
    "pdf_url": "https://arxiv.org/pdf/2601.15897v1",
    "published_date": "2026-01-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "geometry",
      "ar",
      "semantic",
      "3d gaussian",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.15897v1",
      "pdf": "https://arxiv.org/pdf/2601.15897v1"
    },
    "bibtex": ""
  },
  {
    "title": "LL-GaussianImage: Efficient Image Representation for Zero-shot Low-Light Enhancement with 2D Gaussian Splatting",
    "authors": [
      "Yuhan Chen",
      "Wenxuan Yu",
      "Guofa Li",
      "Yijun Xu",
      "Ying Fang",
      "Yicui Shi",
      "Long Cao",
      "Wenbo Chu",
      "Keqiang Li"
    ],
    "abstract": "2D Gaussian Splatting (2DGS) is an emerging explicit scene representation method with significant potential for image compression due to high fidelity and high compression ratios. However, existing low-light enhancement algorithms operate predominantly within the pixel domain. Processing 2DGS-compressed images necessitates a cumbersome decompression-enhancement-recompression pipeline, which compromises efficiency and introduces secondary degradation. To address these limitations, we propose LL-GaussianImage, the first zero-shot unsupervised framework designed for low-light enhancement directly within the 2DGS compressed representation domain. Three primary advantages are offered by this framework. First, a semantic-guided Mixture-of-Experts enhancement framework is designed. Dynamic adaptive transformations are applied to the sparse attribute space of 2DGS using rendered images as guidance to enable compression-as-enhancement without full decompression to a pixel grid. Second, a multi-objective collaborative loss function system is established to strictly constrain smoothness and fidelity during enhancement, suppressing artifacts while improving visual quality. Third, a two-stage optimization process is utilized to achieve reconstruction-as-enhancement. The accuracy of the base representation is ensured through single-scale reconstruction and network robustness is enhanced. High-quality enhancement of low-light images is achieved while high compression ratios are maintained. The feasibility and superiority of the paradigm for direct processing within the compressed representation domain are validated through experimental results.",
    "arxiv_url": "https://arxiv.org/abs/2601.15772v1",
    "pdf_url": "https://arxiv.org/pdf/2601.15772v1",
    "published_date": "2026-01-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "compression",
      "ar",
      "semantic",
      "quality enhancement",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.15772v1",
      "pdf": "https://arxiv.org/pdf/2601.15772v1"
    },
    "bibtex": ""
  },
  {
    "title": "LL-GaussianMap: Zero-shot Low-Light Image Enhancement via 2D Gaussian Splatting Guided Gain Maps",
    "authors": [
      "Yuhan Chen",
      "Ying Fang",
      "Guofa Li",
      "Wenxuan Yu",
      "Yicui Shi",
      "Jingrui Zhang",
      "Kefei Qian",
      "Wenbo Chu",
      "Keqiang Li"
    ],
    "abstract": "Significant progress has been made in low-light image enhancement with respect to visual quality. However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations. As a result, the intrinsic geometric structural priors of images are often neglected. 2D Gaussian Splatting (2DGS) has emerged as a prominent explicit scene representation technique characterized by superior structural fitting capabilities and high rendering efficiency. Despite these advantages, the utilization of 2DGS in low-level vision tasks remains unexplored. To bridge this gap, LL-GaussianMap is proposed as the first unsupervised framework incorporating 2DGS into low-light image enhancement. Distinct from conventional methodologies, the enhancement task is formulated as a gain map generation process guided by 2DGS primitives. The proposed method comprises two primary stages. First, high-fidelity structural reconstruction is executed utilizing 2DGS. Then, data-driven enhancement dictionary coefficients are rendered via the rasterization mechanism of Gaussian splatting through an innovative unified enhancement module. This design effectively incorporates the structural perception capabilities of 2DGS into gain map generation, thereby preserving edges and suppressing artifacts during enhancement. Additionally, the reliance on paired data is circumvented through unsupervised learning. Experimental results demonstrate that LL-GaussianMap achieves superior enhancement performance with an extremely low storage footprint, highlighting the effectiveness of explicit Gaussian representations for image enhancement.",
    "arxiv_url": "https://arxiv.org/abs/2601.15766v2",
    "pdf_url": "https://arxiv.org/pdf/2601.15766v2",
    "published_date": "2026-01-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "efficient",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.15766v2",
      "pdf": "https://arxiv.org/pdf/2601.15766v2"
    },
    "bibtex": ""
  },
  {
    "title": "SplatBus: A Gaussian Splatting Viewer Framework via GPU Interprocess Communication",
    "authors": [
      "Yinghan Xu",
      "Théo Morales",
      "John Dingliana"
    ],
    "abstract": "Radiance field-based rendering methods have attracted significant interest from the computer vision and computer graphics communities. They enable high-fidelity rendering with complex real-world lighting effects, but at the cost of high rendering time. 3D Gaussian Splatting solves this issue with a rasterisation-based approach for real-time rendering, enabling applications such as autonomous driving, robotics, virtual reality, and extended reality. However, current 3DGS implementations are difficult to integrate into traditional mesh-based rendering pipelines, which is a common use case for interactive applications and artistic exploration. To address this limitation, this software solution uses Nvidia's interprocess communication (IPC) APIs to easily integrate into implementations and allow the results to be viewed in external clients such as Unity, Blender, Unreal Engine, and OpenGL viewers. The code is available at https://github.com/RockyXu66/splatbus.",
    "arxiv_url": "https://arxiv.org/abs/2601.15431v1",
    "pdf_url": "https://arxiv.org/pdf/2601.15431v1",
    "published_date": "2026-01-21",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "https://github.com/RockyXu66/splatbus",
    "keywords": [
      "robotics",
      "high-fidelity",
      "real-time rendering",
      "autonomous driving",
      "ar",
      "3d gaussian",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.15431v1",
      "pdf": "https://arxiv.org/pdf/2601.15431v1",
      "github": "https://github.com/RockyXu66/splatbus"
    },
    "bibtex": ""
  },
  {
    "title": "LuxRemix: Lighting Decomposition and Remixing for Indoor Scenes",
    "authors": [
      "Ruofan Liang",
      "Norman Müller",
      "Ethan Weber",
      "Duncan Zauss",
      "Nandita Vijaykumar",
      "Peter Kontschieder",
      "Christian Richardt"
    ],
    "abstract": "We present a novel approach for interactive light editing in indoor scenes from a single multi-view scene capture. Our method leverages a generative image-based light decomposition model that factorizes complex indoor scene illumination into its constituent light sources. This factorization enables independent manipulation of individual light sources, specifically allowing control over their state (on/off), chromaticity, and intensity. We further introduce multi-view lighting harmonization to ensure consistent propagation of the lighting decomposition across all scene views. This is integrated into a relightable 3D Gaussian splatting representation, providing real-time interactive control over the individual light sources. Our results demonstrate highly photorealistic lighting decomposition and relighting outcomes across diverse indoor scenes. We evaluate our method on both synthetic and real-world datasets and provide a quantitative and qualitative comparison to state-of-the-art techniques. For video results and interactive demos, see https://luxremix.github.io.",
    "arxiv_url": "https://arxiv.org/abs/2601.15283v1",
    "pdf_url": "https://arxiv.org/pdf/2601.15283v1",
    "published_date": "2026-01-21",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "illumination",
      "ar",
      "relightable",
      "3d gaussian",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.15283v1",
      "pdf": "https://arxiv.org/pdf/2601.15283v1",
      "project": "https://luxremix.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "ScenDi: 3D-to-2D Scene Diffusion Cascades for Urban Generation",
    "authors": [
      "Hanlei Guo",
      "Jiahao Shao",
      "Xinya Chen",
      "Xiyang Tan",
      "Sheng Miao",
      "Yujun Shen",
      "Yiyi Liao"
    ],
    "abstract": "Recent advancements in 3D object generation using diffusion models have achieved remarkable success, but generating realistic 3D urban scenes remains challenging. Existing methods relying solely on 3D diffusion models tend to suffer a degradation in appearance details, while those utilizing only 2D diffusion models typically compromise camera controllability. To overcome this limitation, we propose ScenDi, a method for urban scene generation that integrates both 3D and 2D diffusion models. We first train a 3D latent diffusion model to generate 3D Gaussians, enabling the rendering of images at a relatively low resolution. To enable controllable synthesis, this 3DGS generation process can be optionally conditioned by specifying inputs such as 3d bounding boxes, road maps, or text prompts. Then, we train a 2D video diffusion model to enhance appearance details conditioned on rendered images from the 3D Gaussians. By leveraging the coarse 3D scene as guidance for 2D video diffusion, ScenDi generates desired scenes based on input conditions and successfully adheres to accurate camera trajectories. Experiments on two challenging real-world datasets, Waymo and KITTI-360, demonstrate the effectiveness of our approach.",
    "arxiv_url": "https://arxiv.org/abs/2601.15221v1",
    "pdf_url": "https://arxiv.org/pdf/2601.15221v1",
    "published_date": "2026-01-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "urban scene",
      "ar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.15221v1",
      "pdf": "https://arxiv.org/pdf/2601.15221v1"
    },
    "bibtex": ""
  },
  {
    "title": "CAG-Avatar: Cross-Attention Guided Gaussian Avatars for High-Fidelity Head Reconstruction",
    "authors": [
      "Zhe Chang",
      "Haodong Jin",
      "Yan Song",
      "Hui Yu"
    ],
    "abstract": "Creating high-fidelity, real-time drivable 3D head avatars is a core challenge in digital animation. While 3D Gaussian Splashing (3D-GS) offers unprecedented rendering speed and quality, current animation techniques often rely on a \"one-size-fits-all\" global tuning approach, where all Gaussian primitives are uniformly driven by a single expression code. This simplistic approach fails to unravel the distinct dynamics of different facial regions, such as deformable skin versus rigid teeth, leading to significant blurring and distortion artifacts. We introduce Conditionally-Adaptive Gaussian Avatars (CAG-Avatar), a framework that resolves this key limitation. At its core is a Conditionally Adaptive Fusion Module built on cross-attention. This mechanism empowers each 3D Gaussian to act as a query, adaptively extracting relevant driving signals from the global expression code based on its canonical position. This \"tailor-made\" conditioning strategy drastically enhances the modeling of fine-grained, localized dynamics. Our experiments confirm a significant improvement in reconstruction fidelity, particularly for challenging regions such as teeth, while preserving real-time rendering performance.",
    "arxiv_url": "https://arxiv.org/abs/2601.14844v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14844v1",
    "published_date": "2026-01-21",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "dynamic",
      "high-fidelity",
      "animation",
      "real-time rendering",
      "avatar",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.14844v1",
      "pdf": "https://arxiv.org/pdf/2601.14844v1"
    },
    "bibtex": ""
  },
  {
    "title": "POTR: Post-Training 3DGS Compression",
    "authors": [
      "Bert Ramlot",
      "Martijn Courteaux",
      "Peter Lambert",
      "Glenn Van Wallendael"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a promising contender to Neural Radiance Fields (NeRF) in 3D scene reconstruction and real-time novel view synthesis. 3DGS outperforms NeRF in training and inference speed but has substantially higher storage requirements. To remedy this downside, we propose POTR, a post-training 3DGS codec built on two novel techniques. First, POTR introduces a novel pruning approach that uses a modified 3DGS rasterizer to efficiently calculate every splat's individual removal effect simultaneously. This technique results in 2-4x fewer splats than other post-training pruning techniques and as a result also significantly accelerates inference with experiments demonstrating 1.5-2x faster inference than other compressed models. Second, we propose a novel method to recompute lighting coefficients, significantly reducing their entropy without using any form of training. Our fast and highly parallel approach especially increases AC lighting coefficient sparsity, with experiments demonstrating increases from 70% to 97%, with minimal loss in quality. Finally, we extend POTR with a simple fine-tuning scheme to further enhance pruning, inference, and rate-distortion performance. Experiments demonstrate that POTR, even without fine-tuning, consistently outperforms all other post-training compression techniques in both rate-distortion performance and inference speed.",
    "arxiv_url": "https://arxiv.org/abs/2601.14821v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14821v1",
    "published_date": "2026-01-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "ar",
      "efficient",
      "nerf",
      "3d gaussian",
      "lighting",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.14821v1",
      "pdf": "https://arxiv.org/pdf/2601.14821v1"
    },
    "bibtex": ""
  },
  {
    "title": "Structured Image-based Coding for Efficient Gaussian Splatting Compression",
    "authors": [
      "Pedro Martin",
      "Antonio Rodrigues",
      "Joao Ascenso",
      "Maria Paula Queluz"
    ],
    "abstract": "Gaussian Splatting (GS) has recently emerged as a state-of-the-art representation for radiance fields, combining real-time rendering with high visual fidelity. However, GS models require storing millions of parameters, leading to large file sizes that impair their use in practical multimedia systems. To address this limitation, this paper introduces GS Image-based Compression (GSICO), a novel GS codec that efficiently compresses pre-trained GS models while preserving perceptual fidelity. The core contribution lies in a mapping procedure that arranges GS parameters into structured images, guided by a novel algorithm that enhances spatial coherence. These GS parameter images are then encoded using a conventional image codec. Experimental evaluations on Tanks and Temples, Deep Blending, and Mip-NeRF360 datasets show that GSICO achieves average compression factors of 20.2x with minimal loss in visual quality, as measured by PSNR, SSIM, and LPIPS. Compared with state-of-the-art GS compression methods, the proposed codec consistently yields superior rate-distortion (RD) trade-offs.",
    "arxiv_url": "https://arxiv.org/abs/2601.14510v2",
    "pdf_url": "https://arxiv.org/pdf/2601.14510v2",
    "published_date": "2026-01-20",
    "categories": [
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "real-time rendering",
      "ar",
      "nerf",
      "mapping",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.14510v2",
      "pdf": "https://arxiv.org/pdf/2601.14510v2"
    },
    "bibtex": ""
  },
  {
    "title": "Gaussian Based Adaptive Multi-Modal 3D Semantic Occupancy Prediction",
    "authors": [
      "A. Enes Doruk"
    ],
    "abstract": "The sparse object detection paradigm shift towards dense 3D semantic occupancy prediction is necessary for dealing with long-tail safety challenges for autonomous vehicles. Nonetheless, the current voxelization methods commonly suffer from excessive computation complexity demands, where the fusion process is brittle, static, and breaks down under dynamic environmental settings. To this end, this research work enhances a novel Gaussian-based adaptive camera-LiDAR multimodal 3D occupancy prediction model that seamlessly bridges the semantic strengths of camera modality with the geometric strengths of LiDAR modality through a memory-efficient 3D Gaussian model. The proposed solution has four key components: (1) LiDAR Depth Feature Aggregation (LDFA), where depth-wise deformable sampling is employed for dealing with geometric sparsity, (2) Entropy-Based Feature Smoothing, where cross-entropy is employed for handling domain-specific noise, (3) Adaptive Camera-LiDAR Fusion, where dynamic recalibration of sensor outputs is performed based on model outputs, and (4) Gauss-Mamba Head that uses Selective State Space Models for global context decoding that enjoys linear computation complexity.",
    "arxiv_url": "https://arxiv.org/abs/2601.14448v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14448v1",
    "published_date": "2026-01-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "dynamic",
      "ar",
      "semantic",
      "3d gaussian",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.14448v1",
      "pdf": "https://arxiv.org/pdf/2601.14448v1"
    },
    "bibtex": ""
  },
  {
    "title": "Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting",
    "authors": [
      "Nitin Kulkarni",
      "Akhil Devarashetti",
      "Charlie Cluss",
      "Livio Forte",
      "Dan Buckmaster",
      "Philip Schneider",
      "Chunming Qiao",
      "Alina Vereshchaka"
    ],
    "abstract": "Inspecting the undercarriage of used vehicles is a labor-intensive task that requires inspectors to crouch or crawl underneath each vehicle to thoroughly examine it. Additionally, online buyers rarely see undercarriage photos. We present an end-to-end pipeline that utilizes a three-camera rig to capture videos of the undercarriage as the vehicle drives over it, and produces an interactive 3D model of the undercarriage. The 3D model enables inspectors and customers to rotate, zoom, and slice through the undercarriage, allowing them to detect rust, leaks, or impact damage in seconds, thereby improving both workplace safety and buyer confidence. Our primary contribution is a rig-aware Structure-from-Motion (SfM) pipeline specifically designed to overcome the challenges of wide-angle lens distortion and low-parallax scenes. Our method overcomes the challenges of wide-angle lens distortion and low-parallax scenes by integrating precise camera calibration, synchronized video streams, and strong geometric priors from the camera rig. We use a constrained matching strategy with learned components, the DISK feature extractor, and the attention-based LightGlue matcher to generate high-quality sparse point clouds that are often unattainable with standard SfM pipelines. These point clouds seed the Gaussian splatting process to generate photorealistic undercarriage models that render in real-time. Our experiments and ablation studies demonstrate that our design choices are essential to achieve state-of-the-art quality.",
    "arxiv_url": "https://arxiv.org/abs/2601.14208v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14208v1",
    "published_date": "2026-01-20",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "3d reconstruction",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.14208v1",
      "pdf": "https://arxiv.org/pdf/2601.14208v1"
    },
    "bibtex": ""
  },
  {
    "title": "One-Shot Refiner: Boosting Feed-forward Novel View Synthesis via One-Step Diffusion",
    "authors": [
      "Yitong Dong",
      "Qi Zhang",
      "Minchao Jiang",
      "Zhiqiang Wu",
      "Qingnan Fan",
      "Ying Feng",
      "Huaqi Zhang",
      "Hujun Bao",
      "Guofeng Zhang"
    ],
    "abstract": "We present a novel framework for high-fidelity novel view synthesis (NVS) from sparse images, addressing key limitations in recent feed-forward 3D Gaussian Splatting (3DGS) methods built on Vision Transformer (ViT) backbones. While ViT-based pipelines offer strong geometric priors, they are often constrained by low-resolution inputs due to computational costs. Moreover, existing generative enhancement methods tend to be 3D-agnostic, resulting in inconsistent structures across views, especially in unseen regions. To overcome these challenges, we design a Dual-Domain Detail Perception Module, which enables handling high-resolution images without being limited by the ViT backbone, and endows Gaussians with additional features to store high-frequency details. We develop a feature-guided diffusion network, which can preserve high-frequency details during the restoration process. We introduce a unified training strategy that enables joint optimization of the ViT-based geometric backbone and the diffusion-based refinement module. Experiments demonstrate that our method can maintain superior generation quality across multiple datasets.",
    "arxiv_url": "https://arxiv.org/abs/2601.14161v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14161v1",
    "published_date": "2026-01-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "high-fidelity",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.14161v1",
      "pdf": "https://arxiv.org/pdf/2601.14161v1"
    },
    "bibtex": ""
  },
  {
    "title": "FastGHA: Generalized Few-Shot 3D Gaussian Head Avatars with Real-Time Animation",
    "authors": [
      "Xinya Ji",
      "Sebastian Weiss",
      "Manuel Kansy",
      "Jacek Naruniec",
      "Xun Cao",
      "Barbara Solenthaler",
      "Derek Bradley"
    ],
    "abstract": "Despite recent progress in 3D Gaussian-based head avatar modeling, efficiently generating high fidelity avatars remains a challenge. Current methods typically rely on extensive multi-view capture setups or monocular videos with per-identity optimization during inference, limiting their scalability and ease of use on unseen subjects. To overcome these efficiency drawbacks, we propose FastGHA, a feed-forward method to generate high-quality Gaussian head avatars from only a few input images while supporting real-time animation. Our approach directly learns a per-pixel Gaussian representation from the input images, and aggregates multi-view information using a transformer-based encoder that fuses image features from both DINOv3 and Stable Diffusion VAE. For real-time animation, we extend the explicit Gaussian representations with per-Gaussian features and introduce a lightweight MLP-based dynamic network to predict 3D Gaussian deformations from expression codes. Furthermore, to enhance geometric smoothness of the 3D head, we employ point maps from a pre-trained large reconstruction model as geometry supervision. Experiments show that our approach significantly outperforms existing methods in both rendering quality and inference efficiency, while supporting real-time dynamic avatar animation.",
    "arxiv_url": "https://arxiv.org/abs/2601.13837v2",
    "pdf_url": "https://arxiv.org/pdf/2601.13837v2",
    "published_date": "2026-01-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "dynamic",
      "few-shot",
      "lightweight",
      "deformation",
      "animation",
      "avatar",
      "geometry",
      "ar",
      "3d gaussian",
      "efficient",
      "fast"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.13837v2",
      "pdf": "https://arxiv.org/pdf/2601.13837v2"
    },
    "bibtex": ""
  },
  {
    "title": "ParkingTwin: Training-Free Streaming 3D Reconstruction for Parking-Lot Digital Twins",
    "authors": [
      "Xinhao Liu",
      "Yu Wang",
      "Xiansheng Guo",
      "Gordon Owusu Boateng",
      "Yu Cao",
      "Haonan Si",
      "Xingchen Guo",
      "Nirwan Ansari"
    ],
    "abstract": "High-fidelity parking-lot digital twins provide essential priors for path planning, collision checking, and perception validation in Automated Valet Parking (AVP). Yet robot-oriented reconstruction faces a trilemma: sparse forward-facing views cause weak parallax and ill-posed geometry; dynamic occlusions and extreme lighting hinder stable texture fusion; and neural rendering typically needs expensive offline optimization, violating edge-side streaming constraints. We propose ParkingTwin, a training-free, lightweight system for online streaming 3D reconstruction. First, OSM-prior-driven geometric construction uses OpenStreetMap semantic topology to directly generate a metric-consistent TSDF, replacing blind geometric search with deterministic mapping and avoiding costly optimization. Second, geometry-aware dynamic filtering employs a quad-modal constraint field (normal/height/depth consistency) to reject moving vehicles and transient occlusions in real time. Third, illumination-robust fusion in CIELAB decouples luminance and chromaticity via adaptive L-channel weighting and depth-gradient suppression, reducing seams under abrupt lighting changes. ParkingTwin runs at 30+ FPS on an entry-level GTX 1660. On a 68,000 m^2 real-world dataset, it achieves SSIM 0.87 (+16.0%), delivers about 15x end-to-end speedup, and reduces GPU memory by 83.3% compared with state-of-the-art 3D Gaussian Splatting (3DGS) that typically requires high-end GPUs (RTX 4090D). The system outputs explicit triangle meshes compatible with Unity/Unreal digital-twin pipelines. Project page: https://mihoutao-liu.github.io/ParkingTwin/",
    "arxiv_url": "https://arxiv.org/abs/2601.13706v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13706v1",
    "published_date": "2026-01-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "dynamic",
      "high-fidelity",
      "lightweight",
      "face",
      "3d reconstruction",
      "neural rendering",
      "geometry",
      "semantic",
      "ar",
      "mapping",
      "3d gaussian",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.13706v1",
      "pdf": "https://arxiv.org/pdf/2601.13706v1",
      "project": "https://mihoutao-liu.github.io/ParkingTwin"
    },
    "bibtex": ""
  },
  {
    "title": "GaussExplorer: 3D Gaussian Splatting for Embodied Exploration and Reasoning",
    "authors": [
      "Kim Yu-Ji",
      "Dahye Lee",
      "Kim Jun-Seong",
      "GeonU Kim",
      "Nam Hyeon-Woo",
      "Yongjin Kwon",
      "Yu-Chiang Frank Wang",
      "Jaesung Choe",
      "Tae-Hyun Oh"
    ],
    "abstract": "We present GaussExplorer, a framework for embodied exploration and reasoning built on 3D Gaussian Splatting (3DGS). While prior approaches to language-embedded 3DGS have made meaningful progress in aligning simple text queries with Gaussian embeddings, they are generally optimized for relatively simple queries and struggle to interpret more complex, compositional language queries. Alternative studies based on object-centric RGB-D structured memories provide spatial grounding but are constrained by pre-fixed viewpoints. To address these issues, GaussExplorer introduces Vision-Language Models (VLMs) on top of 3DGS to enable question-driven exploration and reasoning within 3D scenes. We first identify pre-captured images that are most correlated with the query question, and subsequently adjust them into novel viewpoints to more accurately capture visual information for better reasoning by VLMs. Experiments show that ours outperforms existing methods on several benchmarks, demonstrating the effectiveness of integrating VLM-based reasoning with 3DGS for embodied tasks.",
    "arxiv_url": "https://arxiv.org/abs/2601.13132v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13132v1",
    "published_date": "2026-01-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.13132v1",
      "pdf": "https://arxiv.org/pdf/2601.13132v1"
    },
    "bibtex": ""
  },
  {
    "title": "TreeDGS: Aerial Gaussian Splatting for Distant DBH Measurement",
    "authors": [
      "Belal Shaheen",
      "Minh-Hieu Nguyen",
      "Bach-Thuan Bui",
      "Shubham",
      "Tim Wu",
      "Michael Fairley",
      "Matthew David Zane",
      "Michael Wu",
      "James Tompkin"
    ],
    "abstract": "Aerial remote sensing enables efficient large-area surveying, but accurate direct object-level measurement remains difficult in complex natural scenes. Recent advancements in 3D vision, particularly learned radiance-field representations such as NeRF and 3D Gaussian Splatting, have begun to raise the ceiling on reconstruction fidelity and densifiable geometry from posed imagery. Nevertheless, direct aerial measurement of important natural attributes such as tree diameter at breast height (DBH) remains challenging. Trunks in aerial forest scans are distant and sparsely observed in image views: at typical operating altitudes, stems may span only a few pixels. With these constraints, conventional reconstruction methods leave breast-height trunk geometry weakly constrained. We present TreeDGS, an aerial image reconstruction method that leverages 3D Gaussian Splatting as a continuous, densifiable scene representation for trunk measurement. After SfM--MVS initialization and Gaussian optimization, we extract a dense point set from the Gaussian field using RaDe-GS's depth-aware cumulative-opacity integration and associate each sample with a multi-view opacity reliability score. Then, we estimate DBH from trunk-isolated points using opacity-weighted solid-circle fitting. Evaluated on 10 plots with field-measured DBH, TreeDGS reaches 4.79,cm RMSE (about 2.6 pixels at this GSD) and outperforms a state-of-the-art LiDAR baseline (7.91,cm RMSE). This shows that TreeDGS can enable accurate, low-cost aerial DBH measurement",
    "arxiv_url": "https://arxiv.org/abs/2601.12823v2",
    "pdf_url": "https://arxiv.org/pdf/2601.12823v2",
    "published_date": "2026-01-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "survey",
      "geometry",
      "ar",
      "nerf",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.12823v2",
      "pdf": "https://arxiv.org/pdf/2601.12823v2"
    },
    "bibtex": ""
  },
  {
    "title": "CSGaussian: Progressive Rate-Distortion Compression and Segmentation for 3D Gaussian Splatting",
    "authors": [
      "Yu-Jen Tseng",
      "Chia-Hao Kao",
      "Jing-Zhong Chen",
      "Alessandro Gnutti",
      "Shao-Yuan Lo",
      "Yen-Yu Lin",
      "Wen-Hsiao Peng"
    ],
    "abstract": "We present the first unified framework for rate-distortion-optimized compression and segmentation of 3D Gaussian Splatting (3DGS). While 3DGS has proven effective for both real-time rendering and semantic scene understanding, prior works have largely treated these tasks independently, leaving their joint consideration unexplored. Inspired by recent advances in rate-distortion-optimized 3DGS compression, this work integrates semantic learning into the compression pipeline to support decoder-side applications--such as scene editing and manipulation--that extend beyond traditional scene reconstruction and view synthesis. Our scheme features a lightweight implicit neural representation-based hyperprior, enabling efficient entropy coding of both color and semantic attributes while avoiding costly grid-based hyperprior as seen in many prior works. To facilitate compression and segmentation, we further develop compression-guided segmentation learning, consisting of quantization-aware training to enhance feature separability and a quality-aware weighting mechanism to suppress unreliable Gaussian primitives. Extensive experiments on the LERF and 3D-OVS datasets demonstrate that our approach significantly reduces transmission cost while preserving high rendering quality and strong segmentation performance.",
    "arxiv_url": "https://arxiv.org/abs/2601.12814v1",
    "pdf_url": "https://arxiv.org/pdf/2601.12814v1",
    "published_date": "2026-01-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "lightweight",
      "compression",
      "real-time rendering",
      "segmentation",
      "ar",
      "semantic",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.12814v1",
      "pdf": "https://arxiv.org/pdf/2601.12814v1"
    },
    "bibtex": ""
  },
  {
    "title": "KaoLRM: Repurposing Pre-trained Large Reconstruction Models for Parametric 3D Face Reconstruction",
    "authors": [
      "Qingtian Zhu",
      "Xu Cao",
      "Zhixiang Wang",
      "Yinqiang Zheng",
      "Takafumi Taketomi"
    ],
    "abstract": "We propose KaoLRM to re-target the learned prior of the Large Reconstruction Model (LRM) for parametric 3D face reconstruction from single-view images. Parametric 3D Morphable Models (3DMMs) have been widely used for facial reconstruction due to their compact and interpretable parameterization, yet existing 3DMM regressors often exhibit poor consistency across varying viewpoints. To address this, we harness the pre-trained 3D prior of LRM and incorporate FLAME-based 2D Gaussian Splatting into LRM's rendering pipeline. Specifically, KaoLRM projects LRM's pre-trained triplane features into the FLAME parameter space to recover geometry, and models appearance via 2D Gaussian primitives that are tightly coupled to the FLAME mesh. The rich prior enables the FLAME regressor to be aware of the 3D structure, leading to accurate and robust reconstructions under self-occlusions and diverse viewpoints. Experiments on both controlled and in-the-wild benchmarks demonstrate that KaoLRM achieves superior reconstruction accuracy and cross-view consistency, while existing methods remain sensitive to viewpoint variations. The code is released at https://github.com/CyberAgentAILab/KaoLRM.",
    "arxiv_url": "https://arxiv.org/abs/2601.12736v1",
    "pdf_url": "https://arxiv.org/pdf/2601.12736v1",
    "published_date": "2026-01-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/CyberAgentAILab/KaoLRM",
    "keywords": [
      "compact",
      "face",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.12736v1",
      "pdf": "https://arxiv.org/pdf/2601.12736v1",
      "github": "https://github.com/CyberAgentAILab/KaoLRM"
    },
    "bibtex": ""
  },
  {
    "title": "GaussianTrimmer: Online Trimming Boundaries for 3DGS Segmentation",
    "authors": [
      "Liwei Liao",
      "Ronggang Wang"
    ],
    "abstract": "With the widespread application of 3D Gaussians in 3D scene representation, 3D scene segmentation methods based on 3D Gaussians have also gradually emerged. However, existing 3D Gaussian segmentation methods basically segment on the basis of Gaussian primitives. Due to the large variation range of the scale of 3D Gaussians, large-sized Gaussians that often span the foreground and background lead to jagged boundaries of segmented objects. To this end, we propose an online boundary trimming method, GaussianTrimmer, which is an efficient and plug-and-play post-processing method capable of trimming coarse boundaries for existing 3D Gaussian segmentation methods. Our method consists of two core steps: 1. Generating uniformly and well-covered virtual cameras; 2. Trimming Gaussian at the primitive level based on 2D segmentation results on virtual cameras. Extensive quantitative and qualitative experiments demonstrate that our method can improve the segmentation quality of existing 3D Gaussian segmentation methods as a plug-and-play method.",
    "arxiv_url": "https://arxiv.org/abs/2601.12683v1",
    "pdf_url": "https://arxiv.org/pdf/2601.12683v1",
    "published_date": "2026-01-19",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "segmentation",
      "efficient",
      "ar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.12683v1",
      "pdf": "https://arxiv.org/pdf/2601.12683v1"
    },
    "bibtex": ""
  },
  {
    "title": "Active Semantic Mapping of Horticultural Environments Using Gaussian Splatting",
    "authors": [
      "Jose Cuaran",
      "Naveen K. Upalapati",
      "Girish Chowdhary"
    ],
    "abstract": "Semantic reconstruction of agricultural scenes plays a vital role in tasks such as phenotyping and yield estimation. However, traditional approaches that rely on manual scanning or fixed camera setups remain a major bottleneck in this process. In this work, we propose an active 3D reconstruction framework for horticultural environments using a mobile manipulator. The proposed system integrates the classical Octomap representation with 3D Gaussian Splatting to enable accurate and efficient target-aware mapping. While a low-resolution Octomap provides probabilistic occupancy information for informative viewpoint selection and collision-free planning, 3D Gaussian Splatting leverages geometric, photometric, and semantic information to optimize a set of 3D Gaussians for high-fidelity scene reconstruction. We further introduce simple yet effective strategies to enhance robustness against segmentation noise and reduce memory consumption. Simulation experiments demonstrate that our method outperforms purely occupancy-based approaches in both runtime efficiency and reconstruction accuracy, enabling precise fruit counting and volume estimation. Compared to a 0.01m-resolution Octomap, our approach achieves an improvement of 6.6% in fruit-level F1 score under noise-free conditions, and up to 28.6% under segmentation noise. Additionally, it achieves a 50% reduction in runtime, highlighting its potential for scalable, real-time semantic reconstruction in agricultural robotics.",
    "arxiv_url": "https://arxiv.org/abs/2601.12122v1",
    "pdf_url": "https://arxiv.org/pdf/2601.12122v1",
    "published_date": "2026-01-17",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "high-fidelity",
      "3d reconstruction",
      "segmentation",
      "ar",
      "efficient",
      "semantic",
      "mapping",
      "3d gaussian",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.12122v1",
      "pdf": "https://arxiv.org/pdf/2601.12122v1"
    },
    "bibtex": ""
  },
  {
    "title": "DIAMOND-SSS: Diffusion-Augmented Multi-View Optimization for Data-efficient SubSurface Scattering",
    "authors": [
      "Guillermo Figueroa-Araneda",
      "Iris Diana Jimenez",
      "Florian Hofherr",
      "Manny Ko",
      "Hector Andrade-Loarca",
      "Daniel Cremers"
    ],
    "abstract": "Subsurface scattering (SSS) gives translucent materials -- such as wax, jade, marble, and skin -- their characteristic soft shadows, color bleeding, and diffuse glow. Modeling these effects in neural rendering remains challenging due to complex light transport and the need for densely captured multi-view, multi-light datasets (often more than 100 views and 112 OLATs).   We present DIAMOND-SSS, a data-efficient framework for high-fidelity translucent reconstruction from extremely sparse supervision -- even as few as ten images. We fine-tune diffusion models for novel-view synthesis and relighting, conditioned on estimated geometry and trained on less than 7 percent of the dataset, producing photorealistic augmentations that can replace up to 95 percent of missing captures. To stabilize reconstruction under sparse or synthetic supervision, we introduce illumination-independent geometric priors: a multi-view silhouette consistency loss and a multi-view depth consistency loss.   Across all sparsity regimes, DIAMOND-SSS achieves state-of-the-art quality in relightable Gaussian rendering, reducing real capture requirements by up to 90 percent compared to SSS-3DGS.",
    "arxiv_url": "https://arxiv.org/abs/2601.12020v1",
    "pdf_url": "https://arxiv.org/pdf/2601.12020v1",
    "published_date": "2026-01-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "illumination",
      "light transport",
      "shadow",
      "high-fidelity",
      "face",
      "neural rendering",
      "geometry",
      "ar",
      "efficient",
      "relightable",
      "lighting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.12020v1",
      "pdf": "https://arxiv.org/pdf/2601.12020v1"
    },
    "bibtex": ""
  },
  {
    "title": "studentSplat: Your Student Model Learns Single-view 3D Gaussian Splatting",
    "authors": [
      "Yimu Pan",
      "Hongda Mao",
      "Qingshuang Chen",
      "Yelin Kim"
    ],
    "abstract": "Recent advance in feed-forward 3D Gaussian splatting has enable remarkable multi-view 3D scene reconstruction or single-view 3D object reconstruction but single-view 3D scene reconstruction remain under-explored due to inherited ambiguity in single-view. We present \\textbf{studentSplat}, a single-view 3D Gaussian splatting method for scene reconstruction. To overcome the scale ambiguity and extrapolation problems inherent in novel-view supervision from a single input, we introduce two techniques: 1) a teacher-student architecture where a multi-view teacher model provides geometric supervision to the single-view student during training, addressing scale ambiguity and encourage geometric validity; and 2) an extrapolation network that completes missing scene context, enabling high-quality extrapolation. Extensive experiments show studentSplat achieves state-of-the-art single-view novel-view reconstruction quality and comparable performance to multi-view methods at the scene level. Furthermore, studentSplat demonstrates competitive performance as a self-supervised single-view depth estimation method, highlighting its potential for general single-view 3D understanding tasks.",
    "arxiv_url": "https://arxiv.org/abs/2601.11772v1",
    "pdf_url": "https://arxiv.org/pdf/2601.11772v1",
    "published_date": "2026-01-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "lighting",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.11772v1",
      "pdf": "https://arxiv.org/pdf/2601.11772v1"
    },
    "bibtex": ""
  },
  {
    "title": "RSATalker: Realistic Socially-Aware Talking Head Generation for Multi-Turn Conversation",
    "authors": [
      "Peng Chen",
      "Xiaobao Wei",
      "Yi Yang",
      "Naiming Yao",
      "Hui Chen",
      "Feng Tian"
    ],
    "abstract": "Talking head generation is increasingly important in virtual reality (VR), especially for social scenarios involving multi-turn conversation. Existing approaches face notable limitations: mesh-based 3D methods can model dual-person dialogue but lack realistic textures, while large-model-based 2D methods produce natural appearances but incur prohibitive computational costs. Recently, 3D Gaussian Splatting (3DGS) based methods achieve efficient and realistic rendering but remain speaker-only and ignore social relationships. We introduce RSATalker, the first framework that leverages 3DGS for realistic and socially-aware talking head generation with support for multi-turn conversation. Our method first drives mesh-based 3D facial motion from speech, then binds 3D Gaussians to mesh facets to render high-fidelity 2D avatar videos. To capture interpersonal dynamics, we propose a socially-aware module that encodes social relationships, including blood and non-blood as well as equal and unequal, into high-level embeddings through a learnable query mechanism. We design a three-stage training paradigm and construct the RSATalker dataset with speech-mesh-image triplets annotated with social relationships. Extensive experiments demonstrate that RSATalker achieves state-of-the-art performance in both realism and social awareness. The code and dataset will be released.",
    "arxiv_url": "https://arxiv.org/abs/2601.10606v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10606v1",
    "published_date": "2026-01-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "dynamic",
      "vr",
      "high-fidelity",
      "face",
      "avatar",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "efficient",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.10606v1",
      "pdf": "https://arxiv.org/pdf/2601.10606v1"
    },
    "bibtex": ""
  },
  {
    "title": "Thinking Like Van Gogh: Structure-Aware Style Transfer via Flow-Guided 3D Gaussian Splatting",
    "authors": [
      "Zhendong Wang",
      "Lebin Zhou",
      "Jingchuan Xiao",
      "Rongduo Han",
      "Nam Ling",
      "Cihan Ruan"
    ],
    "abstract": "In 1888, Vincent van Gogh wrote, \"I am seeking exaggeration in the essential.\" This principle, amplifying structural form while suppressing photographic detail, lies at the core of Post-Impressionist art. However, most existing 3D style transfer methods invert this philosophy, treating geometry as a rigid substrate for surface-level texture projection. To authentically reproduce Post-Impressionist stylization, geometric abstraction must be embraced as the primary vehicle of expression.   We propose a flow-guided geometric advection framework for 3D Gaussian Splatting (3DGS) that operationalizes this principle in a mesh-free setting. Our method extracts directional flow fields from 2D paintings and back-propagates them into 3D space, rectifying Gaussian primitives to form flow-aligned brushstrokes that conform to scene topology without relying on explicit mesh priors. This enables expressive structural deformation driven directly by painterly motion rather than photometric constraints.   Our contributions are threefold: (1) a projection-based, mesh-free flow guidance mechanism that transfers 2D artistic motion into 3D Gaussian geometry; (2) a luminance-structure decoupling strategy that isolates geometric deformation from color optimization, mitigating artifacts during aggressive structural abstraction; and (3) a VLM-as-a-Judge evaluation framework that assesses artistic authenticity through aesthetic judgment instead of conventional pixel-level metrics, explicitly addressing the subjective nature of artistic stylization.",
    "arxiv_url": "https://arxiv.org/abs/2601.10075v1",
    "pdf_url": "https://arxiv.org/pdf/2601.10075v1",
    "published_date": "2026-01-15",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "deformation",
      "geometry",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.10075v1",
      "pdf": "https://arxiv.org/pdf/2601.10075v1"
    },
    "bibtex": ""
  },
  {
    "title": "Variable Basis Mapping for Real-Time Volumetric Visualization",
    "authors": [
      "Qibiao Li",
      "Yuxuan Wang",
      "Youcheng Cai",
      "Huangsheng Du",
      "Ligang Liu"
    ],
    "abstract": "Real-time visualization of large-scale volumetric data remains challenging, as direct volume rendering and voxel-based methods suffer from prohibitively high computational cost. We propose Variable Basis Mapping (VBM), a framework that transforms volumetric fields into 3D Gaussian Splatting (3DGS) representations through wavelet-domain analysis. First, we precompute a compact Wavelet-to-Gaussian Transition Bank that provides optimal Gaussian surrogates for canonical wavelet atoms across multiple scales. Second, we perform analytical Gaussian construction that maps discrete wavelet coefficients directly to 3DGS parameters using a closed-form, mathematically principled rule. Finally, a lightweight image-space fine-tuning stage further refines the representation to improve rendering fidelity. Experiments on diverse datasets demonstrate that VBM significantly accelerates convergence and enhances rendering quality, enabling real-time volumetric visualization.",
    "arxiv_url": "https://arxiv.org/abs/2601.09417v1",
    "pdf_url": "https://arxiv.org/pdf/2601.09417v1",
    "published_date": "2026-01-14",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "lightweight",
      "ar",
      "mapping",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.09417v1",
      "pdf": "https://arxiv.org/pdf/2601.09417v1"
    },
    "bibtex": ""
  },
  {
    "title": "TIDI-GS: Floater Suppression in 3D Gaussian Splatting for Enhanced Indoor Scene Fidelity",
    "authors": [
      "Sooyeun Yang",
      "Cheyul Im",
      "Jee Won Lee",
      "Jongseong Brad Choi"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a technique to create high-quality, real-time 3D scenes from images. This method often produces visual artifacts known as floaters--nearly transparent, disconnected elements that drift in space away from the actual surface. This geometric inaccuracy undermines the reliability of these models for practical applications, which is critical. To address this issue, we introduce TIDI-GS, a new training framework designed to eliminate these floaters. A key benefit of our approach is that it functions as a lightweight plugin for the standard 3DGS pipeline, requiring no major architectural changes and adding minimal overhead to the training process. The core of our method is a floater pruning algorithm--TIDI--that identifies and removes floaters based on several criteria: their consistency across multiple viewpoints, their spatial relationship to other elements, and an importance score learned during training. The framework includes a mechanism to preserve fine details, ensuring that important high-frequency elements are not mistakenly removed. This targeted cleanup is supported by a monocular depth-based loss function that helps improve the overall geometric structure of the scene. Our experiments demonstrate that TIDI-GS improves both the perceptual quality and geometric integrity of reconstructions, transforming them into robust digital assets, suitable for high-fidelity applications.",
    "arxiv_url": "https://arxiv.org/abs/2601.09291v2",
    "pdf_url": "https://arxiv.org/pdf/2601.09291v2",
    "published_date": "2026-01-14",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "lightweight",
      "face",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.09291v2",
      "pdf": "https://arxiv.org/pdf/2601.09291v2"
    },
    "bibtex": ""
  },
  {
    "title": "GaussianFluent: Gaussian Simulation for Dynamic Scenes with Mixed Materials",
    "authors": [
      "Bei Huang",
      "Yixin Chen",
      "Ruijie Lu",
      "Gang Zeng",
      "Hongbin Zha",
      "Yuru Pei",
      "Siyuan Huang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a prominent 3D representation for high-fidelity and real-time rendering. Prior work has coupled physics simulation with Gaussians, but predominantly targets soft, deformable materials, leaving brittle fracture largely unresolved. This stems from two key obstacles: the lack of volumetric interiors with coherent textures in GS representation, and the absence of fracture-aware simulation methods for Gaussians. To address these challenges, we introduce GaussianFluent, a unified framework for realistic simulation and rendering of dynamic object states. First, it synthesizes photorealistic interiors by densifying internal Gaussians guided by generative models. Second, it integrates an optimized Continuum Damage Material Point Method (CD-MPM) to enable brittle fracture simulation at remarkably high speed. Our approach handles complex scenarios including mixed-material objects and multi-stage fracture propagation, achieving results infeasible with previous methods. Experiments clearly demonstrate GaussianFluent's capability for photo-realistic, real-time rendering with structurally consistent interiors, highlighting its potential for downstream application, such as VR and Robotics.",
    "arxiv_url": "https://arxiv.org/abs/2601.09265v1",
    "pdf_url": "https://arxiv.org/pdf/2601.09265v1",
    "published_date": "2026-01-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "dynamic",
      "vr",
      "high-fidelity",
      "real-time rendering",
      "ar",
      "3d gaussian",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.09265v1",
      "pdf": "https://arxiv.org/pdf/2601.09265v1"
    },
    "bibtex": ""
  },
  {
    "title": "A$^2$TG: Adaptive Anisotropic Textured Gaussians for Efficient 3D Scene Representation",
    "authors": [
      "Sheng-Chi Hsu",
      "Ting-Yu Yen",
      "Shih-Hsuan Hung",
      "Hung-Kuo Chu"
    ],
    "abstract": "Gaussian Splatting has emerged as a powerful representation for high-quality, real-time 3D scene rendering. While recent works extend Gaussians with learnable textures to enrich visual appearance, existing approaches allocate a fixed square texture per primitive, leading to inefficient memory usage and limited adaptability to scene variability. In this paper, we introduce adaptive anisotropic textured Gaussians (A$^2$TG), a novel representation that generalizes textured Gaussians by equipping each primitive with an anisotropic texture. Our method employs a gradient-guided adaptive rule to jointly determine texture resolution and aspect ratio, enabling non-uniform, detail-aware allocation that aligns with the anisotropic nature of Gaussian splats. This design significantly improves texture efficiency, reducing memory consumption while enhancing image quality. Experiments on multiple benchmark datasets demonstrate that A TG consistently outperforms fixed-texture Gaussian Splatting methods, achieving comparable rendering fidelity with substantially lower memory requirements.",
    "arxiv_url": "https://arxiv.org/abs/2601.09243v1",
    "pdf_url": "https://arxiv.org/pdf/2601.09243v1",
    "published_date": "2026-01-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.09243v1",
      "pdf": "https://arxiv.org/pdf/2601.09243v1"
    },
    "bibtex": ""
  },
  {
    "title": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing",
    "authors": [
      "Jiahua Dong",
      "Yu-Xiong Wang"
    ],
    "abstract": "The transformative potential of 3D content creation has been progressively unlocked through advancements in generative models. Recently, intuitive drag editing with geometric changes has attracted significant attention in 2D editing yet remains challenging for 3D scenes. In this paper, we introduce 3DGS-Drag -- a point-based 3D editing framework that provides efficient, intuitive drag manipulation of real 3D scenes. Our approach bridges the gap between deformation-based and 2D-editing-based 3D editing methods, addressing their limitations to geometry-related content editing. We leverage two key innovations: deformation guidance utilizing 3D Gaussian Splatting for consistent geometric modifications and diffusion guidance for content correction and visual quality enhancement. A progressive editing strategy further supports aggressive 3D drag edits. Our method enables a wide range of edits, including motion change, shape adjustment, inpainting, and content extension. Experimental results demonstrate the effectiveness of 3DGS-Drag in various scenes, achieving state-of-the-art performance in geometry-related 3D content editing. Notably, the editing is efficient, taking 10 to 20 minutes on a single RTX 4090 GPU.",
    "arxiv_url": "https://arxiv.org/abs/2601.07963v1",
    "pdf_url": "https://arxiv.org/pdf/2601.07963v1",
    "published_date": "2026-01-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "deformation",
      "geometry",
      "gaussian splatting",
      "ar",
      "quality enhancement",
      "3d gaussian",
      "efficient",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.07963v1",
      "pdf": "https://arxiv.org/pdf/2601.07963v1"
    },
    "bibtex": ""
  },
  {
    "title": "ViewMorpher3D: A 3D-aware Diffusion Framework for Multi-Camera Novel View Synthesis in Autonomous Driving",
    "authors": [
      "Farhad G. Zanjani",
      "Hong Cai",
      "Amirhossein Habibian"
    ],
    "abstract": "Autonomous driving systems rely heavily on multi-view images to ensure accurate perception and robust decision-making. To effectively develop and evaluate perception stacks and planning algorithms, realistic closed-loop simulators are indispensable. While 3D reconstruction techniques such as Gaussian Splatting offer promising avenues for simulator construction, the rendered novel views often exhibit artifacts, particularly in extrapolated perspectives or when available observations are sparse.   We introduce ViewMorpher3D, a multi-view image enhancement framework based on image diffusion models, designed to elevate photorealism and multi-view coherence in driving scenes. Unlike single-view approaches, ViewMorpher3D jointly processes a set of rendered views conditioned on camera poses, 3D geometric priors, and temporally adjacent or spatially overlapping reference views. This enables the model to infer missing details, suppress rendering artifacts, and enforce cross-view consistency.   Our framework accommodates variable numbers of cameras and flexible reference/target view configurations, making it adaptable to diverse sensor setups. Experiments on real-world driving datasets demonstrate substantial improvements in image quality metrics, effectively reducing artifacts while preserving geometric fidelity.",
    "arxiv_url": "https://arxiv.org/abs/2601.07540v2",
    "pdf_url": "https://arxiv.org/pdf/2601.07540v2",
    "published_date": "2026-01-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "autonomous driving",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.07540v2",
      "pdf": "https://arxiv.org/pdf/2601.07540v2"
    },
    "bibtex": ""
  },
  {
    "title": "Mon3tr: Monocular 3D Telepresence with Pre-built Gaussian Avatars as Amortization",
    "authors": [
      "Fangyu Lin",
      "Yingdong Hu",
      "Zhening Liu",
      "Yufan Zhuang",
      "Zehong Lin",
      "Jun Zhang"
    ],
    "abstract": "Immersive telepresence aims to transform human interaction in AR/VR applications by enabling lifelike full-body holographic representations for enhanced remote collaboration. However, existing systems rely on hardware-intensive multi-camera setups and demand high bandwidth for volumetric streaming, limiting their real-time performance on mobile devices. To overcome these challenges, we propose Mon3tr, a novel Monocular 3D telepresence framework that integrates 3D Gaussian splatting (3DGS) based parametric human modeling into telepresence for the first time. Mon3tr adopts an amortized computation strategy, dividing the process into a one-time offline multi-view reconstruction phase to build a user-specific avatar and a monocular online inference phase during live telepresence sessions. A single monocular RGB camera is used to capture body motions and facial expressions in real time to drive the 3DGS-based parametric human model, significantly reducing system complexity and cost. The extracted motion and appearance features are transmitted at < 0.2 Mbps over WebRTC's data channel, allowing robust adaptation to network fluctuations. On the receiver side, e.g., Meta Quest 3, we develop a lightweight 3DGS attribute deformation network to dynamically generate corrective 3DGS attribute adjustments on the pre-built avatar, synthesizing photorealistic motion and appearance at ~ 60 FPS. Extensive experiments demonstrate the state-of-the-art performance of our method, achieving a PSNR of > 28 dB for novel poses, an end-to-end latency of ~ 80 ms, and > 1000x bandwidth reduction compared to point-cloud streaming, while supporting real-time operation from monocular inputs across diverse scenarios. Our demos can be found at https://mon3tr3d.github.io.",
    "arxiv_url": "https://arxiv.org/abs/2601.07518v1",
    "pdf_url": "https://arxiv.org/pdf/2601.07518v1",
    "published_date": "2026-01-12",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "vr",
      "lightweight",
      "body",
      "deformation",
      "avatar",
      "gaussian splatting",
      "ar",
      "human",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.07518v1",
      "pdf": "https://arxiv.org/pdf/2601.07518v1",
      "project": "https://mon3tr3d.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "R3-RECON: Radiance-Field-Free Active Reconstruction via Renderability",
    "authors": [
      "Xiaofeng Jin",
      "Matteo Frosi",
      "Yiran Guo",
      "Matteo Matteucci"
    ],
    "abstract": "In active reconstruction, an embodied agent must decide where to look next to efficiently acquire views that support high-quality novel-view rendering. Recent work on active view planning for neural rendering largely derives next-best-view (NBV) criteria by backpropagating through radiance fields or estimating information entropy over 3D Gaussian primitives. While effective, these strategies tightly couple view selection to heavy, representation-specific mechanisms and fail to account for the computational and resource constraints required for lightweight online deployment. In this paper, we revisit active reconstruction from a renderability-centric perspective. We propose $\\mathbb{R}^{3}$-RECON, a radiance-fields-free active reconstruction framework that induces an implicit, pose-conditioned renderability field over SE(3) from a lightweight voxel map. Our formulation aggregates per-voxel online observation statistics into a unified scalar renderability score that is cheap to update and can be queried in closed form at arbitrary candidate viewpoints in milliseconds, without requiring gradients or radiance-field training. This renderability field is strongly correlated with image-space reconstruction error, naturally guiding NBV selection. We further introduce a panoramic extension that estimates omnidirectional (360$^\\circ$) view utility to accelerate candidate evaluation. In the standard indoor Replica dataset, $\\mathbb{R}^{3}$-RECON achieves more uniform novel-view quality and higher 3D Gaussian splatting (3DGS) reconstruction accuracy than recent active GS baselines with matched view and time budgets.",
    "arxiv_url": "https://arxiv.org/abs/2601.07484v1",
    "pdf_url": "https://arxiv.org/pdf/2601.07484v1",
    "published_date": "2026-01-12",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "neural rendering",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.07484v1",
      "pdf": "https://arxiv.org/pdf/2601.07484v1"
    },
    "bibtex": ""
  },
  {
    "title": "RenderFlow: Single-Step Neural Rendering via Flow Matching",
    "authors": [
      "Shenghao Zhang",
      "Runtao Liu",
      "Christopher Schroers",
      "Yang Zhang"
    ],
    "abstract": "Conventional physically based rendering (PBR) pipelines generate photorealistic images through computationally intensive light transport simulations. Although recent deep learning approaches leverage diffusion model priors with geometry buffers (G-buffers) to produce visually compelling results without explicit scene geometry or light simulation, they remain constrained by two major limitations. First, the iterative nature of the diffusion process introduces substantial latency. Second, the inherent stochasticity of these generative models compromises physical accuracy and temporal consistency. In response to these challenges, we propose a novel, end-to-end, deterministic, single-step neural rendering framework, RenderFlow, built upon a flow matching paradigm. To further strengthen both rendering quality and generalization, we propose an efficient and effective module for sparse keyframe guidance. Our method significantly accelerates the rendering process and, by optionally incorporating sparsely rendered keyframes as guidance, enhances both the physical plausibility and overall visual quality of the output. The resulting pipeline achieves near real-time performance with photorealistic rendering quality, effectively bridging the gap between the efficiency of modern generative models and the precision of traditional physically based rendering. Furthermore, we demonstrate the versatility of our framework by introducing a lightweight, adapter-based module that efficiently repurposes the pretrained forward model for the inverse rendering task of intrinsic decomposition.",
    "arxiv_url": "https://arxiv.org/abs/2601.06928v1",
    "pdf_url": "https://arxiv.org/pdf/2601.06928v1",
    "published_date": "2026-01-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "light transport",
      "lightweight",
      "neural rendering",
      "geometry",
      "ar",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.06928v1",
      "pdf": "https://arxiv.org/pdf/2601.06928v1"
    },
    "bibtex": ""
  },
  {
    "title": "SARA: Scene-Aware Reconstruction Accelerator",
    "authors": [
      "Jee Won Lee",
      "Hansol Lim",
      "Minhyeok Im",
      "Dohyeon Lee",
      "Jongseong Brad Choi"
    ],
    "abstract": "We present SARA (Scene-Aware Reconstruction Accelerator), a geometry-driven pair selection module for Structure-from-Motion (SfM). Unlike conventional pipelines that select pairs based on visual similarity alone, SARA introduces geometry-first pair selection by scoring reconstruction informativeness - the product of overlap and parallax - before expensive matching. A lightweight pre-matching stage uses mutual nearest neighbors and RANSAC to estimate these cues, then constructs an Information-Weighted Spanning Tree (IWST) augmented with targeted edges for loop closure, long-baseline anchors, and weak-view reinforcement. Compared to exhaustive matching, SARA reduces rotation errors by 46.5+-5.5% and translation errors by 12.5+-6.5% across modern learned detectors, while achieving at most 50x speedup through 98% pair reduction (from 30,848 to 580 pairs). This reduces matching complexity from quadratic to quasi-linear, maintaining within +-3% of baseline reconstruction metrics for 3D Gaussian Splatting and SVRaster.",
    "arxiv_url": "https://arxiv.org/abs/2601.06831v1",
    "pdf_url": "https://arxiv.org/pdf/2601.06831v1",
    "published_date": "2026-01-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "lightweight",
      "geometry",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.06831v1",
      "pdf": "https://arxiv.org/pdf/2601.06831v1"
    },
    "bibtex": ""
  },
  {
    "title": "SRFlow: A Dataset and Regularization Model for High-Resolution Facial Optical Flow via Splatting Rasterization",
    "authors": [
      "JiaLin Zhang",
      "Dong Li"
    ],
    "abstract": "Facial optical flow supports a wide range of tasks in facial motion analysis. However, the lack of high-resolution facial optical flow datasets has hindered progress in this area. In this paper, we introduce Splatting Rasterization Flow (SRFlow), a high-resolution facial optical flow dataset, and Splatting Rasterization Guided FlowNet (SRFlowNet), a facial optical flow model with tailored regularization losses. These losses constrain flow predictions using masks and gradients computed via difference or Sobel operator. This effectively suppresses high-frequency noise and large-scale errors in texture-less or repetitive-pattern regions, enabling SRFlowNet to be the first model explicitly capable of capturing high-resolution skin motion guided by Gaussian splatting rasterization. Experiments show that training with the SRFlow dataset improves facial optical flow estimation across various optical flow models, reducing end-point error (EPE) by up to 42% (from 0.5081 to 0.2953). Furthermore, when coupled with the SRFlow dataset, SRFlowNet achieves up to a 48% improvement in F1-score (from 0.4733 to 0.6947) on a composite of three micro-expression datasets. These results demonstrate the value of advancing both facial optical flow estimation and micro-expression recognition.",
    "arxiv_url": "https://arxiv.org/abs/2601.06479v1",
    "pdf_url": "https://arxiv.org/pdf/2601.06479v1",
    "published_date": "2026-01-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "motion",
      "ar",
      "recognition",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.06479v1",
      "pdf": "https://arxiv.org/pdf/2601.06479v1"
    },
    "bibtex": ""
  },
  {
    "title": "PointSLAM++: Robust Dense Neural Gaussian Point Cloud-based SLAM",
    "authors": [
      "Xu Wang",
      "Boyao Han",
      "Xiaojun Chen",
      "Ying Liu",
      "Ruihui Li"
    ],
    "abstract": "Real-time 3D reconstruction is crucial for robotics and augmented reality, yet current simultaneous localization and mapping(SLAM) approaches often struggle to maintain structural consistency and robust pose estimation in the presence of depth noise. This work introduces PointSLAM++, a novel RGB-D SLAM system that leverages a hierarchically constrained neural Gaussian representation to preserve structural relationships while generating Gaussian primitives for scene mapping. It also employs progressive pose optimization to mitigate depth sensor noise, significantly enhancing localization accuracy. Furthermore, it utilizes a dynamic neural representation graph that adjusts the distribution of Gaussian nodes based on local geometric complexity, enabling the map to adapt to intricate scene details in real time. This combination yields high-precision 3D mapping and photorealistic scene rendering. Experimental results show PointSLAM++ outperforms existing 3DGS-based SLAM methods in reconstruction accuracy and rendering quality, demonstrating its advantages for large-scale AR and robotics.",
    "arxiv_url": "https://arxiv.org/abs/2601.11617v1",
    "pdf_url": "https://arxiv.org/pdf/2601.11617v1",
    "published_date": "2026-01-10",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "localization",
      "dynamic",
      "3d reconstruction",
      "ar",
      "slam",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.11617v1",
      "pdf": "https://arxiv.org/pdf/2601.11617v1"
    },
    "bibtex": ""
  },
  {
    "title": "NAS-GS: Noise-Aware Sonar Gaussian Splatting",
    "authors": [
      "Shida Xu",
      "Jingqi Jiang",
      "Jonatan Scharff Willners",
      "Sen Wang"
    ],
    "abstract": "Underwater sonar imaging plays a crucial role in various applications, including autonomous navigation in murky water, marine archaeology, and environmental monitoring. However, the unique characteristics of sonar images, such as complex noise patterns and the lack of elevation information, pose significant challenges for 3D reconstruction and novel view synthesis. In this paper, we present NAS-GS, a novel Noise-Aware Sonar Gaussian Splatting framework specifically designed to address these challenges. Our approach introduces a Two-Ways Splatting technique that accurately models the dual directions for intensity accumulation and transmittance calculation inherent in sonar imaging, significantly improving rendering speed without sacrificing quality. Moreover, we propose a Gaussian Mixture Model (GMM) based noise model that captures complex sonar noise patterns, including side-lobes, speckle, and multi-path noise. This model enhances the realism of synthesized images while preventing 3D Gaussian overfitting to noise, thereby improving reconstruction accuracy. We demonstrate state-of-the-art performance on both simulated and real-world large-scale offshore sonar scenarios, achieving superior results in novel view synthesis and 3D reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2601.06285v1",
    "pdf_url": "https://arxiv.org/pdf/2601.06285v1",
    "published_date": "2026-01-09",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.06285v1",
      "pdf": "https://arxiv.org/pdf/2601.06285v1"
    },
    "bibtex": ""
  },
  {
    "title": "LayerGS: Decomposition and Inpainting of Layered 3D Human Avatars via 2D Gaussian Splatting",
    "authors": [
      "Yinghan Xu",
      "John Dingliana"
    ],
    "abstract": "We propose a novel framework for decomposing arbitrarily posed humans into animatable multi-layered 3D human avatars, separating the body and garments. Conventional single-layer reconstruction methods lock clothing to one identity, while prior multi-layer approaches struggle with occluded regions. We overcome both limitations by encoding each layer as a set of 2D Gaussians for accurate geometry and photorealistic rendering, and inpainting hidden regions with a pretrained 2D diffusion model via score-distillation sampling (SDS). Our three-stage training strategy first reconstructs the coarse canonical garment via single-layer reconstruction, followed by multi-layer training to jointly recover the inner-layer body and outer-layer garment details. Experiments on two 3D human benchmark datasets (4D-Dress, Thuman2.0) show that our approach achieves better rendering quality and layer decomposition and recomposition than the previous state-of-the-art, enabling realistic virtual try-on under novel viewpoints and poses, and advancing practical creation of high-fidelity 3D human assets for immersive applications. Our code is available at https://github.com/RockyXu66/LayerGS",
    "arxiv_url": "https://arxiv.org/abs/2601.05853v1",
    "pdf_url": "https://arxiv.org/pdf/2601.05853v1",
    "published_date": "2026-01-09",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "https://github.com/RockyXu66/LayerGS",
    "keywords": [
      "4d",
      "high-fidelity",
      "body",
      "avatar",
      "geometry",
      "ar",
      "human",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.05853v1",
      "pdf": "https://arxiv.org/pdf/2601.05853v1",
      "github": "https://github.com/RockyXu66/LayerGS"
    },
    "bibtex": ""
  },
  {
    "title": "FeatureSLAM: Feature-enriched 3D gaussian splatting SLAM in real time",
    "authors": [
      "Christopher Thirgood",
      "Oscar Mendez",
      "Erin Ling",
      "Jon Storey",
      "Simon Hadfield"
    ],
    "abstract": "We present a real-time tracking SLAM system that unifies efficient camera tracking with photorealistic feature-enriched mapping using 3D Gaussian Splatting (3DGS). Our main contribution is integrating dense feature rasterization into the novel-view synthesis, aligned with a visual foundation model. This yields strong semantics, going beyond basic RGB-D input, aiding both tracking and mapping accuracy. Unlike previous semantic SLAM approaches (which embed pre-defined class labels) FeatureSLAM enables entirely new downstream tasks via free-viewpoint, open-set segmentation. Across standard benchmarks, our method achieves real-time tracking, on par with state-of-the-art systems while improving tracking stability and map fidelity without prohibitive compute. Quantitatively, we obtain 9\\% lower pose error and 8\\% higher mapping accuracy compared to recent fixed-set SLAM baselines. Our results confirm that real-time feature-embedded SLAM, is not only valuable for enabling new downstream applications. It also improves the performance of the underlying tracking and mapping subsystems, providing semantic and language masking results that are on-par with offline 3DGS models, alongside state-of-the-art tracking, depth and RGB rendering.",
    "arxiv_url": "https://arxiv.org/abs/2601.05738v1",
    "pdf_url": "https://arxiv.org/pdf/2601.05738v1",
    "published_date": "2026-01-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "tracking",
      "segmentation",
      "ar",
      "semantic",
      "mapping",
      "3d gaussian",
      "slam",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.05738v1",
      "pdf": "https://arxiv.org/pdf/2601.05738v1"
    },
    "bibtex": ""
  },
  {
    "title": "GS-DMSR: Dynamic Sensitive Multi-scale Manifold Enhancement for Accelerated High-Quality 3D Gaussian Splatting",
    "authors": [
      "Nengbo Lu",
      "Minghua Pan",
      "Shaohua Sun",
      "Yizhou Liang"
    ],
    "abstract": "In the field of 3D dynamic scene reconstruction, how to balance model convergence rate and rendering quality has long been a critical challenge that urgently needs to be addressed, particularly in high-precision modeling of scenes with complex dynamic motions. To tackle this issue, this study proposes the GS-DMSR method. By quantitatively analyzing the dynamic evolution process of Gaussian attributes, this mechanism achieves adaptive gradient focusing, enabling it to dynamically identify significant differences in the motion states of Gaussian models. It then applies differentiated optimization strategies to Gaussian models with varying degrees of significance, thereby significantly improving the model convergence rate. Additionally, this research integrates a multi-scale manifold enhancement module, which leverages the collaborative optimization of an implicit nonlinear decoder and an explicit deformation field to enhance the modeling efficiency for complex deformation scenes. Experimental results demonstrate that this method achieves a frame rate of up to 96 FPS on synthetic datasets, while effectively reducing both storage overhead and training time.Our code and data are available at https://anonymous.4open.science/r/GS-DMSR-2212.",
    "arxiv_url": "https://arxiv.org/abs/2601.05584v1",
    "pdf_url": "https://arxiv.org/pdf/2601.05584v1",
    "published_date": "2026-01-09",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "dynamic",
      "deformation",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.05584v1",
      "pdf": "https://arxiv.org/pdf/2601.05584v1",
      "project": "https://anonymous.4open.science/r/GS-DMSR-2212"
    },
    "bibtex": ""
  },
  {
    "title": "GaussianSwap: Animatable Video Face Swapping with 3D Gaussian Splatting",
    "authors": [
      "Xuan Cheng",
      "Jiahao Rao",
      "Chengyang Li",
      "Wenhao Wang",
      "Weilin Chen",
      "Lvqing Yang"
    ],
    "abstract": "We introduce GaussianSwap, a novel video face swapping framework that constructs a 3D Gaussian Splatting based face avatar from a target video while transferring identity from a source image to the avatar. Conventional video swapping frameworks are limited to generating facial representations in pixel-based formats. The resulting swapped faces exist merely as a set of unstructured pixels without any capacity for animation or interactive manipulation. Our work introduces a paradigm shift from conventional pixel-based video generation to the creation of high-fidelity avatar with swapped faces. The framework first preprocesses target video to extract FLAME parameters, camera poses and segmentation masks, and then rigs 3D Gaussian splats to the FLAME model across frames, enabling dynamic facial control. To ensure identity preserving, we propose an compound identity embedding constructed from three state-of-the-art face recognition models for avatar finetuning. Finally, we render the face-swapped avatar on the background frames to obtain the face-swapped video. Experimental results demonstrate that GaussianSwap achieves superior identity preservation, visual clarity and temporal consistency, while enabling previously unattainable interactive applications.",
    "arxiv_url": "https://arxiv.org/abs/2601.05511v1",
    "pdf_url": "https://arxiv.org/pdf/2601.05511v1",
    "published_date": "2026-01-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "high-fidelity",
      "face",
      "recognition",
      "animation",
      "avatar",
      "segmentation",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.05511v1",
      "pdf": "https://arxiv.org/pdf/2601.05511v1"
    },
    "bibtex": ""
  },
  {
    "title": "Sketch&Patch++: Efficient Structure-Aware 3D Gaussian Representation",
    "authors": [
      "Yuang Shi",
      "Géraldine Morin",
      "Simone Gasparini",
      "Wei Tsang Ooi"
    ],
    "abstract": "We observe that Gaussians exhibit distinct roles and characteristics analogous to traditional artistic techniques -- like how artists first sketch outlines before filling in broader areas with color, some Gaussians capture high-frequency features such as edges and contours, while others represent broader, smoother regions analogous to brush strokes that add volume and depth. Based on this observation, we propose a hybrid representation that categorizes Gaussians into (i) Sketch Gaussians, which represent high-frequency, boundary-defining features, and (ii) Patch Gaussians, which cover low-frequency, smooth regions. This semantic separation naturally enables layered progressive streaming, where the compact Sketch Gaussians establish the structural skeleton before Patch Gaussians incrementally refine volumetric detail.   In this work, we extend our previous method to arbitrary 3D scenes by proposing a novel hierarchical adaptive categorization framework that operates directly on the 3DGS representation. Our approach employs multi-criteria density-based clustering, combined with adaptive quality-driven refinement. This method eliminates dependency on external 3D line primitives while ensuring optimal parametric encoding effectiveness. Our comprehensive evaluation across diverse scenes, including both man-made and natural environments, demonstrates that our method achieves up to 1.74 dB improvement in PSNR, 6.7% in SSIM, and 41.4% in LPIPS at equivalent model sizes compared to uniform pruning baselines. For indoor scenes, our method can maintain visual quality with only 0.5\\% of the original model size. This structure-aware representation enables efficient storage, adaptive streaming, and rendering of high-fidelity 3D content across bandwidth-constrained networks and resource-limited devices.",
    "arxiv_url": "https://arxiv.org/abs/2601.05394v2",
    "pdf_url": "https://arxiv.org/pdf/2601.05394v2",
    "published_date": "2026-01-08",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.MM",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "high-fidelity",
      "ar",
      "semantic",
      "3d gaussian",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.05394v2",
      "pdf": "https://arxiv.org/pdf/2601.05394v2"
    },
    "bibtex": ""
  },
  {
    "title": "MOSAIC-GS: Monocular Scene Reconstruction via Advanced Initialization for Complex Dynamic Environments",
    "authors": [
      "Svitlana Morkva",
      "Maximum Wilder-Smith",
      "Michael Oechsle",
      "Alessio Tonioni",
      "Marco Hutter",
      "Vaishakh Patil"
    ],
    "abstract": "We present MOSAIC-GS, a novel, fully explicit, and computationally efficient approach for high-fidelity dynamic scene reconstruction from monocular videos using Gaussian Splatting. Monocular reconstruction is inherently ill-posed due to the lack of sufficient multiview constraints, making accurate recovery of object geometry and temporal coherence particularly challenging. To address this, we leverage multiple geometric cues, such as depth, optical flow, dynamic object segmentation, and point tracking. Combined with rigidity-based motion constraints, these cues allow us to estimate preliminary 3D scene dynamics during an initialization stage. Recovering scene dynamics prior to the photometric optimization reduces reliance on motion inference from visual appearance alone, which is often ambiguous in monocular settings. To enable compact representations, fast training, and real-time rendering while supporting non-rigid deformations, the scene is decomposed into static and dynamic components. Each Gaussian in the dynamic part of the scene is assigned a trajectory represented as time-dependent Poly-Fourier curve for parameter-efficient motion encoding. We demonstrate that MOSAIC-GS achieves substantially faster optimization and rendering compared to existing methods, while maintaining reconstruction quality on par with state-of-the-art approaches across standard monocular dynamic scene benchmarks.",
    "arxiv_url": "https://arxiv.org/abs/2601.05368v1",
    "pdf_url": "https://arxiv.org/pdf/2601.05368v1",
    "published_date": "2026-01-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "dynamic",
      "high-fidelity",
      "tracking",
      "deformation",
      "real-time rendering",
      "motion",
      "geometry",
      "segmentation",
      "ar",
      "efficient",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.05368v1",
      "pdf": "https://arxiv.org/pdf/2601.05368v1"
    },
    "bibtex": ""
  },
  {
    "title": "Akasha 2: Hamiltonian State Space Duality and Visual-Language Joint Embedding Predictive Architectur",
    "authors": [
      "Yani Meziani"
    ],
    "abstract": "We present Akasha 2, a state-of-the-art multimodal architecture that integrates Hamiltonian State Space Duality (H-SSD) with Visual-Language Joint Embedding Predictive Architecture (VL-JEPA). The system leverages the Mamba-3 Selective State Space Model (SSM) augmented by a Sparse Mixture of Hamiltonian Experts (SMoE-HE) that enforces latent physical conservation laws through symplectic integration. For visual synthesis, we introduce Hamiltonian Flow Matching (HFM) and persistent 3D Gaussian Splatting (3DGS), enabling ultra-low latency (<50ms) on mobile hardware. This work establishes a new paradigm in latent world models, achieving unprecedented spatiotemporal coherence through a holographic memory architecture. Our approach demonstrates that incorporating physics-inspired inductive biases into neural architectures yields significant improvements: state-of-the-art video prediction (FVD: 287), 4x faster visual synthesis than diffusion models, and 3-18x inference speedup over transformer baselines while maintaining energy conservation over extended horizons.",
    "arxiv_url": "https://arxiv.org/abs/2601.06212v1",
    "pdf_url": "https://arxiv.org/pdf/2601.06212v1",
    "published_date": "2026-01-08",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.06212v1",
      "pdf": "https://arxiv.org/pdf/2601.06212v1"
    },
    "bibtex": ""
  },
  {
    "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
    "authors": [
      "Sixiao Zheng",
      "Minghao Yin",
      "Wenbo Hu",
      "Xiaoyu Li",
      "Ying Shan",
      "Yanwei Fu"
    ],
    "abstract": "Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.",
    "arxiv_url": "https://arxiv.org/abs/2601.05138v1",
    "pdf_url": "https://arxiv.org/pdf/2601.05138v1",
    "published_date": "2026-01-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "high-fidelity",
      "ar",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.05138v1",
      "pdf": "https://arxiv.org/pdf/2601.05138v1"
    },
    "bibtex": ""
  },
  {
    "title": "OceanSplat: Object-aware Gaussian Splatting with Trinocular View Consistency for Underwater Scene Reconstruction",
    "authors": [
      "Minseong Kweon",
      "Jinsun Park"
    ],
    "abstract": "We introduce OceanSplat, a novel 3D Gaussian Splatting-based approach for high-fidelity underwater scene reconstruction. To overcome multi-view inconsistencies caused by scattering media, we design a trinocular setup for each camera pose by rendering from horizontally and vertically translated virtual viewpoints, enforcing view consistency to facilitate spatial optimization of 3D Gaussians. Furthermore, we derive synthetic epipolar depth priors from the virtual viewpoints, which serve as self-supervised depth regularizers to compensate for the limited geometric cues in degraded underwater scenes. We also propose a depth-aware alpha adjustment that modulates the opacity of 3D Gaussians during early training based on their depth along the viewing direction, deterring the formation of medium-induced primitives. Our approach promotes the disentanglement of 3D Gaussians from the scattering medium through effective geometric constraints, enabling accurate representation of scene structure and significantly reducing floating artifacts. Experiments on real-world underwater and simulated scenes demonstrate that OceanSplat substantially outperforms existing methods for both scene reconstruction and restoration in scattering media.",
    "arxiv_url": "https://arxiv.org/abs/2601.04984v2",
    "pdf_url": "https://arxiv.org/pdf/2601.04984v2",
    "published_date": "2026-01-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "high-fidelity",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.04984v2",
      "pdf": "https://arxiv.org/pdf/2601.04984v2"
    },
    "bibtex": ""
  },
  {
    "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
    "authors": [
      "Yen-Jen Chiou",
      "Wei-Tse Cheng",
      "Yuan-Fu Yang"
    ],
    "abstract": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA. Additional details are available at our project page https://chiou1203.github.io/ProFuse/.",
    "arxiv_url": "https://arxiv.org/abs/2601.04754v2",
    "pdf_url": "https://arxiv.org/pdf/2601.04754v2",
    "published_date": "2026-01-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "understanding",
      "geometry",
      "ar",
      "semantic",
      "3d gaussian",
      "efficient",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.04754v2",
      "pdf": "https://arxiv.org/pdf/2601.04754v2",
      "project": "https://chiou1203.github.io/ProFuse"
    },
    "bibtex": ""
  },
  {
    "title": "SCAR-GS: Spatial Context Attention for Residuals in Progressive Gaussian Splatting",
    "authors": [
      "Diego Revilla",
      "Pooja Suresh",
      "Anand Bhojan",
      "Ooi Wei Tsang"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting have allowed for real-time, high-fidelity novel view synthesis. Nonetheless, these models have significant storage requirements for large and medium-sized scenes, hindering their deployment over cloud and streaming services. Some of the most recent progressive compression techniques for these models rely on progressive masking and scalar quantization techniques to reduce the bitrate of Gaussian attributes using spatial context models. While effective, scalar quantization may not optimally capture the correlations of high-dimensional feature vectors, which can potentially limit the rate-distortion performance.   In this work, we introduce a novel progressive codec for 3D Gaussian Splatting that replaces traditional methods with a more powerful Residual Vector Quantization approach to compress the primitive features. Our key contribution is an auto-regressive entropy model, guided by a multi-resolution hash grid, that accurately predicts the conditional probability of each successive transmitted index, allowing for coarse and refinement layers to be compressed with high efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2601.04348v1",
    "pdf_url": "https://arxiv.org/pdf/2601.04348v1",
    "published_date": "2026-01-07",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "compression",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.04348v1",
      "pdf": "https://arxiv.org/pdf/2601.04348v1"
    },
    "bibtex": ""
  },
  {
    "title": "IDESplat: Iterative Depth Probability Estimation for Generalizable 3D Gaussian Splatting",
    "authors": [
      "Wei Long",
      "Haifeng Wu",
      "Shiyin Jiang",
      "Jinhua Zhang",
      "Xinchun Ji",
      "Shuhang Gu"
    ],
    "abstract": "Generalizable 3D Gaussian Splatting aims to directly predict Gaussian parameters using a feed-forward network for scene reconstruction. Among these parameters, Gaussian means are particularly difficult to predict, so depth is usually estimated first and then unprojected to obtain the Gaussian sphere centers. Existing methods typically rely solely on a single warp to estimate depth probability, which hinders their ability to fully leverage cross-view geometric cues, resulting in unstable and coarse depth maps. To address this limitation, we propose IDESplat, which iteratively applies warp operations to boost depth probability estimation for accurate Gaussian mean prediction. First, to eliminate the inherent instability of a single warp, we introduce a Depth Probability Boosting Unit (DPBU) that integrates epipolar attention maps produced by cascading warp operations in a multiplicative manner. Next, we construct an iterative depth estimation process by stacking multiple DPBUs, progressively identifying potential depth candidates with high likelihood. As IDESplat iteratively boosts depth probability estimates and updates the depth candidates, the depth map is gradually refined, resulting in accurate Gaussian means. We conduct experiments on RealEstate10K, ACID, and DL3DV. IDESplat achieves outstanding reconstruction quality and state-of-the-art performance with real-time efficiency. On RE10K, it outperforms DepthSplat by 0.33 dB in PSNR, using only 10.7% of the parameters and 70% of the memory. Additionally, our IDESplat improves PSNR by 2.95 dB over DepthSplat on the DTU dataset in cross-dataset experiments, demonstrating its strong generalization ability.",
    "arxiv_url": "https://arxiv.org/abs/2601.03824v2",
    "pdf_url": "https://arxiv.org/pdf/2601.03824v2",
    "published_date": "2026-01-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.03824v2",
      "pdf": "https://arxiv.org/pdf/2601.03824v2"
    },
    "bibtex": ""
  },
  {
    "title": "G2P: Gaussian-to-Point Attribute Alignment for Boundary-Aware 3D Semantic Segmentation",
    "authors": [
      "Hojun Song",
      "Chae-yeong Song",
      "Jeong-hun Hong",
      "Chaewon Moon",
      "Dong-hwi Kim",
      "Gahyeon Kim",
      "Soo Ye Kim",
      "Yiyi Liao",
      "Jaehyup Lee",
      "Sang-hyo Park"
    ],
    "abstract": "Semantic segmentation on point clouds is critical for 3D scene understanding. However, sparse and irregular point distributions provide limited appearance evidence, making geometry-only features insufficient to distinguish objects with similar shapes but distinct appearances (e.g., color, texture, material). We propose Gaussian-to-Point (G2P), which transfers appearance-aware attributes from 3D Gaussian Splatting to point clouds for more discriminative and appearance-consistent segmentation. Our G2P address the misalignment between optimized Gaussians and original point geometry by establishing point-wise correspondences. By leveraging Gaussian opacity attributes, we resolve the geometric ambiguity that limits existing models. Additionally, Gaussian scale attributes enable precise boundary localization in complex 3D scenes. Extensive experiments demonstrate that our approach achieves superior performance on standard benchmarks and shows significant improvements on geometrically challenging classes, all without any 2D or language supervision.",
    "arxiv_url": "https://arxiv.org/abs/2601.03510v1",
    "pdf_url": "https://arxiv.org/pdf/2601.03510v1",
    "published_date": "2026-01-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "localization",
      "geometry",
      "segmentation",
      "ar",
      "semantic",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.03510v1",
      "pdf": "https://arxiv.org/pdf/2601.03510v1"
    },
    "bibtex": ""
  },
  {
    "title": "RelightAnyone: A Generalized Relightable 3D Gaussian Head Model",
    "authors": [
      "Yingyan Xu",
      "Pramod Rao",
      "Sebastian Weiss",
      "Gaspard Zoss",
      "Markus Gross",
      "Christian Theobalt",
      "Marc Habermann",
      "Derek Bradley"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become a standard approach to reconstruct and render photorealistic 3D head avatars. A major challenge is to relight the avatars to match any scene illumination. For high quality relighting, existing methods require subjects to be captured under complex time-multiplexed illumination, such as one-light-at-a-time (OLAT). We propose a new generalized relightable 3D Gaussian head model that can relight any subject observed in a single- or multi-view images without requiring OLAT data for that subject. Our core idea is to learn a mapping from flat-lit 3DGS avatars to corresponding relightable Gaussian parameters for that avatar. Our model consists of two stages: a first stage that models flat-lit 3DGS avatars without OLAT lighting, and a second stage that learns the mapping to physically-based reflectance parameters for high-quality relighting. This two-stage design allows us to train the first stage across diverse existing multi-view datasets without OLAT lighting ensuring cross-subject generalization, where we learn a dataset-specific lighting code for self-supervised lighting alignment. Subsequently, the second stage can be trained on a significantly smaller dataset of subjects captured under OLAT illumination. Together, this allows our method to generalize well and relight any subject from the first stage as if we had captured them under OLAT lighting. Furthermore, we can fit our model to unseen subjects from as little as a single image, allowing several applications in novel view synthesis and relighting for digital avatars.",
    "arxiv_url": "https://arxiv.org/abs/2601.03357v1",
    "pdf_url": "https://arxiv.org/pdf/2601.03357v1",
    "published_date": "2026-01-06",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "illumination",
      "head",
      "high quality",
      "avatar",
      "ar",
      "mapping",
      "relightable",
      "3d gaussian",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.03357v1",
      "pdf": "https://arxiv.org/pdf/2601.03357v1"
    },
    "bibtex": ""
  },
  {
    "title": "A High-Fidelity Digital Twin for Robotic Manipulation Based on 3D Gaussian Splatting",
    "authors": [
      "Ziyang Sun",
      "Lingfan Bao",
      "Tianhu Peng",
      "Jingcheng Sun",
      "Chengxu Zhou"
    ],
    "abstract": "Developing high-fidelity, interactive digital twins is crucial for enabling closed-loop motion planning and reliable real-world robot execution, which are essential to advancing sim-to-real transfer. However, existing approaches often suffer from slow reconstruction, limited visual fidelity, and difficulties in converting photorealistic models into planning-ready collision geometry. We present a practical framework that constructs high-quality digital twins within minutes from sparse RGB inputs. Our system employs 3D Gaussian Splatting (3DGS) for fast, photorealistic reconstruction as a unified scene representation. We enhance 3DGS with visibility-aware semantic fusion for accurate 3D labelling and introduce an efficient, filter-based geometry conversion method to produce collision-ready models seamlessly integrated with a Unity-ROS2-MoveIt physics engine. In experiments with a Franka Emika Panda robot performing pick-and-place tasks, we demonstrate that this enhanced geometric accuracy effectively supports robust manipulation in real-world trials. These results demonstrate that 3DGS-based digital twins, enriched with semantic and geometric consistency, offer a fast, reliable, and scalable path from perception to manipulation in unstructured environments.",
    "arxiv_url": "https://arxiv.org/abs/2601.03200v1",
    "pdf_url": "https://arxiv.org/pdf/2601.03200v1",
    "published_date": "2026-01-06",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "motion",
      "geometry",
      "ar",
      "semantic",
      "3d gaussian",
      "efficient",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.03200v1",
      "pdf": "https://arxiv.org/pdf/2601.03200v1"
    },
    "bibtex": ""
  },
  {
    "title": "SA-ResGS: Self-Augmented Residual 3D Gaussian Splatting for Next Best View Selection",
    "authors": [
      "Kim Jun-Seong",
      "Tae-Hyun Oh",
      "Eduardo Pérez-Pellitero",
      "Youngkyoon Jang"
    ],
    "abstract": "We propose Self-Augmented Residual 3D Gaussian Splatting (SA-ResGS), a novel framework to stabilize uncertainty quantification and enhancing uncertainty-aware supervision in next-best-view (NBV) selection for active scene reconstruction. SA-ResGS improves both the reliability of uncertainty estimates and their effectiveness for supervision by generating Self-Augmented point clouds (SA-Points) via triangulation between a training view and a rasterized extrapolated view, enabling efficient scene coverage estimation. While improving scene coverage through physically guided view selection, SA-ResGS also addresses the challenge of under-supervised Gaussians, exacerbated by sparse and wide-baseline views, by introducing the first residual learning strategy tailored for 3D Gaussian Splatting. This targeted supervision enhances gradient flow in high-uncertainty Gaussians by combining uncertainty-driven filtering with dropout- and hard-negative-mining-inspired sampling. Our contributions are threefold: (1) a physically grounded view selection strategy that promotes efficient and uniform scene coverage; (2) an uncertainty-aware residual supervision scheme that amplifies learning signals for weakly contributing Gaussians, improving training stability and uncertainty estimation across scenes with diverse camera distributions; (3) an implicit unbiasing of uncertainty quantification as a consequence of constrained view selection and residual supervision, which together mitigate conflicting effects of wide-baseline exploration and sparse-view ambiguity in NBV planning. Experiments on active view selection demonstrate that SA-ResGS outperforms state-of-the-art baselines in both reconstruction quality and view selection robustness.",
    "arxiv_url": "https://arxiv.org/abs/2601.03024v2",
    "pdf_url": "https://arxiv.org/pdf/2601.03024v2",
    "published_date": "2026-01-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.03024v2",
      "pdf": "https://arxiv.org/pdf/2601.03024v2"
    },
    "bibtex": ""
  },
  {
    "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature",
    "authors": [
      "Eldad Matmon",
      "Amit Bracha",
      "Noam Rotstein",
      "Ron Kimmel"
    ],
    "abstract": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.",
    "arxiv_url": "https://arxiv.org/abs/2601.03319v1",
    "pdf_url": "https://arxiv.org/pdf/2601.03319v1",
    "published_date": "2026-01-06",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "deformation",
      "avatar",
      "geometry",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.03319v1",
      "pdf": "https://arxiv.org/pdf/2601.03319v1"
    },
    "bibtex": ""
  },
  {
    "title": "CAMO: Category-Agnostic 3D Motion Transfer from Monocular 2D Videos",
    "authors": [
      "Taeyeon Kim",
      "Youngju Na",
      "Jumin Lee",
      "Minhyuk Sung",
      "Sung-Eui Yoon"
    ],
    "abstract": "Motion transfer from 2D videos to 3D assets is a challenging problem, due to inherent pose ambiguities and diverse object shapes, often requiring category-specific parametric templates. We propose CAMO, a category-agnostic framework that transfers motion to diverse target meshes directly from monocular 2D videos without relying on predefined templates or explicit 3D supervision. The core of CAMO is a morphology-parameterized articulated 3D Gaussian splatting model combined with dense semantic correspondences to jointly adapt shape and pose through optimization. This approach effectively alleviates shape-pose ambiguities, enabling visually faithful motion transfer for diverse categories. Experimental results demonstrate superior motion accuracy, efficiency, and visual coherence compared to existing methods, significantly advancing motion transfer in varied object categories and casual video scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2601.02716v1",
    "pdf_url": "https://arxiv.org/pdf/2601.02716v1",
    "published_date": "2026-01-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "semantic",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.02716v1",
      "pdf": "https://arxiv.org/pdf/2601.02716v1"
    },
    "bibtex": ""
  },
  {
    "title": "Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding",
    "authors": [
      "Jingming He",
      "Chongyi Li",
      "Shiqi Wang",
      "Sam Kwong"
    ],
    "abstract": "Recent works propose extending 3DGS with semantic feature vectors for simultaneous semantic segmentation and image rendering. However, these methods often treat the semantic and rendering branches separately, relying solely on 2D supervision while ignoring the 3D Gaussian geometry. Moreover, current adaptive strategies adapt the Gaussian set depending solely on rendering gradients, which can be insufficient in subtle or textureless regions. In this work, we propose a joint enhancement framework for 3D semantic Gaussian modeling that synergizes both semantic and rendering branches. Firstly, unlike conventional point cloud shape encoding, we introduce an anisotropic 3D Gaussian Chebyshev descriptor using the Laplace-Beltrami operator to capture fine-grained 3D shape details, thereby distinguishing objects with similar appearances and reducing reliance on potentially noisy 2D guidance. In addition, without relying solely on rendering gradient, we adaptively adjust Gaussian allocation and spherical harmonics with local semantic and shape signals, enhancing rendering efficiency through selective resource allocation. Finally, we employ a cross-scene knowledge transfer module to continuously update learned shape patterns, enabling faster convergence and robust representations without relearning shape information from scratch for each new scene. Experiments on multiple datasets demonstrate improvements in segmentation accuracy and rendering quality while maintaining high rendering frame rates.",
    "arxiv_url": "https://arxiv.org/abs/2601.02339v1",
    "pdf_url": "https://arxiv.org/pdf/2601.02339v1",
    "published_date": "2026-01-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "segmentation",
      "ar",
      "semantic",
      "3d gaussian",
      "fast"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.02339v1",
      "pdf": "https://arxiv.org/pdf/2601.02339v1"
    },
    "bibtex": ""
  },
  {
    "title": "HeadLighter: Disentangling Illumination in Generative 3D Gaussian Heads via Lightstage Captures",
    "authors": [
      "Yating Wang",
      "Yuan Sun",
      "Xuan Wang",
      "Ran Yi",
      "Boyao Zhou",
      "Yipengjing Sun",
      "Hongyu Liu",
      "Yinuo Wang",
      "Lizhuang Ma"
    ],
    "abstract": "Recent 3D-aware head generative models based on 3D Gaussian Splatting achieve real-time, photorealistic and view-consistent head synthesis. However, a fundamental limitation persists: the deep entanglement of illumination and intrinsic appearance prevents controllable relighting. Existing disentanglement methods rely on strong assumptions to enable weakly supervised learning, which restricts their capacity for complex illumination. To address this challenge, we introduce HeadLighter, a novel supervised framework that learns a physically plausible decomposition of appearance and illumination in head generative models. Specifically, we design a dual-branch architecture that separately models lighting-invariant head attributes and physically grounded rendering components. A progressive disentanglement training is employed to gradually inject head appearance priors into the generative architecture, supervised by multi-view images captured under controlled light conditions with a light stage setup. We further introduce a distillation strategy to generate high-quality normals for realistic rendering. Experiments demonstrate that our method preserves high-quality generation and real-time rendering, while simultaneously supporting explicit lighting and viewpoint editing. We will publicly release our code and dataset.",
    "arxiv_url": "https://arxiv.org/abs/2601.02103v2",
    "pdf_url": "https://arxiv.org/pdf/2601.02103v2",
    "published_date": "2026-01-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "illumination",
      "head",
      "real-time rendering",
      "ar",
      "3d gaussian",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.02103v2",
      "pdf": "https://arxiv.org/pdf/2601.02103v2"
    },
    "bibtex": ""
  },
  {
    "title": "360-GeoGS: Geometrically Consistent Feed-Forward 3D Gaussian Splatting Reconstruction for 360 Images",
    "authors": [
      "Jiaqi Yao",
      "Zhongmiao Yan",
      "Jingyi Xu",
      "Songpengcheng Xia",
      "Yan Xiang",
      "Ling Pei"
    ],
    "abstract": "3D scene reconstruction is fundamental for spatial intelligence applications such as AR, robotics, and digital twins. Traditional multi-view stereo struggles with sparse viewpoints or low-texture regions, while neural rendering approaches, though capable of producing high-quality results, require per-scene optimization and lack real-time efficiency. Explicit 3D Gaussian Splatting (3DGS) enables efficient rendering, but most feed-forward variants focus on visual quality rather than geometric consistency, limiting accurate surface reconstruction and overall reliability in spatial perception tasks. This paper presents a novel feed-forward 3DGS framework for 360 images, capable of generating geometrically consistent Gaussian primitives while maintaining high rendering quality. A Depth-Normal geometric regularization is introduced to couple rendered depth gradients with normal information, supervising Gaussian rotation, scale, and position to improve point cloud and surface accuracy. Experimental results show that the proposed method maintains high rendering quality while significantly improving geometric consistency, providing an effective solution for 3D reconstruction in spatial perception tasks.",
    "arxiv_url": "https://arxiv.org/abs/2601.02102v1",
    "pdf_url": "https://arxiv.org/pdf/2601.02102v1",
    "published_date": "2026-01-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "sparse view",
      "face",
      "3d reconstruction",
      "neural rendering",
      "ar",
      "efficient rendering",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.02102v1",
      "pdf": "https://arxiv.org/pdf/2601.02102v1"
    },
    "bibtex": ""
  },
  {
    "title": "InpaintHuman: Reconstructing Occluded Humans with Multi-Scale UV Mapping and Identity-Preserving Diffusion Inpainting",
    "authors": [
      "Jinlong Fan",
      "Shanshan Zhao",
      "Liang Zheng",
      "Jing Zhang",
      "Yuxiang Yang",
      "Mingming Gong"
    ],
    "abstract": "Reconstructing complete and animatable 3D human avatars from monocular videos remains challenging, particularly under severe occlusions. While 3D Gaussian Splatting has enabled photorealistic human rendering, existing methods struggle with incomplete observations, often producing corrupted geometry and temporal inconsistencies. We present InpaintHuman, a novel method for generating high-fidelity, complete, and animatable avatars from occluded monocular videos. Our approach introduces two key innovations: (i) a multi-scale UV-parameterized representation with hierarchical coarse-to-fine feature interpolation, enabling robust reconstruction of occluded regions while preserving geometric details; and (ii) an identity-preserving diffusion inpainting module that integrates textual inversion with semantic-conditioned guidance for subject-specific, temporally coherent completion. Unlike SDS-based methods, our approach employs direct pixel-level supervision to ensure identity fidelity. Experiments on synthetic benchmarks (PeopleSnapshot, ZJU-MoCap) and real-world scenarios (OcMotion) demonstrate competitive performance with consistent improvements in reconstruction quality across diverse poses and viewpoints.",
    "arxiv_url": "https://arxiv.org/abs/2601.02098v1",
    "pdf_url": "https://arxiv.org/pdf/2601.02098v1",
    "published_date": "2026-01-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "avatar",
      "geometry",
      "gaussian splatting",
      "ar",
      "semantic",
      "human",
      "3d gaussian",
      "mapping",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.02098v1",
      "pdf": "https://arxiv.org/pdf/2601.02098v1"
    },
    "bibtex": ""
  },
  {
    "title": "SketchRodGS: Sketch-based Extraction of Slender Geometries for Animating Gaussian Splatting Scenes",
    "authors": [
      "Haato Watanabe",
      "Nobuyuki Umetani"
    ],
    "abstract": "Physics simulation of slender elastic objects often requires discretization as a polyline. However, constructing a polyline from Gaussian splatting is challenging as Gaussian splatting lacks connectivity information and the configuration of Gaussian primitives contains much noise. This paper presents a method to extract a polyline representation of the slender part of the objects in a Gaussian splatting scene from the user's sketching input. Our method robustly constructs a polyline mesh that represents the slender parts using the screen-space shortest path analysis that can be efficiently solved using dynamic programming. We demonstrate the effectiveness of our approach in several in-the-wild examples.",
    "arxiv_url": "https://arxiv.org/abs/2601.02072v1",
    "pdf_url": "https://arxiv.org/pdf/2601.02072v1",
    "published_date": "2026-01-05",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.02072v1",
      "pdf": "https://arxiv.org/pdf/2601.02072v1"
    },
    "bibtex": ""
  },
  {
    "title": "ESGaussianFace: Emotional and Stylized Audio-Driven Facial Animation via 3D Gaussian Splatting",
    "authors": [
      "Chuhang Ma",
      "Shuai Tan",
      "Ye Pan",
      "Jiaolong Yang",
      "Xin Tong"
    ],
    "abstract": "Most current audio-driven facial animation research primarily focuses on generating videos with neutral emotions. While some studies have addressed the generation of facial videos driven by emotional audio, efficiently generating high-quality talking head videos that integrate both emotional expressions and style features remains a significant challenge. In this paper, we propose ESGaussianFace, an innovative framework for emotional and stylized audio-driven facial animation. Our approach leverages 3D Gaussian Splatting to reconstruct 3D scenes and render videos, ensuring efficient generation of 3D consistent results. We propose an emotion-audio-guided spatial attention method that effectively integrates emotion features with audio content features. Through emotion-guided attention, the model is able to reconstruct facial details across different emotional states more accurately. To achieve emotional and stylized deformations of the 3D Gaussian points through emotion and style features, we introduce two 3D Gaussian deformation predictors. Futhermore, we propose a multi-stage training strategy, enabling the step-by-step learning of the character's lip movements, emotional variations, and style features. Our generated results exhibit high efficiency, high quality, and 3D consistency. Extensive experimental results demonstrate that our method outperforms existing state-of-the-art techniques in terms of lip movement accuracy, expression variation, and style feature expressiveness.",
    "arxiv_url": "https://arxiv.org/abs/2601.01847v1",
    "pdf_url": "https://arxiv.org/pdf/2601.01847v1",
    "published_date": "2026-01-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "face",
      "high quality",
      "deformation",
      "animation",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "efficient",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.01847v1",
      "pdf": "https://arxiv.org/pdf/2601.01847v1"
    },
    "bibtex": ""
  },
  {
    "title": "MANGO:Natural Multi-speaker 3D Talking Head Generation via 2D-Lifted Enhancement",
    "authors": [
      "Lei Zhu",
      "Lijian Lin",
      "Ye Zhu",
      "Jiahao Wu",
      "Xuehan Hou",
      "Yu Li",
      "Yunfei Liu",
      "Jie Chen"
    ],
    "abstract": "Current audio-driven 3D head generation methods mainly focus on single-speaker scenarios, lacking natural, bidirectional listen-and-speak interaction. Achieving seamless conversational behavior, where speaking and listening states transition fluidly remains a key challenge. Existing 3D conversational avatar approaches rely on error-prone pseudo-3D labels that fail to capture fine-grained facial dynamics. To address these limitations, we introduce a novel two-stage framework MANGO, which leveraging pure image-level supervision by alternately training to mitigate the noise introduced by pseudo-3D labels, thereby achieving better alignment with real-world conversational behaviors. Specifically, in the first stage, a diffusion-based transformer with a dual-audio interaction module models natural 3D motion from multi-speaker audio. In the second stage, we use a fast 3D Gaussian Renderer to generate high-fidelity images and provide 2D-level photometric supervision for the 3D motions through alternate training. Additionally, we introduce MANGO-Dialog, a high-quality dataset with over 50 hours of aligned 2D-3D conversational data across 500+ identities. Extensive experiments demonstrate that our method achieves exceptional accuracy and realism in modeling two-person 3D dialogue motion, significantly advancing the fidelity and controllability of audio-driven talking heads.",
    "arxiv_url": "https://arxiv.org/abs/2601.01749v1",
    "pdf_url": "https://arxiv.org/pdf/2601.01749v1",
    "published_date": "2026-01-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "dynamic",
      "high-fidelity",
      "avatar",
      "ar",
      "3d gaussian",
      "fast",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.01749v1",
      "pdf": "https://arxiv.org/pdf/2601.01749v1"
    },
    "bibtex": ""
  },
  {
    "title": "Animated 3DGS Avatars in Diverse Scenes with Consistent Lighting and Shadows",
    "authors": [
      "Aymen Mir",
      "Riza Alp Guler",
      "Jian Wang",
      "Gerard Pons-Moll",
      "Bing Zhou"
    ],
    "abstract": "We present a method for consistent lighting and shadows when animated 3D Gaussian Splatting (3DGS) avatars interact with 3DGS scenes or with dynamic objects inserted into otherwise static scenes. Our key contribution is Deep Gaussian Shadow Maps (DGSM), a modern analogue of the classical shadow mapping algorithm tailored to the volumetric 3DGS representation. Building on the classic deep shadow mapping idea, we show that 3DGS admits closed form light accumulation along light rays, enabling volumetric shadow computation without meshing. For each estimated light, we tabulate transmittance over concentric radial shells and store them in octahedral atlases, which modern GPUs can sample in real time per query to attenuate affected scene Gaussians and thus cast and receive shadows consistently. To relight moving avatars, we approximate the local environment illumination with HDRI probes represented in a spherical harmonic (SH) basis and apply a fast per Gaussian radiance transfer, avoiding explicit BRDF estimation or offline optimization. We demonstrate environment consistent lighting for avatars from AvatarX and ActorsHQ, composited into ScanNet++, DL3DV, and SuperSplat scenes, and show interactions with inserted objects. Across single and multi avatar settings, DGSM and SH relighting operate fully in the volumetric 3DGS representation, yielding coherent shadows and relighting while avoiding meshing.",
    "arxiv_url": "https://arxiv.org/abs/2601.01660v1",
    "pdf_url": "https://arxiv.org/pdf/2601.01660v1",
    "published_date": "2026-01-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "illumination",
      "dynamic",
      "shadow",
      "avatar",
      "ar",
      "mapping",
      "3d gaussian",
      "lighting",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.01660v1",
      "pdf": "https://arxiv.org/pdf/2601.01660v1"
    },
    "bibtex": ""
  },
  {
    "title": "ShadowGS: Shadow-Aware 3D Gaussian Splatting for Satellite Imagery",
    "authors": [
      "Feng Luo",
      "Hongbo Pan",
      "Xiang Yang",
      "Baoyu Jiang",
      "Fengqing Liu",
      "Tao Huang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a novel paradigm for 3D reconstruction from satellite imagery. However, in multi-temporal satellite images, prevalent shadows exhibit significant inconsistencies due to varying illumination conditions. To address this, we propose ShadowGS, a novel framework based on 3DGS. It leverages a physics-based rendering equation from remote sensing, combined with an efficient ray marching technique, to precisely model geometrically consistent shadows while maintaining efficient rendering. Additionally, it effectively disentangles different illumination components and apparent attributes in the scene. Furthermore, we introduce a shadow consistency constraint that significantly enhances the geometric accuracy of 3D reconstruction. We also incorporate a novel shadow map prior to improve performance with sparse-view inputs. Extensive experiments demonstrate that ShadowGS outperforms current state-of-the-art methods in shadow decoupling accuracy, 3D reconstruction precision, and novel view synthesis quality, with only a few minutes of training. ShadowGS exhibits robust performance across various settings, including RGB, pansharpened, and sparse-view satellite inputs.",
    "arxiv_url": "https://arxiv.org/abs/2601.00939v1",
    "pdf_url": "https://arxiv.org/pdf/2601.00939v1",
    "published_date": "2026-01-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "sparse-view",
      "shadow",
      "ray marching",
      "3d reconstruction",
      "ar",
      "efficient rendering",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.00939v1",
      "pdf": "https://arxiv.org/pdf/2601.00939v1"
    },
    "bibtex": ""
  },
  {
    "title": "ParkGaussian: Surround-view 3D Gaussian Splatting for Autonomous Parking",
    "authors": [
      "Xiaobao Wei",
      "Zhangjie Ye",
      "Yuxiang Gu",
      "Zunjie Zhu",
      "Yunfei Guo",
      "Yingying Shen",
      "Shan Zhao",
      "Ming Lu",
      "Haiyang Sun",
      "Bing Wang",
      "Guang Chen",
      "Rongfeng Lu",
      "Hangjun Ye"
    ],
    "abstract": "Parking is a critical task for autonomous driving systems (ADS), with unique challenges in crowded parking slots and GPS-denied environments. However, existing works focus on 2D parking slot perception, mapping, and localization, 3D reconstruction remains underexplored, which is crucial for capturing complex spatial geometry in parking scenarios. Naively improving the visual quality of reconstructed parking scenes does not directly benefit autonomous parking, as the key entry point for parking is the slots perception module. To address these limitations, we curate the first benchmark named ParkRecon3D, specifically designed for parking scene reconstruction. It includes sensor data from four surround-view fisheye cameras with calibrated extrinsics and dense parking slot annotations. We then propose ParkGaussian, the first framework that integrates 3D Gaussian Splatting (3DGS) for parking scene reconstruction. To further improve the alignment between reconstruction and downstream parking slot detection, we introduce a slot-aware reconstruction strategy that leverages existing parking perception methods to enhance the synthesis quality of slot regions. Experiments on ParkRecon3D demonstrate that ParkGaussian achieves state-of-the-art reconstruction quality and better preserves perception consistency for downstream tasks. The code and dataset will be released at: https://github.com/wm-research/ParkGaussian",
    "arxiv_url": "https://arxiv.org/abs/2601.01386v1",
    "pdf_url": "https://arxiv.org/pdf/2601.01386v1",
    "published_date": "2026-01-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/wm-research/ParkGaussian",
    "keywords": [
      "localization",
      "3d reconstruction",
      "geometry",
      "autonomous driving",
      "ar",
      "3d gaussian",
      "mapping",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.01386v1",
      "pdf": "https://arxiv.org/pdf/2601.01386v1",
      "github": "https://github.com/wm-research/ParkGaussian"
    },
    "bibtex": ""
  },
  {
    "title": "Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians",
    "authors": [
      "Melonie de Almeida",
      "Daniela Ivanova",
      "Tong Shi",
      "John H. Williamson",
      "Paul Henderson"
    ],
    "abstract": "Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at https://melonienimasha.github.io/Pixel-to-4D-Website.",
    "arxiv_url": "https://arxiv.org/abs/2601.00678v1",
    "pdf_url": "https://arxiv.org/pdf/2601.00678v1",
    "published_date": "2026-01-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "ar",
      "human",
      "3d gaussian",
      "fast",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.00678v1",
      "pdf": "https://arxiv.org/pdf/2601.00678v1",
      "project": "https://melonienimasha.github.io/Pixel-to-4D-Website"
    },
    "bibtex": ""
  },
  {
    "title": "Joint Geometry-Appearance Human Reconstruction in a Unified Latent Space via Bridge Diffusion",
    "authors": [
      "Yingzhi Tang",
      "Qijian Zhang",
      "Junhui Hou"
    ],
    "abstract": "Achieving consistent and high-fidelity geometry and appearance reconstruction of 3D digital humans from a single RGB image is inherently a challenging task. Existing studies typically resort to decoupled pipelines for geometry estimation and appearance synthesis, often hindering unified reconstruction and causing inconsistencies. This paper introduces \\textbf{JGA-LBD}, a novel framework that unifies the modeling of geometry and appearance into a joint latent representation and formulates the generation process as bridge diffusion. Observing that directly integrating heterogeneous input conditions (e.g., depth maps, SMPL models) leads to substantial training difficulties, we unify all conditions into the 3D Gaussian representations, which can be further compressed into a unified latent space through a shared sparse variational autoencoder (VAE). Subsequently, the specialized form of bridge diffusion enables to start with a partial observation of the target latent code and solely focuses on inferring the missing components. Finally, a dedicated decoding module extracts the complete 3D human geometric structure and renders novel views from the inferred latent representation. Experiments demonstrate that JGA-LBD outperforms current state-of-the-art approaches in terms of both geometry fidelity and appearance quality, including challenging in-the-wild scenarios. Our code will be made publicly available at https://github.com/haiantyz/JGA-LBD.",
    "arxiv_url": "https://arxiv.org/abs/2601.00328v1",
    "pdf_url": "https://arxiv.org/pdf/2601.00328v1",
    "published_date": "2026-01-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/haiantyz/JGA-LBD",
    "keywords": [
      "high-fidelity",
      "geometry",
      "ar",
      "human",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.00328v1",
      "pdf": "https://arxiv.org/pdf/2601.00328v1",
      "github": "https://github.com/haiantyz/JGA-LBD"
    },
    "bibtex": ""
  },
  {
    "title": "SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting",
    "authors": [
      "Jun-Jee Chao",
      "Volkan Isler"
    ],
    "abstract": "Reconstructing a dynamic target moving over a large area is challenging. Standard approaches for dynamic object reconstruction require dense coverage in both the viewing space and the temporal dimension, typically relying on multi-view videos captured at each time step. However, such setups are only possible in constrained environments. In real-world scenarios, observations are often sparse over time and captured sparsely from diverse viewpoints (e.g., from security cameras), making dynamic reconstruction highly ill-posed. We present SV-GS, a framework that simultaneously estimates a deformation model and the object's motion over time under sparse observations. To initialize SV-GS, we leverage a rough skeleton graph and an initial static reconstruction as inputs to guide motion estimation. (Later, we show that this input requirement can be relaxed.) Our method optimizes a skeleton-driven deformation field composed of a coarse skeleton joint pose estimator and a module for fine-grained deformations. By making only the joint pose estimator time-dependent, our model enables smooth motion interpolation while preserving learned geometric details. Experiments on synthetic datasets show that our method outperforms existing approaches under sparse observations by up to 34% in PSNR, and achieves comparable performance to dense monocular video methods on real-world datasets despite using significantly fewer frames. Moreover, we demonstrate that the input initial static reconstruction can be replaced by a diffusion-based generative prior, making our method more practical for real-world scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2601.00285v1",
    "pdf_url": "https://arxiv.org/pdf/2601.00285v1",
    "published_date": "2026-01-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "sparse view",
      "deformation",
      "gaussian splatting",
      "ar",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.00285v1",
      "pdf": "https://arxiv.org/pdf/2601.00285v1"
    },
    "bibtex": ""
  },
  {
    "title": "Clean-GS: Semantic Mask-Guided Pruning for 3D Gaussian Splatting",
    "authors": [
      "Subhankar Mishra"
    ],
    "abstract": "3D Gaussian Splatting produces high-quality scene reconstructions but generates hundreds of thousands of spurious Gaussians (floaters) scattered throughout the environment. These artifacts obscure objects of interest and inflate model sizes, hindering deployment in bandwidth-constrained applications. We present Clean-GS, a method for removing background clutter and floaters from 3DGS reconstructions using sparse semantic masks. Our approach combines whitelist-based spatial filtering with color-guided validation and outlier removal to achieve 60-80\\% model compression while preserving object quality. Unlike existing 3DGS pruning methods that rely on global importance metrics, Clean-GS uses semantic information from as few as 3 segmentation masks (1\\% of views) to identify and remove Gaussians not belonging to the target object. Our multi-stage approach consisting of (1) whitelist filtering via projection to masked regions, (2) depth-buffered color validation, and (3) neighbor-based outlier removal isolates monuments and objects from complex outdoor scenes. Experiments on Tanks and Temples show that Clean-GS reduces file sizes from 125MB to 47MB while maintaining rendering quality, making 3DGS models practical for web deployment and AR/VR applications. Our code is available at https://github.com/smlab-niser/clean-gs",
    "arxiv_url": "https://arxiv.org/abs/2601.00913v1",
    "pdf_url": "https://arxiv.org/pdf/2601.00913v1",
    "published_date": "2026-01-01",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "https://github.com/smlab-niser/clean-gs",
    "keywords": [
      "outdoor",
      "vr",
      "compression",
      "segmentation",
      "ar",
      "semantic",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.00913v1",
      "pdf": "https://arxiv.org/pdf/2601.00913v1",
      "github": "https://github.com/smlab-niser/clean-gs"
    },
    "bibtex": ""
  },
  {
    "title": "PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes",
    "authors": [
      "Luca Collorone",
      "Mert Kiray",
      "Indro Spinelli",
      "Fabio Galasso",
      "Benjamin Busam"
    ],
    "abstract": "Realistic visual simulations are omnipresent, yet their creation requires computing time, rendering, and expert animation knowledge. Open-vocabulary visual effects generation from text inputs emerges as a promising solution that can unlock immense creative potential. However, current pipelines lack both physical realism and effective language interfaces, requiring slow offline optimization. In contrast, PhysTalk takes a 3D Gaussian Splatting (3DGS) scene as input and translates arbitrary user prompts into real time, physics based, interactive 4D animations. A large language model (LLM) generates executable code that directly modifies 3DGS parameters through lightweight proxies and particle dynamics. Notably, PhysTalk is the first framework to couple 3DGS directly with a physics simulator without relying on time consuming mesh extraction. While remaining open vocabulary, this design enables interactive 3D Gaussian animation via collision aware, physics based manipulation of arbitrary, multi material objects. Finally, PhysTalk is train-free and computationally lightweight: this makes 4D animation broadly accessible and shifts these workflows from a \"render and wait\" paradigm toward an interactive dialogue with a modern, physics-informed pipeline.",
    "arxiv_url": "https://arxiv.org/abs/2512.24986v1",
    "pdf_url": "https://arxiv.org/pdf/2512.24986v1",
    "published_date": "2025-12-31",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "lightweight",
      "face",
      "animation",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.24986v1",
      "pdf": "https://arxiv.org/pdf/2512.24986v1"
    },
    "bibtex": ""
  },
  {
    "title": "UniC-Lift: Unified 3D Instance Segmentation via Contrastive Learning",
    "authors": [
      "Ankit Dhiman",
      "Srinath R",
      "Jaswanth Reddy",
      "Lokesh R Boregowda",
      "Venkatesh Babu Radhakrishnan"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have advanced novel-view synthesis. Recent methods extend multi-view 2D segmentation to 3D, enabling instance/semantic segmentation for better scene understanding. A key challenge is the inconsistency of 2D instance labels across views, leading to poor 3D predictions. Existing methods use a two-stage approach in which some rely on contrastive learning with hyperparameter-sensitive clustering, while others preprocess labels for consistency. We propose a unified framework that merges these steps, reducing training time and improving performance by introducing a learnable feature embedding for segmentation in Gaussian primitives. This embedding is then efficiently decoded into instance labels through a novel \"Embedding-to-Label\" process, effectively integrating the optimization. While this unified framework offers substantial benefits, we observed artifacts at the object boundaries. To address the object boundary issues, we propose hard-mining samples along these boundaries. However, directly applying hard mining to the feature embeddings proved unstable. Therefore, we apply a linear layer to the rasterized feature embeddings before calculating the triplet loss, which stabilizes training and significantly improves performance. Our method outperforms baselines qualitatively and quantitatively on the ScanNet, Replica3D, and Messy-Rooms datasets.",
    "arxiv_url": "https://arxiv.org/abs/2512.24763v1",
    "pdf_url": "https://arxiv.org/pdf/2512.24763v1",
    "published_date": "2025-12-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "segmentation",
      "ar",
      "semantic",
      "nerf",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.24763v1",
      "pdf": "https://arxiv.org/pdf/2512.24763v1"
    },
    "bibtex": ""
  },
  {
    "title": "Splatwizard: A Benchmark Toolkit for 3D Gaussian Splatting Compression",
    "authors": [
      "Xiang Liu",
      "Yimin Zhou",
      "Jinxiang Wang",
      "Yujun Huang",
      "Shuzhao Xie",
      "Shiyu Qin",
      "Mingyao Hong",
      "Jiawei Li",
      "Yaowei Wang",
      "Zhi Wang",
      "Shu-Tao Xia",
      "Bin Chen"
    ],
    "abstract": "The recent advent of 3D Gaussian Splatting (3DGS) has marked a significant breakthrough in real-time novel view synthesis. However, the rapid proliferation of 3DGS-based algorithms has created a pressing need for standardized and comprehensive evaluation tools, especially for compression task. Existing benchmarks often lack the specific metrics necessary to holistically assess the unique characteristics of different methods, such as rendering speed, rate distortion trade-offs memory efficiency, and geometric accuracy. To address this gap, we introduce Splatwizard, a unified benchmark toolkit designed specifically for benchmarking 3DGS compression models. Splatwizard provides an easy-to-use framework to implement new 3DGS compression model and utilize state-of-the-art techniques proposed by previous work. Besides, an integrated pipeline that automates the calculation of key performance indicators, including image-based quality metrics, chamfer distance of reconstruct mesh, rendering frame rates, and computational resource consumption is included in the framework as well. Code is available at https://github.com/splatwizard/splatwizard",
    "arxiv_url": "https://arxiv.org/abs/2512.24742v1",
    "pdf_url": "https://arxiv.org/pdf/2512.24742v1",
    "published_date": "2025-12-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/splatwizard/splatwizard",
    "keywords": [
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.24742v1",
      "pdf": "https://arxiv.org/pdf/2512.24742v1",
      "github": "https://github.com/splatwizard/splatwizard"
    },
    "bibtex": ""
  },
  {
    "title": "Structure-Guided Allocation of 2D Gaussians for Image Representation and Compression",
    "authors": [
      "Huanxiong Liang",
      "Yunuo Chen",
      "Yicheng Pan",
      "Sixian Wang",
      "Jincheng Dai",
      "Guo Lu",
      "Wenjun Zhang"
    ],
    "abstract": "Recent advances in 2D Gaussian Splatting (2DGS) have demonstrated its potential as a compact image representation with millisecond-level decoding. However, existing 2DGS-based pipelines allocate representation capacity and parameter precision largely oblivious to image structure, limiting their rate-distortion (RD) efficiency at low bitrates. To address this, we propose a structure-guided allocation principle for 2DGS, which explicitly couples image structure with both representation capacity and quantization precision, while preserving native decoding speed. First, we introduce a structure-guided initialization that assigns 2D Gaussians according to spatial structural priors inherent in natural images, yielding a localized and semantically meaningful distribution. Second, during quantization-aware fine-tuning, we propose adaptive bitwidth quantization of covariance parameters, which grants higher precision to small-scale Gaussians in complex regions and lower precision elsewhere, enabling RD-aware optimization, thereby reducing redundancy without degrading edge quality. Third, we impose a geometry-consistent regularization that aligns Gaussian orientations with local gradient directions to better preserve structural details. Extensive experiments demonstrate that our approach substantially improves both the representational power and the RD performance of 2DGS while maintaining over 1000 FPS decoding. Compared with the baseline GSImage, we reduce BD-rate by 43.44% on Kodak and 29.91% on DIV2K.",
    "arxiv_url": "https://arxiv.org/abs/2512.24018v1",
    "pdf_url": "https://arxiv.org/pdf/2512.24018v1",
    "published_date": "2025-12-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "compression",
      "geometry",
      "ar",
      "semantic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.24018v1",
      "pdf": "https://arxiv.org/pdf/2512.24018v1"
    },
    "bibtex": ""
  },
  {
    "title": "Improved 3D Gaussian Splatting of Unknown Spacecraft Structure Using Space Environment Illumination Knowledge",
    "authors": [
      "Tae Ha Park",
      "Simone D'Amico"
    ],
    "abstract": "This work presents a novel pipeline to recover the 3D structure of an unknown target spacecraft from a sequence of images captured during Rendezvous and Proximity Operations (RPO) in space. The target's geometry and appearance are represented as a 3D Gaussian Splatting (3DGS) model. However, learning 3DGS requires static scenes, an assumption in contrast to dynamic lighting conditions encountered in spaceborne imagery. The trained 3DGS model can also be used for camera pose estimation through photometric optimization. Therefore, in addition to recovering a geometrically accurate 3DGS model, the photometric accuracy of the rendered images is imperative to downstream pose estimation tasks during the RPO process. This work proposes to incorporate the prior knowledge of the Sun's position, estimated and maintained by the servicer spacecraft, into the training pipeline for improved photometric quality of 3DGS rasterization. Experimental studies demonstrate the effectiveness of the proposed solution, as 3DGS models trained on a sequence of images learn to adapt to rapidly changing illumination conditions in space and reflect global shadowing and self-occlusion.",
    "arxiv_url": "https://arxiv.org/abs/2512.23998v1",
    "pdf_url": "https://arxiv.org/pdf/2512.23998v1",
    "published_date": "2025-12-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "dynamic",
      "shadow",
      "geometry",
      "ar",
      "3d gaussian",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.23998v1",
      "pdf": "https://arxiv.org/pdf/2512.23998v1"
    },
    "bibtex": ""
  },
  {
    "title": "DriveExplorer: Images-Only Decoupled 4D Reconstruction with Progressive Restoration for Driving View Extrapolation",
    "authors": [
      "Yuang Jia",
      "Jinlong Wang",
      "Jiayi Zhao",
      "Chunlam Li",
      "Shunzhou Wang",
      "Wei Gao"
    ],
    "abstract": "This paper presents an effective solution for view extrapolation in autonomous driving scenarios. Recent approaches focus on generating shifted novel view images from given viewpoints using diffusion models. However, these methods heavily rely on priors such as LiDAR point clouds, 3D bounding boxes, and lane annotations, which demand expensive sensors or labor-intensive labeling, limiting applicability in real-world deployment. In this work, with only images and optional camera poses, we first estimate a global static point cloud and per-frame dynamic point clouds, fusing them into a unified representation. We then employ a deformable 4D Gaussian framework to reconstruct the scene. The initially trained 4D Gaussian model renders degraded and pseudo-images to train a video diffusion model. Subsequently, progressively shifted Gaussian renderings are iteratively refined by the diffusion model,and the enhanced results are incorporated back as training data for 4DGS. This process continues until extrapolation reaches the target viewpoints. Compared with baselines, our method produces higher-quality images at novel extrapolated viewpoints.",
    "arxiv_url": "https://arxiv.org/abs/2512.23983v1",
    "pdf_url": "https://arxiv.org/pdf/2512.23983v1",
    "published_date": "2025-12-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "autonomous driving",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.23983v1",
      "pdf": "https://arxiv.org/pdf/2512.23983v1"
    },
    "bibtex": ""
  },
  {
    "title": "Contour Information Aware 2D Gaussian Splatting for Image Representation",
    "authors": [
      "Masaya Takabe",
      "Hiroshi Watanabe",
      "Sujun Hong",
      "Tomohiro Ikai",
      "Zheming Fan",
      "Ryo Ishimoto",
      "Kakeru Sugimoto",
      "Ruri Imichi"
    ],
    "abstract": "Image representation is a fundamental task in computer vision. Recently, Gaussian Splatting has emerged as an efficient representation framework, and its extension to 2D image representation enables lightweight, yet expressive modeling of visual content. While recent 2D Gaussian Splatting (2DGS) approaches provide compact storage and real-time decoding, they often produce blurry or indistinct boundaries when the number of Gaussians is small due to the lack of contour awareness. In this work, we propose a Contour Information-Aware 2D Gaussian Splatting framework that incorporates object segmentation priors into Gaussian-based image representation. By constraining each Gaussian to a specific segmentation region during rasterization, our method prevents cross-boundary blending and preserves edge structures under high compression. We also introduce a warm-up scheme to stabilize training and improve convergence. Experiments on synthetic color charts and the DAVIS dataset demonstrate that our approach achieves higher reconstruction quality around object edges compared to existing 2DGS methods. The improvement is particularly evident in scenarios with very few Gaussians, while our method still maintains fast rendering and low memory usage.",
    "arxiv_url": "https://arxiv.org/abs/2512.23255v1",
    "pdf_url": "https://arxiv.org/pdf/2512.23255v1",
    "published_date": "2025-12-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "lightweight",
      "compression",
      "segmentation",
      "ar",
      "efficient",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.23255v1",
      "pdf": "https://arxiv.org/pdf/2512.23255v1"
    },
    "bibtex": ""
  },
  {
    "title": "GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation",
    "authors": [
      "Tianchen Deng",
      "Xuefeng Chen",
      "Yi Chen",
      "Qu Chen",
      "Yuyao Xu",
      "Lijin Yang",
      "Le Xu",
      "Yu Zhang",
      "Bo Zhang",
      "Wuxiong Huang",
      "Hesheng Wang"
    ],
    "abstract": "Driving World Models (DWMs) have been developing rapidly with the advances of generative models. However, existing DWMs lack 3D scene understanding capabilities and can only generate content conditioned on input data, without the ability to interpret or reason about the driving environment. Moreover, current approaches represent 3D spatial information with point cloud or BEV features do not accurately align textual information with the underlying 3D scene. To address these limitations, we propose a novel unified DWM framework based on 3D Gaussian scene representation, which enables both 3D scene understanding and multi-modal scene generation, while also enabling contextual enrichment for understanding and generation tasks. Our approach directly aligns textual information with the 3D scene by embedding rich linguistic features into each Gaussian primitive, thereby achieving early modality alignment. In addition, we design a novel task-aware language-guided sampling strategy that removes redundant 3D Gaussians and injects accurate and compact 3D tokens into LLM. Furthermore, we design a dual-condition multi-modal generation model, where the information captured by our vision-language model is leveraged as a high-level language condition in combination with a low-level image condition, jointly guiding the multi-modal generation process. We conduct comprehensive studies on the nuScenes, and NuInteract datasets to validate the effectiveness of our framework. Our method achieves state-of-the-art performance. We will release the code publicly on GitHub https://github.com/dtc111111/GaussianDWM.",
    "arxiv_url": "https://arxiv.org/abs/2512.23180v2",
    "pdf_url": "https://arxiv.org/pdf/2512.23180v2",
    "published_date": "2025-12-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/dtc111111/GaussianDWM",
    "keywords": [
      "3d gaussian",
      "compact",
      "ar",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.23180v2",
      "pdf": "https://arxiv.org/pdf/2512.23180v2",
      "github": "https://github.com/dtc111111/GaussianDWM"
    },
    "bibtex": ""
  },
  {
    "title": "GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection",
    "authors": [
      "Yi Zhang",
      "Yi Wang",
      "Lei Yao",
      "Lap-Pui Chau"
    ],
    "abstract": "Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).",
    "arxiv_url": "https://arxiv.org/abs/2512.23176v1",
    "pdf_url": "https://arxiv.org/pdf/2512.23176v1",
    "published_date": "2025-12-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "face",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.23176v1",
      "pdf": "https://arxiv.org/pdf/2512.23176v1"
    },
    "bibtex": ""
  },
  {
    "title": "Hash Grid Feature Pruning",
    "authors": [
      "Yangzhi Ma",
      "Bojun Liu",
      "Jie Li",
      "Li Li",
      "Dong Liu"
    ],
    "abstract": "Hash grids are widely used to learn an implicit neural field for Gaussian splatting, serving either as part of the entropy model or for inter-frame prediction. However, due to the irregular and non-uniform distribution of Gaussian splats in 3D space, numerous sparse regions exist, rendering many features in the hash grid invalid. This leads to redundant storage and transmission overhead. In this work, we propose a hash grid feature pruning method that identifies and prunes invalid features based on the coordinates of the input Gaussian splats, so that only the valid features are encoded. This approach reduces the storage size of the hash grid without compromising model performance, leading to improved rate-distortion performance. Following the Common Test Conditions (CTC) defined by the standardization committee, our method achieves an average bitrate reduction of 8% compared to the baseline approach.",
    "arxiv_url": "https://arxiv.org/abs/2512.22882v1",
    "pdf_url": "https://arxiv.org/pdf/2512.22882v1",
    "published_date": "2025-12-28",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.22882v1",
      "pdf": "https://arxiv.org/pdf/2512.22882v1"
    },
    "bibtex": ""
  },
  {
    "title": "3D Scene Change Modeling With Consistent Multi-View Aggregation",
    "authors": [
      "Zirui Zhou",
      "Junfeng Ni",
      "Shujie Zhang",
      "Yixin Chen",
      "Siyuan Huang"
    ],
    "abstract": "Change detection plays a vital role in scene monitoring, exploration, and continual reconstruction. Existing 3D change detection methods often exhibit spatial inconsistency in the detected changes and fail to explicitly separate pre- and post-change states. To address these limitations, we propose SCaR-3D, a novel 3D scene change detection framework that identifies object-level changes from a dense-view pre-change image sequence and sparse-view post-change images. Our approach consists of a signed-distance-based 2D differencing module followed by multi-view aggregation with voting and pruning, leveraging the consistent nature of 3DGS to robustly separate pre- and post-change states. We further develop a continual scene reconstruction strategy that selectively updates dynamic regions while preserving the unchanged areas. We also contribute CCS3D, a challenging synthetic dataset that allows flexible combinations of 3D change types to support controlled evaluations. Extensive experiments demonstrate that our method achieves both high accuracy and efficiency, outperforming existing methods.",
    "arxiv_url": "https://arxiv.org/abs/2512.22830v1",
    "pdf_url": "https://arxiv.org/pdf/2512.22830v1",
    "published_date": "2025-12-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "sparse-view",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.22830v1",
      "pdf": "https://arxiv.org/pdf/2512.22830v1"
    },
    "bibtex": ""
  },
  {
    "title": "Medical Scene Reconstruction and Segmentation based on 3D Gaussian Representation",
    "authors": [
      "Bin Liu",
      "Wenyan Tian",
      "Huangxin Fu",
      "Zizheng Li",
      "Zhifen He",
      "Bo Li"
    ],
    "abstract": "3D reconstruction of medical images is a key technology in medical image analysis and clinical diagnosis, providing structural visualization support for disease assessment and surgical planning. Traditional methods are computationally expensive and prone to structural discontinuities and loss of detail in sparse slices, making it difficult to meet clinical accuracy requirements.To address these challenges, we propose an efficient 3D reconstruction method based on 3D Gaussian and tri-plane representations. This method not only maintains the advantages of Gaussian representation in efficient rendering and geometric representation but also significantly enhances structural continuity and semantic consistency under sparse slicing conditions. Experimental results on multimodal medical datasets such as US and MRI show that our proposed method can generate high-quality, anatomically coherent, and semantically stable medical images under sparse data conditions, while significantly improving reconstruction efficiency. This provides an efficient and reliable new approach for 3D visualization and clinical analysis of medical images.",
    "arxiv_url": "https://arxiv.org/abs/2512.22800v1",
    "pdf_url": "https://arxiv.org/pdf/2512.22800v1",
    "published_date": "2025-12-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "medical",
      "3d reconstruction",
      "segmentation",
      "ar",
      "efficient rendering",
      "semantic",
      "3d gaussian",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.22800v1",
      "pdf": "https://arxiv.org/pdf/2512.22800v1"
    },
    "bibtex": ""
  },
  {
    "title": "Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting",
    "authors": [
      "Yiqian Li",
      "Wen Jiang",
      "Kostas Daniilidis"
    ],
    "abstract": "Understanding semantics and dynamics has been crucial for embodied agents in various tasks. Both tasks have much more data redundancy than the static scene understanding task. We formulate the view selection problem as an active learning problem, where the goal is to prioritize frames that provide the greatest information gain for model training. To this end, we propose an active learning algorithm with Fisher Information that quantifies the informativeness of candidate views with respect to both semantic Gaussian parameters and deformation networks. This formulation allows our method to jointly handle semantic reasoning and dynamic scene modeling, providing a principled alternative to heuristic or random strategies. We evaluate our method on large-scale static images and dynamic video datasets by selecting informative frames from multi-camera setups. Experimental results demonstrate that our approach consistently improves rendering quality and semantic segmentation performance, outperforming baseline methods based on random selection and uncertainty-based heuristics.",
    "arxiv_url": "https://arxiv.org/abs/2512.22771v1",
    "pdf_url": "https://arxiv.org/pdf/2512.22771v1",
    "published_date": "2025-12-28",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "understanding",
      "dynamic",
      "deformation",
      "segmentation",
      "ar",
      "semantic",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.22771v1",
      "pdf": "https://arxiv.org/pdf/2512.22771v1"
    },
    "bibtex": ""
  },
  {
    "title": "RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization",
    "authors": [
      "Wei-Tse Cheng",
      "Yen-Jen Chiou",
      "Yuan-Fu Yang"
    ],
    "abstract": "We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS. Additional details and resources are available at this URL: https://breeze1124.github.io/rgs-slam-project-page/",
    "arxiv_url": "https://arxiv.org/abs/2601.00705v3",
    "pdf_url": "https://arxiv.org/pdf/2601.00705v3",
    "published_date": "2025-12-28",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "geometry",
      "ar",
      "slam",
      "mapping",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2601.00705v3",
      "pdf": "https://arxiv.org/pdf/2601.00705v3",
      "project": "https://breeze1124.github.io/rgs-slam-project-page"
    },
    "bibtex": ""
  },
  {
    "title": "SCPainter: A Unified Framework for Realistic 3D Asset Insertion and Novel View Synthesis",
    "authors": [
      "Paul Dobre",
      "Jackson Cooper",
      "Xin Wang",
      "Hongzhou Yang"
    ],
    "abstract": "3D Asset insertion and novel view synthesis (NVS) are key components for autonomous driving simulation, enhancing the diversity of training data. With better training data that is diverse and covers a wide range of situations, including long-tailed driving scenarios, autonomous driving models can become more robust and safer. This motivates a unified simulation framework that can jointly handle realistic integration of inserted 3D assets and NVS. Recent 3D asset reconstruction methods enable reconstruction of dynamic actors from video, supporting their re-insertion into simulated driving scenes. While the overall structure and appearance can be accurate, it still struggles to capture the realism of 3D assets through lighting or shadows, particularly when inserted into scenes. In parallel, recent advances in NVS methods have demonstrated promising results in synthesizing viewpoints beyond the originally recorded trajectories. However, existing approaches largely treat asset insertion and NVS capabilities in isolation. To allow for interaction with the rest of the scene and to enable more diverse creation of new scenarios for training, realistic 3D asset insertion should be combined with NVS. To address this, we present SCPainter (Street Car Painter), a unified framework which integrates 3D Gaussian Splat (GS) car asset representations and 3D scene point clouds with diffusion-based generation to jointly enable realistic 3D asset insertion and NVS. The 3D GS assets and 3D scene point clouds are projected together into novel views, and these projections are used to condition a diffusion model to generate high quality images. Evaluation on the Waymo Open Dataset demonstrate the capability of our framework to enable 3D asset insertion and NVS, facilitating the creation of diverse and realistic driving data.",
    "arxiv_url": "https://arxiv.org/abs/2512.22706v1",
    "pdf_url": "https://arxiv.org/pdf/2512.22706v1",
    "published_date": "2025-12-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "shadow",
      "high quality",
      "autonomous driving",
      "ar",
      "3d gaussian",
      "lighting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.22706v1",
      "pdf": "https://arxiv.org/pdf/2512.22706v1"
    },
    "bibtex": ""
  },
  {
    "title": "Tracking by Predicting 3-D Gaussians Over Time",
    "authors": [
      "Tanish Baranwal",
      "Himanshu Gaurav Singh",
      "Jathushan Rajasegaran",
      "Jitendra Malik"
    ],
    "abstract": "We propose Video Gaussian Masked Autoencoders (Video-GMAE), a self-supervised approach for representation learning that encodes a sequence of images into a set of Gaussian splats moving over time. Representing a video as a set of Gaussians enforces a reasonable inductive bias: that 2-D videos are often consistent projections of a dynamic 3-D scene. We find that tracking emerges when pretraining a network with this architecture. Mapping the trajectory of the learnt Gaussians onto the image plane gives zero-shot tracking performance comparable to state-of-the-art. With small-scale finetuning, our models achieve 34.6% improvement on Kinetics, and 13.1% on Kubric datasets, surpassing existing self-supervised video approaches. The project page and code are publicly available at https://videogmae.org/ and https://github.com/tekotan/video-gmae.",
    "arxiv_url": "https://arxiv.org/abs/2512.22489v2",
    "pdf_url": "https://arxiv.org/pdf/2512.22489v2",
    "published_date": "2025-12-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/tekotan/video-gmae",
    "keywords": [
      "tracking",
      "mapping",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.22489v2",
      "pdf": "https://arxiv.org/pdf/2512.22489v2",
      "project": "https://videogmae.org",
      "github": "https://github.com/tekotan/video-gmae"
    },
    "bibtex": ""
  },
  {
    "title": "Resolving compositional and conformational heterogeneity in cryo-EM with deformable 3D Gaussian representations",
    "authors": [
      "Bintao He",
      "Yiran Cheng",
      "Hongjia Li",
      "Xiang Gao",
      "Xin Gao",
      "Fa Zhang",
      "Renmin Han"
    ],
    "abstract": "Understanding protein flexibility and its dynamic interactions with other molecules is essential for studying protein function. Although cryogenic electron microscopy(cryo-EM) provides an opportunity to observe macromolecular dynamics directly, computational analysis of datasets mixing continuous and discrete structural states remains a formidable challenge. Here we introduce GaussianEM, a Gaussian-based pseudo-atomic framework that simultaneously resolves compositional and conformational heterogeneity from cryo-EM images. GaussianEM employs a dual-encoder-single-decoder architecture to decompose images into learnable Gaussian components, with variability encoded through modulated parameters. This explicit parameterization yields a continuous, intuitive representation of conformational dynamics that inherently preserves local structural integrity. By modeling displacements in Gaussian space, we capture atomic-scale conformational landscapes, bridging density maps and all-atom models. In comprehensive experiments, GaussianEM successfully reconstructs complex compositional and conformational variability,and resolves previously unobserved details in public datasets. Quantitative evaluations further confirm its ability to capture broader conformational diversity without sacrificing structural fidelity.",
    "arxiv_url": "https://arxiv.org/abs/2512.21599v2",
    "pdf_url": "https://arxiv.org/pdf/2512.21599v2",
    "published_date": "2025-12-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "dynamic",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.21599v2",
      "pdf": "https://arxiv.org/pdf/2512.21599v2"
    },
    "bibtex": ""
  },
  {
    "title": "TexAvatars : Hybrid Texel-3D Representations for Stable Rigging of Photorealistic Gaussian Head Avatars",
    "authors": [
      "Jaeseong Lee",
      "Junyeong Ahn",
      "Taewoong Kang",
      "Jaegul Choo"
    ],
    "abstract": "Constructing drivable and photorealistic 3D head avatars has become a central task in AR/XR, enabling immersive and expressive user experiences. With the emergence of high-fidelity and efficient representations such as 3D Gaussians, recent works have pushed toward ultra-detailed head avatars. Existing approaches typically fall into two categories: rule-based analytic rigging or neural network-based deformation fields. While effective in constrained settings, both approaches often fail to generalize to unseen expressions and poses, particularly in extreme reenactment scenarios. Other methods constrain Gaussians to the global texel space of 3DMMs to reduce rendering complexity. However, these texel-based avatars tend to underutilize the underlying mesh structure. They apply minimal analytic deformation and rely heavily on neural regressors and heuristic regularization in UV space, which weakens geometric consistency and limits extrapolation to complex, out-of-distribution deformations. To address these limitations, we introduce TexAvatars, a hybrid avatar representation that combines the explicit geometric grounding of analytic rigging with the spatial continuity of texel space. Our approach predicts local geometric attributes in UV space via CNNs, but drives 3D deformation through mesh-aware Jacobians, enabling smooth and semantically meaningful transitions across triangle boundaries. This hybrid design separates semantic modeling from geometric control, resulting in improved generalization, interpretability, and stability. Furthermore, TexAvatars captures fine-grained expression effects, including muscle-induced wrinkles, glabellar lines, and realistic mouth cavity geometry, with high fidelity. Our method achieves state-of-the-art performance under extreme pose and expression variations, demonstrating strong generalization in challenging head reenactment settings.",
    "arxiv_url": "https://arxiv.org/abs/2512.21099v1",
    "pdf_url": "https://arxiv.org/pdf/2512.21099v1",
    "published_date": "2025-12-24",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "deformation",
      "avatar",
      "geometry",
      "ar",
      "semantic",
      "3d gaussian",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.21099v1",
      "pdf": "https://arxiv.org/pdf/2512.21099v1"
    },
    "bibtex": ""
  },
  {
    "title": "AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences",
    "authors": [
      "Zhe Wang",
      "Jinghang Li",
      "Yifei Zhu"
    ],
    "abstract": "Free-viewpoint video (FVV) enables immersive viewing experiences by allowing users to view scenes from arbitrary perspectives. As a prominent reconstruction technique for FVV generation, 4D Gaussian Splatting (4DGS) models dynamic scenes with time-varying 3D Gaussian ellipsoids and achieves high-quality rendering via fast rasterization. However, existing 4DGS approaches suffer from quality degradation over long sequences and impose substantial bandwidth and storage overhead, limiting their applicability in real-time and wide-scale deployments. Therefore, we present AirGS, a streaming-optimized 4DGS framework that rearchitects the training and delivery pipeline to enable high-quality, low-latency FVV experiences. AirGS converts Gaussian video streams into multi-channel 2D formats and intelligently identifies keyframes to enhance frame reconstruction quality. It further combines temporal coherence with inflation loss to reduce training time and representation size. To support communication-efficient transmission, AirGS models 4DGS delivery as an integer linear programming problem and design a lightweight pruning level selection algorithm to adaptively prune the Gaussian updates to be transmitted, balancing reconstruction quality and bandwidth consumption. Extensive experiments demonstrate that AirGS reduces quality deviation in PSNR by more than 20% when scene changes, maintains frame-level PSNR consistently above 30, accelerates training by 6 times, reduces per-frame transmission size by nearly 50% compared to the SOTA 4DGS approaches.",
    "arxiv_url": "https://arxiv.org/abs/2512.20943v1",
    "pdf_url": "https://arxiv.org/pdf/2512.20943v1",
    "published_date": "2025-12-24",
    "categories": [
      "cs.GR",
      "cs.DC",
      "cs.LG",
      "cs.MM",
      "cs.NI",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "head",
      "dynamic",
      "lightweight",
      "ar",
      "3d gaussian",
      "efficient",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.20943v1",
      "pdf": "https://arxiv.org/pdf/2512.20943v1"
    },
    "bibtex": ""
  },
  {
    "title": "Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting",
    "authors": [
      "Yoonwoo Jeong",
      "Cheng Sun",
      "Frank Wang",
      "Minsu Cho",
      "Jaesung Choe"
    ],
    "abstract": "Recent advancements in computer vision have successfully extended Open-vocabulary segmentation (OVS) to the 3D domain by leveraging 3D Gaussian Splatting (3D-GS). Despite this progress, efficiently rendering the high-dimensional features required for open-vocabulary queries poses a significant challenge. Existing methods employ codebooks or feature compression, causing information loss, thereby degrading segmentation quality. To address this limitation, we introduce Quantile Rendering (Q-Render), a novel rendering strategy for 3D Gaussians that efficiently handles high-dimensional features while maintaining high fidelity. Unlike conventional volume rendering, which densely samples all 3D Gaussians intersecting each ray, Q-Render sparsely samples only those with dominant influence along the ray. By integrating Q-Render into a generalizable 3D neural network, we also propose Gaussian Splatting Network (GS-Net), which predicts Gaussian features in a generalizable manner. Extensive experiments on ScanNet and LeRF demonstrate that our framework outperforms state-of-the-art methods, while enabling real-time rendering with an approximate ~43.7x speedup on 512-D feature maps. Code will be made publicly available.",
    "arxiv_url": "https://arxiv.org/abs/2512.20927v1",
    "pdf_url": "https://arxiv.org/pdf/2512.20927v1",
    "published_date": "2025-12-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "real-time rendering",
      "segmentation",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.20927v1",
      "pdf": "https://arxiv.org/pdf/2512.20927v1"
    },
    "bibtex": ""
  },
  {
    "title": "Nebula: Enable City-Scale 3D Gaussian Splatting in Virtual Reality via Collaborative Rendering and Accelerated Stereo Rasterization",
    "authors": [
      "He Zhu",
      "Zheng Liu",
      "Xingyang Li",
      "Anbang Wu",
      "Jieru Zhao",
      "Fangxin Liu",
      "Yiming Gan",
      "Jingwen Leng",
      "Yu Feng"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has drawn significant attention in the architectural community recently. However, current architectural designs often overlook the 3DGS scalability, making them fragile for extremely large-scale 3DGS. Meanwhile, the VR bandwidth requirement makes it impossible to deliver high-fidelity and smooth VR content from the cloud.   We present Nebula, a coherent acceleration framework for large-scale 3DGS collaborative rendering. Instead of streaming videos, Nebula streams intermediate results after the LoD search, reducing 1925% data communication between the cloud and the client. To further enhance the motion-to-photon experience, we introduce a temporal-aware LoD search in the cloud that tames the irregular memory access and reduces redundant data access by exploiting temporal coherence across frames. On the client side, we propose a novel stereo rasterization that enables two eyes to share most computations during the stereo rendering with bit-accurate quality. With minimal hardware augmentations, Nebula achieves 2.7$\\times$ motion-to-photon speedup and reduces 1925% bandwidth over lossy video streaming.",
    "arxiv_url": "https://arxiv.org/abs/2512.20495v1",
    "pdf_url": "https://arxiv.org/pdf/2512.20495v1",
    "published_date": "2025-12-23",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "high-fidelity",
      "acceleration",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.20495v1",
      "pdf": "https://arxiv.org/pdf/2512.20495v1"
    },
    "bibtex": ""
  },
  {
    "title": "SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images",
    "authors": [
      "Linfei Li",
      "Lin Zhang",
      "Zhong Wang",
      "Ying Shen"
    ],
    "abstract": "Recent advances in generative AI have accelerated the production of ultra-high-resolution visual content, posing significant challenges for efficient compression and real-time decoding on end-user devices. Inspired by 3D Gaussian Splatting, recent 2D Gaussian image models improve representation efficiency, yet existing methods struggle to balance compression ratio and reconstruction fidelity in ultra-high-resolution scenarios. To address this issue, we propose SmartSplat, a highly adaptive and feature-aware GS-based image compression framework that supports arbitrary image resolutions and compression ratios. SmartSplat leverages image-aware features such as gradients and color variances, introducing a Gradient-Color Guided Variational Sampling strategy together with an Exclusion-based Uniform Sampling scheme to improve the non-overlapping coverage of Gaussian primitives in pixel space. In addition, we propose a Scale-Adaptive Gaussian Color Sampling method to enhance color initialization across scales. Through joint optimization of spatial layout, scale, and color initialization, SmartSplat efficiently captures both local structures and global textures using a limited number of Gaussians, achieving high reconstruction quality under strong compression. Extensive experiments on DIV8K and a newly constructed 16K dataset demonstrate that SmartSplat consistently outperforms state-of-the-art methods at comparable compression ratios and exceeds their compression limits, showing strong scalability and practical applicability. The code is publicly available at https://github.com/lif314/SmartSplat.",
    "arxiv_url": "https://arxiv.org/abs/2512.20377v1",
    "pdf_url": "https://arxiv.org/pdf/2512.20377v1",
    "published_date": "2025-12-23",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "https://github.com/lif314/SmartSplat",
    "keywords": [
      "compression",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.20377v1",
      "pdf": "https://arxiv.org/pdf/2512.20377v1",
      "github": "https://github.com/lif314/SmartSplat"
    },
    "bibtex": ""
  },
  {
    "title": "Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)",
    "authors": [
      "Robert van de Ven",
      "Trim Bresilla",
      "Bram Nelissen",
      "Ard Nieuwenhuizen",
      "Eldert J. van Henten",
      "Gert Kootstra"
    ],
    "abstract": "Automating tasks in orchards is challenging because of the large amount of variation in the environment and occlusions. One of the challenges is apple pose estimation, where key points, such as the calyx, are often occluded. Recently developed pose estimation methods no longer rely on these key points, but still require them for annotations, making annotating challenging and time-consuming. Due to the abovementioned occlusions, there can be conflicting and missing annotations of the same fruit between different images. Novel 3D reconstruction methods can be used to simplify annotating and enlarge datasets. We propose a novel pipeline consisting of 3D Gaussian Splatting to reconstruct an orchard scene, simplified annotations, automated projection of the annotations to images, and the training and evaluation of a pose estimation method. Using our pipeline, 105 manual annotations were required to obtain 28,191 training labels, a reduction of 99.6%. Experimental results indicated that training with labels of fruits that are $\\leq95\\%$ occluded resulted in the best performance, with a neutral F1 score of 0.927 on the original images and 0.970 on the rendered images. Adjusting the size of the training dataset had small effects on the model performance in terms of F1 score and pose estimation accuracy. It was found that the least occluded fruits had the best position estimation, which worsened as the fruits became more occluded. It was also found that the tested pose estimation method was unable to correctly learn the orientation estimation of apples.",
    "arxiv_url": "https://arxiv.org/abs/2512.20148v1",
    "pdf_url": "https://arxiv.org/pdf/2512.20148v1",
    "published_date": "2025-12-23",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.20148v1",
      "pdf": "https://arxiv.org/pdf/2512.20148v1"
    },
    "bibtex": ""
  },
  {
    "title": "Dreamcrafter: Immersive Editing of 3D Radiance Fields Through Flexible, Generative Inputs and Outputs",
    "authors": [
      "Cyrus Vachha",
      "Yixiao Kang",
      "Zach Dive",
      "Ashwat Chidambaram",
      "Anik Gupta",
      "Eunice Jun",
      "Bjoern Hartmann"
    ],
    "abstract": "Authoring 3D scenes is a central task for spatial computing applications. Competing visions for lowering existing barriers are (1) focus on immersive, direct manipulation of 3D content or (2) leverage AI techniques that capture real scenes (3D Radiance Fields such as, NeRFs, 3D Gaussian Splatting) and modify them at a higher level of abstraction, at the cost of high latency. We unify the complementary strengths of these approaches and investigate how to integrate generative AI advances into real-time, immersive 3D Radiance Field editing. We introduce Dreamcrafter, a VR-based 3D scene editing system that: (1) provides a modular architecture to integrate generative AI algorithms; (2) combines different levels of control for creating objects, including natural language and direct manipulation; and (3) introduces proxy representations that support interaction during high-latency operations. We contribute empirical findings on control preferences and discuss how generative AI interfaces beyond text input enhance creativity in scene editing and world building.",
    "arxiv_url": "https://arxiv.org/abs/2512.20129v2",
    "pdf_url": "https://arxiv.org/pdf/2512.20129v2",
    "published_date": "2025-12-23",
    "categories": [
      "cs.HC",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "face",
      "ar",
      "nerf",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.20129v2",
      "pdf": "https://arxiv.org/pdf/2512.20129v2"
    },
    "bibtex": ""
  },
  {
    "title": "HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction",
    "authors": [
      "Jong Wook Kim",
      "Wonseok Roh",
      "Ha Dam Baek",
      "Pilhyeon Lee",
      "Jonghyun Choi",
      "Sangpil Kim"
    ],
    "abstract": "3D Panoptic Occupancy Prediction aims to reconstruct a dense volumetric scene map by predicting the semantic class and instance identity of every occupied region in 3D space. Achieving such fine-grained 3D understanding requires precise geometric reasoning and spatially consistent scene representation across complex environments. However, existing approaches often struggle to maintain precise geometry and capture the precise spatial range of 3D instances critical for robust panoptic separation. To overcome these limitations, we introduce HyGE-Occ, a novel framework that leverages a hybrid view-transformation branch with 3D Gaussian and edge priors to enhance both geometric consistency and boundary awareness in 3D panoptic occupancy prediction. HyGE-Occ employs a hybrid view-transformation branch that fuses a continuous Gaussian-based depth representation with a discretized depth-bin formulation, producing BEV features with improved geometric consistency and structural coherence. In parallel, we extract edge maps from BEV features and use them as auxiliary information to learn edge cues. In our extensive experiments on the Occ3D-nuScenes dataset, HyGE-Occ outperforms existing work, demonstrating superior 3D geometric reasoning.",
    "arxiv_url": "https://arxiv.org/abs/2512.19871v1",
    "pdf_url": "https://arxiv.org/pdf/2512.19871v1",
    "published_date": "2025-12-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "semantic",
      "3d gaussian",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.19871v1",
      "pdf": "https://arxiv.org/pdf/2512.19871v1"
    },
    "bibtex": ""
  },
  {
    "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
    "authors": [
      "Hanyang Kong",
      "Xingyi Yang",
      "Xiaoxu Zheng",
      "Xinchao Wang"
    ],
    "abstract": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \\href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.",
    "arxiv_url": "https://arxiv.org/abs/2512.19678v1",
    "pdf_url": "https://arxiv.org/pdf/2512.19678v1",
    "published_date": "2025-12-22",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.19678v1",
      "pdf": "https://arxiv.org/pdf/2512.19678v1",
      "project": "https://hyokong.github.io/worldwarp-page"
    },
    "bibtex": ""
  },
  {
    "title": "4D Gaussian Splatting as a Learned Dynamical System",
    "authors": [
      "Arnold Caleb Asiimwe",
      "Carl Vondrick"
    ],
    "abstract": "We reinterpret 4D Gaussian Splatting as a continuous-time dynamical system, where scene motion arises from integrating a learned neural dynamical field rather than applying per-frame deformations. This formulation, which we call EvoGS, treats the Gaussian representation as an evolving physical system whose state evolves continuously under a learned motion law. This unlocks capabilities absent in deformation-based approaches:(1) sample-efficient learning from sparse temporal supervision by modeling the underlying motion law; (2) temporal extrapolation enabling forward and backward prediction beyond observed time ranges; and (3) compositional dynamics that allow localized dynamics injection for controllable scene synthesis. Experiments on dynamic scene benchmarks show that EvoGS achieves better motion coherence and temporal consistency compared to deformation-field baselines while maintaining real-time rendering",
    "arxiv_url": "https://arxiv.org/abs/2512.19648v1",
    "pdf_url": "https://arxiv.org/pdf/2512.19648v1",
    "published_date": "2025-12-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "deformation",
      "real-time rendering",
      "gaussian splatting",
      "ar",
      "efficient",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.19648v1",
      "pdf": "https://arxiv.org/pdf/2512.19648v1"
    },
    "bibtex": ""
  },
  {
    "title": "TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation",
    "authors": [
      "Hongwei Fan",
      "Hang Dai",
      "Jiyao Zhang",
      "Jinzhou Li",
      "Qiyang Yan",
      "Yujie Zhao",
      "Mingju Gao",
      "Jinghang Wu",
      "Hao Tang",
      "Hao Dong"
    ],
    "abstract": "The robotics field is evolving towards data-driven, end-to-end learning, inspired by multimodal large models. However, reliance on expensive real-world data limits progress. Simulators offer cost-effective alternatives, but the gap between simulation and reality challenges effective policy transfer. This paper introduces TwinAligner, a novel Real2Sim2Real system that addresses both visual and dynamic gaps. The visual alignment module achieves pixel-level alignment through SDF reconstruction and editable 3DGS rendering, while the dynamic alignment module ensures dynamic consistency by identifying rigid physics from robot-object interaction. TwinAligner improves robot learning by providing scalable data collection and establishing a trustworthy iterative cycle, accelerating algorithm development. Quantitative evaluations highlight TwinAligner's strong capabilities in visual and dynamic real-to-sim alignment. This system enables policies trained in simulation to achieve strong zero-shot generalization to the real world. The high consistency between real-world and simulated policy performance underscores TwinAligner's potential to advance scalable robot learning. Code and data will be released on https://twin-aligner.github.io",
    "arxiv_url": "https://arxiv.org/abs/2512.19390v1",
    "pdf_url": "https://arxiv.org/pdf/2512.19390v1",
    "published_date": "2025-12-22",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.19390v1",
      "pdf": "https://arxiv.org/pdf/2512.19390v1",
      "project": "https://twin-aligner.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "GaussianImage++: Boosted Image Representation and Compression with 2D Gaussian Splatting",
    "authors": [
      "Tiantian Li",
      "Xinjie Zhang",
      "Xingtong Ge",
      "Tongda Xu",
      "Dailan He",
      "Jun Zhang",
      "Yan Wang"
    ],
    "abstract": "Implicit neural representations (INRs) have achieved remarkable success in image representation and compression, but they require substantial training time and memory. Meanwhile, recent 2D Gaussian Splatting (GS) methods (\\textit{e.g.}, GaussianImage) offer promising alternatives through efficient primitive-based rendering. However, these methods require excessive Gaussian primitives to maintain high visual fidelity. To exploit the potential of GS-based approaches, we present GaussianImage++, which utilizes limited Gaussian primitives to achieve impressive representation and compression performance. Firstly, we introduce a distortion-driven densification mechanism. It progressively allocates Gaussian primitives according to signal intensity. Secondly, we employ context-aware Gaussian filters for each primitive, which assist in the densification to optimize Gaussian primitives based on varying image content. Thirdly, we integrate attribute-separated learnable scalar quantizers and quantization-aware training, enabling efficient compression of primitive attributes. Experimental results demonstrate the effectiveness of our method. In particular, GaussianImage++ outperforms GaussianImage and INRs-based COIN in representation and compression performance while maintaining real-time decoding and low memory usage.",
    "arxiv_url": "https://arxiv.org/abs/2512.19108v2",
    "pdf_url": "https://arxiv.org/pdf/2512.19108v2",
    "published_date": "2025-12-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.19108v2",
      "pdf": "https://arxiv.org/pdf/2512.19108v2"
    },
    "bibtex": ""
  },
  {
    "title": "EcoSplat: Efficiency-controllable Feed-forward 3D Gaussian Splatting from Multi-view Images",
    "authors": [
      "Jongmin Park",
      "Minh-Quan Viet Bui",
      "Juan Luis Gonzalez Bello",
      "Jaeho Moon",
      "Jihyong Oh",
      "Munchurl Kim"
    ],
    "abstract": "Feed-forward 3D Gaussian Splatting (3DGS) enables efficient one-pass scene reconstruction, providing 3D representations for novel view synthesis without per-scene optimization. However, existing methods typically predict pixel-aligned primitives per-view, producing an excessive number of primitives in dense-view settings and offering no explicit control over the number of predicted Gaussians. To address this, we propose EcoSplat, the first efficiency-controllable feed-forward 3DGS framework that adaptively predicts the 3D representation for any given target primitive count at inference time. EcoSplat adopts a two-stage optimization process. The first stage is Pixel-aligned Gaussian Training (PGT) where our model learns initial primitive prediction. The second stage is Importance-aware Gaussian Finetuning (IGF) stage where our model learns rank primitives and adaptively adjust their parameters based on the target primitive count. Extensive experiments across multiple dense-view settings show that EcoSplat is robust and outperforms state-of-the-art methods under strict primitive-count constraints, making it well-suited for flexible downstream rendering tasks.",
    "arxiv_url": "https://arxiv.org/abs/2512.18692v1",
    "pdf_url": "https://arxiv.org/pdf/2512.18692v1",
    "published_date": "2025-12-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.18692v1",
      "pdf": "https://arxiv.org/pdf/2512.18692v1"
    },
    "bibtex": ""
  },
  {
    "title": "SplatBright: Generalizable Low-Light Scene Reconstruction from Sparse Views via Physically-Guided Gaussian Enhancement",
    "authors": [
      "Yue Wen",
      "Liang Song",
      "Hesheng Wang"
    ],
    "abstract": "Low-light 3D reconstruction from sparse views remains challenging due to exposure imbalance and degraded color fidelity. While existing methods struggle with view inconsistency and require per-scene training, we propose SplatBright, which is, to our knowledge, the first generalizable 3D Gaussian framework for joint low-light enhancement and reconstruction from sparse sRGB inputs. Our key idea is to integrate physically guided illumination modeling with geometry-appearance decoupling for consistent low-light reconstruction. Specifically, we adopt a dual-branch predictor that provides stable geometric initialization of 3D Gaussian parameters. On the appearance side, illumination consistency leverages frequency priors to enable controllable and cross-view coherent lighting, while an appearance refinement module further separates illumination, material, and view-dependent cues to recover fine texture. To tackle the lack of large-scale geometrically consistent paired data, we synthesize dark views via a physics-based camera model for training. Extensive experiments on public and self-collected datasets demonstrate that SplatBright achieves superior novel view synthesis, cross-view consistency, and better generalization to unseen low-light scenes compared with both 2D and 3D methods.",
    "arxiv_url": "https://arxiv.org/abs/2512.18655v1",
    "pdf_url": "https://arxiv.org/pdf/2512.18655v1",
    "published_date": "2025-12-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "sparse view",
      "3d reconstruction",
      "geometry",
      "ar",
      "3d gaussian",
      "lighting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.18655v1",
      "pdf": "https://arxiv.org/pdf/2512.18655v1"
    },
    "bibtex": ""
  },
  {
    "title": "Geometric-Photometric Event-based 3D Gaussian Ray Tracing",
    "authors": [
      "Kai Kohyama",
      "Yoshimitsu Aoki",
      "Guillermo Gallego",
      "Shintaro Shiba"
    ],
    "abstract": "Event cameras offer a high temporal resolution over traditional frame-based cameras, which makes them suitable for motion and structure estimation. However, it has been unclear how event-based 3D Gaussian Splatting (3DGS) approaches could leverage fine-grained temporal information of sparse events. This work proposes a framework to address the trade-off between accuracy and temporal resolution in event-based 3DGS. Our key idea is to decouple the rendering into two branches: event-by-event geometry (depth) rendering and snapshot-based radiance (intensity) rendering, by using ray-tracing and the image of warped events. The extensive evaluation shows that our method achieves state-of-the-art performance on the real-world datasets and competitive performance on the synthetic dataset. Also, the proposed method works without prior information (e.g., pretrained image reconstruction models) or COLMAP-based initialization, is more flexible in the event selection number, and achieves sharp reconstruction on scene edges with fast training time. We hope that this work deepens our understanding of the sparse nature of events for 3D reconstruction. The code will be released.",
    "arxiv_url": "https://arxiv.org/abs/2512.18640v1",
    "pdf_url": "https://arxiv.org/pdf/2512.18640v1",
    "published_date": "2025-12-21",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "ray tracing",
      "understanding",
      "motion",
      "3d reconstruction",
      "geometry",
      "ar",
      "3d gaussian",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.18640v1",
      "pdf": "https://arxiv.org/pdf/2512.18640v1"
    },
    "bibtex": ""
  },
  {
    "title": "ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning",
    "authors": [
      "Zhenhao Zhou",
      "Dan Negrut"
    ],
    "abstract": "We present ChronoDreamer, an action-conditioned world model for contact-rich robotic manipulation. Given a history of egocentric RGB frames, contact maps, actions, and joint states, ChronoDreamer predicts future video frames, contact distributions, and joint angles via a spatial-temporal transformer trained with MaskGIT-style masked prediction. Contact is encoded as depth-weighted Gaussian splat images that render 3D forces into a camera-aligned format suitable for vision backbones. At inference, predicted rollouts are evaluated by a vision-language model that reasons about collision likelihood, enabling rejection sampling of unsafe actions before execution. We train and evaluate on DreamerBench, a simulation dataset generated with Project Chrono that provides synchronized RGB, contact splat, proprioception, and physics annotations across rigid and deformable object scenarios. Qualitative results demonstrate that the model preserves spatial coherence during non-contact motion and generates plausible contact predictions, while the LLM-based judge distinguishes collision from non-collision trajectories.",
    "arxiv_url": "https://arxiv.org/abs/2512.18619v1",
    "pdf_url": "https://arxiv.org/pdf/2512.18619v1",
    "published_date": "2025-12-21",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.18619v1",
      "pdf": "https://arxiv.org/pdf/2512.18619v1"
    },
    "bibtex": ""
  },
  {
    "title": "MatSpray: Fusing 2D Material World Knowledge on 3D Geometry",
    "authors": [
      "Philipp Langsteiner",
      "Jan-Niklas Dihlmann",
      "Hendrik P. A. Lensch"
    ],
    "abstract": "Manual modeling of material parameters and 3D geometry is a time consuming yet essential task in the gaming and film industries. While recent advances in 3D reconstruction have enabled accurate approximations of scene geometry and appearance, these methods often fall short in relighting scenarios due to the lack of precise, spatially varying material parameters. At the same time, diffusion models operating on 2D images have shown strong performance in predicting physically based rendering (PBR) properties such as albedo, roughness, and metallicity. However, transferring these 2D material maps onto reconstructed 3D geometry remains a significant challenge. We propose a framework for fusing 2D material data into 3D geometry using a combination of novel learning-based and projection-based approaches. We begin by reconstructing scene geometry via Gaussian Splatting. From the input images, a diffusion model generates 2D maps for albedo, roughness, and metallic parameters. Any existing diffusion model that can convert images or videos to PBR materials can be applied. The predictions are further integrated into the 3D representation either by optimizing an image-based loss or by directly projecting the material parameters onto the Gaussians using Gaussian ray tracing. To enhance fine-scale accuracy and multi-view consistency, we further introduce a light-weight neural refinement step (Neural Merger), which takes ray-traced material features as input and produces detailed adjustments. Our results demonstrate that the proposed methods outperform existing techniques in both quantitative metrics and perceived visual realism. This enables more accurate, relightable, and photorealistic renderings from reconstructed scenes, significantly improving the realism and efficiency of asset creation workflows in content production pipelines.",
    "arxiv_url": "https://arxiv.org/abs/2512.18314v1",
    "pdf_url": "https://arxiv.org/pdf/2512.18314v1",
    "published_date": "2025-12-20",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "ray tracing",
      "3d reconstruction",
      "geometry",
      "ar",
      "relightable",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.18314v1",
      "pdf": "https://arxiv.org/pdf/2512.18314v1"
    },
    "bibtex": ""
  },
  {
    "title": "Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding",
    "authors": [
      "Yue Li",
      "Qi Ma",
      "Runyi Yang",
      "Mengjiao Ma",
      "Bin Ren",
      "Nikola Popovic",
      "Nicu Sebe",
      "Theo Gevers",
      "Luc Van Gool",
      "Danda Pani Paudel",
      "Martin R. Oswald"
    ],
    "abstract": "While 3DGS has emerged as a high-fidelity scene representation, encoding rich, general-purpose features directly from its primitives remains under-explored. We address this gap by introducing Chorus, a multi-teacher pretraining framework that learns a holistic feed-forward 3D Gaussian Splatting (3DGS) scene encoder by distilling complementary signals from 2D foundation models. Chorus employs a shared 3D encoder and teacher-specific projectors to learn from language-aligned, generalist, and object-aware teachers, encouraging a shared embedding space that captures signals from high-level semantics to fine-grained structure.   We evaluate Chorus on a wide range of tasks: open-vocabulary semantic and instance segmentation, linear and decoder probing, as well as data-efficient supervision. Besides 3DGS, we also test Chorus on several benchmarks that only support point clouds by pretraining a variant using only Gaussians' centers, colors, estimated normals as inputs. Interestingly, this encoder shows strong transfer and outperforms the point clouds baseline while using 39.9 times fewer training scenes. Finally, we propose a render-and-distill adaptation that facilitates out-of-domain finetuning. Our code and model will be released upon publication.",
    "arxiv_url": "https://arxiv.org/abs/2512.17817v2",
    "pdf_url": "https://arxiv.org/pdf/2512.17817v2",
    "published_date": "2025-12-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "segmentation",
      "ar",
      "semantic",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.17817v2",
      "pdf": "https://arxiv.org/pdf/2512.17817v2"
    },
    "bibtex": ""
  },
  {
    "title": "G3Splat: Geometrically Consistent Generalizable Gaussian Splatting",
    "authors": [
      "Mehdi Hosseinzadeh",
      "Shin-Fang Chng",
      "Yi Xu",
      "Simon Lucey",
      "Ian Reid",
      "Ravi Garg"
    ],
    "abstract": "3D Gaussians have recently emerged as an effective scene representation for real-time splatting and accurate novel-view synthesis, motivating several works to adapt multi-view structure prediction networks to regress per-pixel 3D Gaussians from images. However, most prior work extends these networks to predict additional Gaussian parameters -- orientation, scale, opacity, and appearance -- while relying almost exclusively on view-synthesis supervision. We show that a view-synthesis loss alone is insufficient to recover geometrically meaningful splats in this setting. We analyze and address the ambiguities of learning 3D Gaussian splats under self-supervision for pose-free generalizable splatting, and introduce G3Splat, which enforces geometric priors to obtain geometrically consistent 3D scene representations. Trained on RE10K, our approach achieves state-of-the-art performance in (i) geometrically consistent reconstruction, (ii) relative pose estimation, and (iii) novel-view synthesis. We further demonstrate strong zero-shot generalization on ScanNet, substantially outperforming prior work in both geometry recovery and relative pose estimation. Code and pretrained models are released on our project page (https://m80hz.github.io/g3splat/).",
    "arxiv_url": "https://arxiv.org/abs/2512.17547v1",
    "pdf_url": "https://arxiv.org/pdf/2512.17547v1",
    "published_date": "2025-12-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "geometry",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.17547v1",
      "pdf": "https://arxiv.org/pdf/2512.17547v1",
      "project": "https://m80hz.github.io/g3splat"
    },
    "bibtex": ""
  },
  {
    "title": "FLEG: Feed-Forward Language Embedded Gaussian Splatting from Any Views",
    "authors": [
      "Qijian Tian",
      "Xin Tan",
      "Jiayu Ying",
      "Xuhong Wang",
      "Yuan Xie",
      "Lizhuang Ma"
    ],
    "abstract": "We present FLEG, a feed-forward network that reconstructs language-embedded 3D Gaussians from any views. Previous straightforward solutions combine feed-forward reconstruction with Gaussian heads but suffer from fixed input views and insufficient 3D training data. In contrast, we propose a 3D-annotation-free training framework for 2D-to-3D lifting from arbitrary uncalibrated and unposed multi-view images. Since the framework does not require 3D annotations, we can leverage large-scale video data with easily obtained 2D instance information to enrich semantic embedding. We also propose an instance-guided contrastive learning to align 2D semantics with the 3D representations. In addition, to mitigate the high memory and computational cost of dense views, we further propose a geometry-semantic hierarchical sparsification strategy. Our FLEG efficiently reconstructs language-embedded 3D Gaussian representation in a feed-forward manner from arbitrary sparse or dense views, jointly producing accurate geometry, high-fidelity appearance, and language-aligned semantics. Extensive experiments show that it outperforms existing methods on various related tasks. Project page: https://fangzhou2000.github.io/projects/fleg.",
    "arxiv_url": "https://arxiv.org/abs/2512.17541v1",
    "pdf_url": "https://arxiv.org/pdf/2512.17541v1",
    "published_date": "2025-12-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "geometry",
      "ar",
      "semantic",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.17541v1",
      "pdf": "https://arxiv.org/pdf/2512.17541v1",
      "project": "https://fangzhou2000.github.io/projects/fleg"
    },
    "bibtex": ""
  },
  {
    "title": "Voxel-GS: Quantized Scaffold Gaussian Splatting Compression with Run-Length Coding",
    "authors": [
      "Chunyang Fu",
      "Xiangrui Liu",
      "Shiqi Wang",
      "Zhu Li"
    ],
    "abstract": "Substantial Gaussian splatting format point clouds require effective compression. In this paper, we propose Voxel-GS, a simple yet highly effective framework that departs from the complex neural entropy models of prior work, instead achieving competitive performance using only a lightweight rate proxy and run-length coding. Specifically, we employ a differentiable quantization to discretize the Gaussian attributes of Scaffold-GS. Subsequently, a Laplacian-based rate proxy is devised to impose an entropy constraint, guiding the generation of high-fidelity and compact reconstructions. Finally, this integer-type Gaussian point cloud is compressed losslessly using Octree and run-length coding. Experiments validate that the proposed rate proxy accurately estimates the bitrate of run-length coding, enabling Voxel-GS to eliminate redundancy and optimize for a more compact representation. Consequently, our method achieves a remarkable compression ratio with significantly faster coding speeds than prior art. The code is available at https://github.com/zb12138/VoxelGS.",
    "arxiv_url": "https://arxiv.org/abs/2512.17528v1",
    "pdf_url": "https://arxiv.org/pdf/2512.17528v1",
    "published_date": "2025-12-19",
    "categories": [
      "cs.MM"
    ],
    "github_url": "https://github.com/zb12138/VoxelGS",
    "keywords": [
      "compact",
      "high-fidelity",
      "lightweight",
      "compression",
      "ar",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.17528v1",
      "pdf": "https://arxiv.org/pdf/2512.17528v1",
      "github": "https://github.com/zb12138/VoxelGS"
    },
    "bibtex": ""
  },
  {
    "title": "Flying in Clutter on Monocular RGB by Learning in 3D Radiance Fields with Domain Adaptation",
    "authors": [
      "Xijie Huang",
      "Jinhan Li",
      "Tianyue Wu",
      "Xin Zhou",
      "Zhichao Han",
      "Fei Gao"
    ],
    "abstract": "Modern autonomous navigation systems predominantly rely on lidar and depth cameras. However, a fundamental question remains: Can flying robots navigate in clutter using solely monocular RGB images? Given the prohibitive costs of real-world data collection, learning policies in simulation offers a promising path. Yet, deploying such policies directly in the physical world is hindered by the significant sim-to-real perception gap. Thus, we propose a framework that couples the photorealism of 3D Gaussian Splatting (3DGS) environments with Adversarial Domain Adaptation. By training in high-fidelity simulation while explicitly minimizing feature discrepancy, our method ensures the policy relies on domain-invariant cues. Experimental results demonstrate that our policy achieves robust zero-shot transfer to the physical world, enabling safe and agile flight in unstructured environments with varying illumination.",
    "arxiv_url": "https://arxiv.org/abs/2512.17349v1",
    "pdf_url": "https://arxiv.org/pdf/2512.17349v1",
    "published_date": "2025-12-19",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "high-fidelity",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.17349v1",
      "pdf": "https://arxiv.org/pdf/2512.17349v1"
    },
    "bibtex": ""
  },
  {
    "title": "DGH: Dynamic Gaussian Hair",
    "authors": [
      "Junying Wang",
      "Yuanlu Xu",
      "Edith Tretschk",
      "Ziyan Wang",
      "Anastasia Ianina",
      "Aljaz Bozic",
      "Ulrich Neumann",
      "Tony Tung"
    ],
    "abstract": "The creation of photorealistic dynamic hair remains a major challenge in digital human modeling because of the complex motions, occlusions, and light scattering. Existing methods often resort to static capture and physics-based models that do not scale as they require manual parameter fine-tuning to handle the diversity of hairstyles and motions, and heavy computation to obtain high-quality appearance. In this paper, we present Dynamic Gaussian Hair (DGH), a novel framework that efficiently learns hair dynamics and appearance. We propose: (1) a coarse-to-fine model that learns temporally coherent hair motion dynamics across diverse hairstyles; (2) a strand-guided optimization module that learns a dynamic 3D Gaussian representation for hair appearance with support for differentiable rendering, enabling gradient-based learning of view-consistent appearance under motion. Unlike prior simulation-based pipelines, our approach is fully data-driven, scales with training data, and generalizes across various hairstyles and head motion sequences. Additionally, DGH can be seamlessly integrated into a 3D Gaussian avatar framework, enabling realistic, animatable hair for high-fidelity avatar representation. DGH achieves promising geometry and appearance results, providing a scalable, data-driven alternative to physics-based simulation and rendering.",
    "arxiv_url": "https://arxiv.org/abs/2512.17094v1",
    "pdf_url": "https://arxiv.org/pdf/2512.17094v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "dynamic",
      "high-fidelity",
      "avatar",
      "geometry",
      "ar",
      "human",
      "3d gaussian",
      "efficient",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.17094v1",
      "pdf": "https://arxiv.org/pdf/2512.17094v1"
    },
    "bibtex": ""
  },
  {
    "title": "Animate Any Character in Any World",
    "authors": [
      "Yitong Wang",
      "Fangyun Wei",
      "Hongyang Zhang",
      "Bo Dai",
      "Yan Lu"
    ],
    "abstract": "Recent advances in world models have greatly enhanced interactive environment simulation. Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow a single entity to perform limited actions in an otherwise uncontrollable environment. In this work, we introduce AniX, leveraging the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing open-ended actions. Users can provide a 3DGS scene and a character, then direct the character through natural language to perform diverse behaviors from basic locomotion to object-centric interactions while freely exploring the environment. AniX synthesizes temporally coherent video clips that preserve visual fidelity with the provided scene and character, formulated as a conditional autoregressive video generation problem. Built upon a pre-trained video generator, our training strategy significantly enhances motion dynamics while maintaining generalization across actions and characters. Our evaluation covers a broad range of aspects, including visual quality, character consistency, action controllability, and long-horizon coherence.",
    "arxiv_url": "https://arxiv.org/abs/2512.17796v1",
    "pdf_url": "https://arxiv.org/pdf/2512.17796v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.17796v1",
      "pdf": "https://arxiv.org/pdf/2512.17796v1"
    },
    "bibtex": ""
  },
  {
    "title": "Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation",
    "authors": [
      "Kaiwen Jiang",
      "Xueting Li",
      "Seonwook Park",
      "Ravi Ramamoorthi",
      "Shalini De Mello",
      "Koki Nagano"
    ],
    "abstract": "Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods -- built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting -- ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face's 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d",
    "arxiv_url": "https://arxiv.org/abs/2512.16893v1",
    "pdf_url": "https://arxiv.org/pdf/2512.16893v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "head",
      "lightweight",
      "face",
      "animation",
      "avatar",
      "gaussian splatting",
      "ar",
      "efficient",
      "fast",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.16893v1",
      "pdf": "https://arxiv.org/pdf/2512.16893v1",
      "project": "https://research.nvidia.com/labs/amri/projects/instant4d"
    },
    "bibtex": ""
  },
  {
    "title": "GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation",
    "authors": [
      "Jingjing Qian",
      "Boyao Han",
      "Chen Shi",
      "Lei Xiao",
      "Long Yang",
      "Shaoshuai Shi",
      "Li Jiang"
    ],
    "abstract": "Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2512.16811v1",
    "pdf_url": "https://arxiv.org/pdf/2512.16811v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "geometry",
      "ar",
      "human",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.16811v1",
      "pdf": "https://arxiv.org/pdf/2512.16811v1"
    },
    "bibtex": ""
  },
  {
    "title": "SDFoam: Signed-Distance Foam for explicit surface reconstruction",
    "authors": [
      "Antonella Rech",
      "Nicola Conci",
      "Nicola Garau"
    ],
    "abstract": "Neural radiance fields (NeRF) have driven impressive progress in view synthesis by using ray-traced volumetric rendering. Splatting-based methods such as 3D Gaussian Splatting (3DGS) provide faster rendering by rasterizing 3D primitives. RadiantFoam (RF) brought ray tracing back, achieving throughput comparable to Gaussian Splatting by organizing radiance with an explicit Voronoi Diagram (VD). Yet, all the mentioned methods still struggle with precise mesh reconstruction. We address this gap by jointly learning an explicit VD with an implicit Signed Distance Field (SDF). The scene is optimized via ray tracing and regularized by an Eikonal objective. The SDF introduces metric-consistent isosurfaces, which, in turn, bias near-surface Voronoi cell faces to align with the zero level set. The resulting model produces crisper, view-consistent surfaces with fewer floaters and improved topology, while preserving photometric quality and maintaining training speed on par with RadiantFoam. Across diverse scenes, our hybrid implicit-explicit formulation, which we name SDFoam, substantially improves mesh reconstruction accuracy (Chamfer distance) with comparable appearance (PSNR, SSIM), without sacrificing efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2512.16706v1",
    "pdf_url": "https://arxiv.org/pdf/2512.16706v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ray tracing",
      "face",
      "ar",
      "nerf",
      "3d gaussian",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.16706v1",
      "pdf": "https://arxiv.org/pdf/2512.16706v1"
    },
    "bibtex": ""
  },
  {
    "title": "Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture",
    "authors": [
      "Haodi He",
      "Jihun Yu",
      "Ronald Fedkiw"
    ],
    "abstract": "We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.",
    "arxiv_url": "https://arxiv.org/abs/2512.16397v1",
    "pdf_url": "https://arxiv.org/pdf/2512.16397v1",
    "published_date": "2025-12-18",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "geometry",
      "segmentation",
      "ar",
      "semantic",
      "nerf",
      "relightable",
      "human",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.16397v1",
      "pdf": "https://arxiv.org/pdf/2512.16397v1"
    },
    "bibtex": ""
  },
  {
    "title": "Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering",
    "authors": [
      "Divam Gupta",
      "Anuj Pahuja",
      "Nemanja Bartolovic",
      "Tomas Simon",
      "Forrest Iandola",
      "Giljoo Nam"
    ],
    "abstract": "We present Gaussian Pixel Codec Avatars (GPiCA), photorealistic head avatars that can be generated from multi-view images and efficiently rendered on mobile devices. GPiCA utilizes a unique hybrid representation that combines a triangle mesh and anisotropic 3D Gaussians. This combination maximizes memory and rendering efficiency while maintaining a photorealistic appearance. The triangle mesh is highly efficient in representing surface areas like facial skin, while the 3D Gaussians effectively handle non-surface areas such as hair and beard. To this end, we develop a unified differentiable rendering pipeline that treats the mesh as a semi-transparent layer within the volumetric rendering paradigm of 3D Gaussian Splatting. We train neural networks to decode a facial expression code into three components: a 3D face mesh, an RGBA texture, and a set of 3D Gaussians. These components are rendered simultaneously in a unified rendering engine. The networks are trained using multi-view image supervision. Our results demonstrate that GPiCA achieves the realism of purely Gaussian-based avatars while matching the rendering performance of mesh-based avatars.",
    "arxiv_url": "https://arxiv.org/abs/2512.15711v1",
    "pdf_url": "https://arxiv.org/pdf/2512.15711v1",
    "published_date": "2025-12-17",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "face",
      "avatar",
      "ar",
      "efficient rendering",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.15711v1",
      "pdf": "https://arxiv.org/pdf/2512.15711v1"
    },
    "bibtex": ""
  },
  {
    "title": "Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting",
    "authors": [
      "Arthur Moreau",
      "Richard Shaw",
      "Michal Nazarczuk",
      "Jisu Shin",
      "Thomas Tanay",
      "Zhensong Zhang",
      "Songcen Xu",
      "Eduardo Pérez-Pellitero"
    ],
    "abstract": "Feed-forward 3D Gaussian Splatting (3DGS) models enable real-time scene generation but are hindered by suboptimal pixel-aligned primitive placement, which relies on a dense, rigid grid and limits both quality and efficiency. We introduce a new feed-forward architecture that detects 3D Gaussian primitives at a sub-pixel level, replacing the pixel grid with an adaptive, \"Off The Grid\" distribution. Inspired by keypoint detection, our multi-resolution decoder learns to distribute primitives across image patches. This module is trained end-to-end with a 3D reconstruction backbone using self-supervised learning. Our resulting pose-free model generates photorealistic scenes in seconds, achieving state-of-the-art novel view synthesis for feed-forward models. It outperforms competitors while using far fewer primitives, demonstrating a more accurate and efficient allocation that captures fine details and reduces artifacts. Moreover, we observe that by learning to render 3D Gaussians, our 3D reconstruction backbone improves camera pose estimation, suggesting opportunities to train these foundational models without labels.",
    "arxiv_url": "https://arxiv.org/abs/2512.15508v1",
    "pdf_url": "https://arxiv.org/pdf/2512.15508v1",
    "published_date": "2025-12-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.15508v1",
      "pdf": "https://arxiv.org/pdf/2512.15508v1"
    },
    "bibtex": ""
  },
  {
    "title": "VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments",
    "authors": [
      "Yuze Wu",
      "Mo Zhu",
      "Xingxing Li",
      "Yuheng Du",
      "Yuxin Fan",
      "Wenjun Li",
      "Zhichao Han",
      "Xin Zhou",
      "Fei Gao"
    ],
    "abstract": "This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.",
    "arxiv_url": "https://arxiv.org/abs/2512.15258v2",
    "pdf_url": "https://arxiv.org/pdf/2512.15258v2",
    "published_date": "2025-12-17",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "lightweight",
      "ar",
      "3d gaussian",
      "efficient",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.15258v2",
      "pdf": "https://arxiv.org/pdf/2512.15258v2"
    },
    "bibtex": ""
  },
  {
    "title": "COSMOS: Coherent Supergaussian Modeling with Spatial Priors for Sparse-View 3D Splatting",
    "authors": [
      "Chaeyoung Jeong",
      "Kwangsu Kim"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a promising approach for 3D reconstruction, providing explicit, point-based representations and enabling high-quality real time rendering. However, when trained with sparse input views, 3DGS suffers from overfitting and structural degradation, leading to poor generalization on novel views. This limitation arises from its optimization relying solely on photometric loss without incorporating any 3D structure priors. To address this issue, we propose Coherent supergaussian Modeling with Spatial Priors (COSMOS). Inspired by the concept of superpoints from 3D segmentation, COSMOS introduces 3D structure priors by newly defining supergaussian groupings of Gaussians based on local geometric cues and appearance features. To this end, COSMOS applies inter group global self-attention across supergaussian groups and sparse local attention among individual Gaussians, enabling the integration of global and local spatial information. These structure-aware features are then used for predicting Gaussian attributes, facilitating more consistent 3D reconstructions. Furthermore, by leveraging supergaussian-based grouping, COSMOS enforces an intra-group positional regularization to maintain structural coherence and suppress floaters, thereby enhancing training stability under sparse-view conditions. Our experiments on Blender and DTU show that COSMOS surpasses state-of-the-art methods in sparse-view settings without any external depth supervision.",
    "arxiv_url": "https://arxiv.org/abs/2602.06044v1",
    "pdf_url": "https://arxiv.org/pdf/2602.06044v1",
    "published_date": "2025-12-17",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "3d reconstruction",
      "segmentation",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2602.06044v1",
      "pdf": "https://arxiv.org/pdf/2602.06044v1"
    },
    "bibtex": ""
  },
  {
    "title": "MVGSR: Multi-View Consistent 3D Gaussian Super-Resolution via Epipolar Guidance",
    "authors": [
      "Kaizhe Zhang",
      "Shinan Chen",
      "Qian Zhao",
      "Weizhan Zhang",
      "Caixia Yan",
      "Yudeng Xin"
    ],
    "abstract": "Scenes reconstructed by 3D Gaussian Splatting (3DGS) trained on low-resolution (LR) images are unsuitable for high-resolution (HR) rendering. Consequently, a 3DGS super-resolution (SR) method is needed to bridge LR inputs and HR rendering. Early 3DGS SR methods rely on single-image SR networks, which lack cross-view consistency and fail to fuse complementary information across views. More recent video-based SR approaches attempt to address this limitation but require strictly sequential frames, limiting their applicability to unstructured multi-view datasets. In this work, we introduce Multi-View Consistent 3D Gaussian Splatting Super-Resolution (MVGSR), a framework that focuses on integrating multi-view information for 3DGS rendering with high-frequency details and enhanced consistency. We first propose an Auxiliary View Selection Method based on camera poses, making our method adaptable for arbitrarily organized multi-view datasets without the need of temporal continuity or data reordering. Furthermore, we introduce, for the first time, an epipolar-constrained multi-view attention mechanism into 3DGS SR, which serves as the core of our proposed multi-view SR network. This design enables the model to selectively aggregate consistent information from auxiliary views, enhancing the geometric consistency and detail fidelity of 3DGS representations. Extensive experiments demonstrate that our method achieves state-of-the-art performance on both object-centric and scene-level 3DGS SR benchmarks.",
    "arxiv_url": "https://arxiv.org/abs/2512.15048v1",
    "pdf_url": "https://arxiv.org/pdf/2512.15048v1",
    "published_date": "2025-12-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.15048v1",
      "pdf": "https://arxiv.org/pdf/2512.15048v1"
    },
    "bibtex": ""
  },
  {
    "title": "Broadening View Synthesis of Dynamic Scenes from Constrained Monocular Videos",
    "authors": [
      "Le Jiang",
      "Shaotong Zhu",
      "Yedi Luo",
      "Shayda Moezzi",
      "Sarah Ostadabbas"
    ],
    "abstract": "In dynamic Neural Radiance Fields (NeRF) systems, state-of-the-art novel view synthesis methods often fail under significant viewpoint deviations, producing unstable and unrealistic renderings. To address this, we introduce Expanded Dynamic NeRF (ExpanDyNeRF), a monocular NeRF framework that leverages Gaussian splatting priors and a pseudo-ground-truth generation strategy to enable realistic synthesis under large-angle rotations. ExpanDyNeRF optimizes density and color features to improve scene reconstruction from challenging perspectives. We also present the Synthetic Dynamic Multiview (SynDM) dataset, the first synthetic multiview dataset for dynamic scenes with explicit side-view supervision-created using a custom GTA V-based rendering pipeline. Quantitative and qualitative results on SynDM and real-world datasets demonstrate that ExpanDyNeRF significantly outperforms existing dynamic NeRF methods in rendering fidelity under extreme viewpoint shifts. Further details are provided in the supplementary materials.",
    "arxiv_url": "https://arxiv.org/abs/2512.14406v1",
    "pdf_url": "https://arxiv.org/pdf/2512.14406v1",
    "published_date": "2025-12-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.14406v1",
      "pdf": "https://arxiv.org/pdf/2512.14406v1"
    },
    "bibtex": ""
  },
  {
    "title": "HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis",
    "authors": [
      "Kaizhe Zhang",
      "Yijie Zhou",
      "Weizhan Zhang",
      "Caixia Yan",
      "Haipeng Du",
      "yugui xie",
      "Yu-Hui Wen",
      "Yong-Jin Liu"
    ],
    "abstract": "Dynamic novel view synthesis (NVS) is essential for creating immersive experiences. Existing approaches have advanced dynamic NVS by introducing 3D Gaussian Splatting (3DGS) with implicit deformation fields or indiscriminately assigned time-varying parameters, surpassing NeRF-based methods. However, due to excessive model complexity and parameter redundancy, they incur large model sizes and slow rendering speeds, making them inefficient for real-time applications, particularly on resource-constrained devices. To obtain a more efficient model with fewer redundant parameters, in this paper, we propose Hybrid Gaussian Splatting (HGS), a compact and efficient framework explicitly designed to disentangle static and dynamic regions of a scene within a unified representation. The core innovation of HGS lies in our Static-Dynamic Decomposition (SDD) strategy, which leverages Radial Basis Function (RBF) modeling for Gaussian primitives. Specifically, for dynamic regions, we employ time-dependent RBFs to effectively capture temporal variations and handle abrupt scene changes, while for static regions, we reduce redundancy by sharing temporally invariant parameters. Additionally, we introduce a two-stage training strategy tailored for explicit models to enhance temporal coherence at static-dynamic boundaries. Experimental results demonstrate that our method reduces model size by up to 98% and achieves real-time rendering at up to 125 FPS at 4K resolution on a single RTX 3090 GPU. It further sustains 160 FPS at 1352 * 1014 on an RTX 3050 and has been integrated into the VR system. Moreover, HGS achieves comparable rendering quality to state-of-the-art methods while providing significantly improved visual fidelity for high-frequency details and abrupt scene changes.",
    "arxiv_url": "https://arxiv.org/abs/2512.14352v1",
    "pdf_url": "https://arxiv.org/pdf/2512.14352v1",
    "published_date": "2025-12-16",
    "categories": [
      "cs.CV",
      "cs.CG"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "dynamic",
      "vr",
      "deformation",
      "real-time rendering",
      "ar",
      "nerf",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.14352v1",
      "pdf": "https://arxiv.org/pdf/2512.14352v1"
    },
    "bibtex": ""
  },
  {
    "title": "Beyond a Single Light: A Large-Scale Aerial Dataset for Urban Scene Reconstruction Under Varying Illumination",
    "authors": [
      "Zhuoxiao Li",
      "Wenzong Ma",
      "Taoyu Wu",
      "Jinjing Zhu",
      "Zhenchao Q",
      "Shuai Zhang",
      "Jing Ou",
      "Yinrui Ren",
      "Weiqing Qi",
      "Guobin Shen",
      "Hui Xiong",
      "Wufan Zhao"
    ],
    "abstract": "Recent advances in Neural Radiance Fields and 3D Gaussian Splatting have demonstrated strong potential for large-scale UAV-based 3D reconstruction tasks by fitting the appearance of images. However, real-world large-scale captures are often based on multi-temporal data capture, where illumination inconsistencies across different times of day can significantly lead to color artifacts, geometric inaccuracies, and inconsistent appearance. Due to the lack of UAV datasets that systematically capture the same areas under varying illumination conditions, this challenge remains largely underexplored. To fill this gap, we introduceSkyLume, a large-scale, real-world UAV dataset specifically designed for studying illumination robust 3D reconstruction in urban scene modeling: (1) We collect data from 10 urban regions data comprising more than 100k high resolution UAV images (four oblique views and nadir), where each region is captured at three periods of the day to systematically isolate illumination changes. (2) To support precise evaluation of geometry and appearance, we provide per-scene LiDAR scans and accurate 3D ground-truth for assessing depth, surface normals, and reconstruction quality under varying illumination. (3) For the inverse rendering task, we introduce the Temporal Consistency Coefficient (TCC), a metric that measuress cross-time albedo stability and directly evaluates the robustness of the disentanglement of light and material. We aim for this resource to serve as a foundation that advances research and real-world evaluation in large-scale inverse rendering, geometry reconstruction, and novel view synthesis.",
    "arxiv_url": "https://arxiv.org/abs/2512.14200v1",
    "pdf_url": "https://arxiv.org/pdf/2512.14200v1",
    "published_date": "2025-12-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "urban scene",
      "face",
      "3d reconstruction",
      "geometry",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.14200v1",
      "pdf": "https://arxiv.org/pdf/2512.14200v1"
    },
    "bibtex": ""
  },
  {
    "title": "Spherical Voronoi: Directional Appearance as a Differentiable Partition of the Sphere",
    "authors": [
      "Francesco Di Sario",
      "Daniel Rebain",
      "Dor Verbin",
      "Marco Grangetto",
      "Andrea Tagliasacchi"
    ],
    "abstract": "Radiance field methods (e.g. 3D Gaussian Splatting) have emerged as a powerful paradigm for novel view synthesis, yet their appearance modeling often relies on Spherical Harmonics (SH), which impose fundamental limitations. SH struggle with high-frequency signals, exhibit Gibbs ringing artifacts, and fail to capture specular reflections - a key component of realistic rendering. Although alternatives like spherical Gaussians offer improvements, they add significant optimization complexity. We propose Spherical Voronoi (SV) as a unified framework for appearance representation in 3D Gaussian Splatting. SV partitions the directional domain into learnable regions with smooth boundaries, providing an intuitive and stable parameterization for view-dependent effects. For diffuse appearance, SV achieves competitive results while keeping optimization simpler than existing alternatives. For reflections - where SH fail - we leverage SV as learnable reflection probes, taking reflected directions as input following principles from classical graphics. This formulation attains state-of-the-art results on synthetic and real-world datasets, demonstrating that SV offers a principled, efficient, and general solution for appearance modeling in explicit 3D representations.",
    "arxiv_url": "https://arxiv.org/abs/2512.14180v1",
    "pdf_url": "https://arxiv.org/pdf/2512.14180v1",
    "published_date": "2025-12-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "reflection",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.14180v1",
      "pdf": "https://arxiv.org/pdf/2512.14180v1"
    },
    "bibtex": ""
  },
  {
    "title": "Consistent Instance Field for Dynamic Scene Understanding",
    "authors": [
      "Junyi Wu",
      "Van Nguyen Nguyen",
      "Benjamin Planche",
      "Jiachen Tao",
      "Changchang Sun",
      "Zhongpai Gao",
      "Zhenghao Zhao",
      "Anwesa Choudhuri",
      "Gengyu Zhang",
      "Meng Zheng",
      "Feiran Wang",
      "Terrence Chen",
      "Yan Yan",
      "Ziyan Wu"
    ],
    "abstract": "We introduce Consistent Instance Field, a continuous and probabilistic spatio-temporal representation for dynamic scene understanding. Unlike prior methods that rely on discrete tracking or view-dependent features, our approach disentangles visibility from persistent object identity by modeling each space-time point with an occupancy probability and a conditional instance distribution. To realize this, we introduce a novel instance-embedded representation based on deformable 3D Gaussians, which jointly encode radiance and semantic information and are learned directly from input RGB images and instance masks through differentiable rasterization. Furthermore, we introduce new mechanisms to calibrate per-Gaussian identities and resample Gaussians toward semantically active regions, ensuring consistent instance representations across space and time. Experiments on HyperNeRF and Neu3D datasets demonstrate that our method significantly outperforms state-of-the-art methods on novel-view panoptic segmentation and open-vocabulary 4D querying tasks.",
    "arxiv_url": "https://arxiv.org/abs/2512.14126v1",
    "pdf_url": "https://arxiv.org/pdf/2512.14126v1",
    "published_date": "2025-12-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "tracking",
      "segmentation",
      "ar",
      "semantic",
      "nerf",
      "3d gaussian",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.14126v1",
      "pdf": "https://arxiv.org/pdf/2512.14126v1"
    },
    "bibtex": ""
  },
  {
    "title": "GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants",
    "authors": [
      "Yang Yang",
      "Risa Shinoda",
      "Hiroaki Santo",
      "Fumio Okura"
    ],
    "abstract": "We present a method for jointly recovering the appearance and internal structure of botanical plants from multi-view images based on 3D Gaussian Splatting (3DGS). While 3DGS exhibits robust reconstruction of scene appearance for novel-view synthesis, it lacks structural representations underlying those appearances (e.g., branching patterns of plants), which limits its applicability to tasks such as plant phenotyping. To achieve both high-fidelity appearance and structural reconstruction, we introduce GaussianPlant, a hierarchical 3DGS representation, which disentangles structure and appearance. Specifically, we employ structure primitives (StPs) to explicitly represent branch and leaf geometry, and appearance primitives (ApPs) to the plants' appearance using 3D Gaussians. StPs represent a simplified structure of the plant, i.e., modeling branches as cylinders and leaves as disks. To accurately distinguish the branches and leaves, StP's attributes (i.e., branches or leaves) are optimized in a self-organized manner. ApPs are bound to each StP to represent the appearance of branches or leaves as in conventional 3DGS. StPs and ApPs are jointly optimized using a re-rendering loss on the input multi-view images, as well as the gradient flow from ApP to StP using the binding correspondence information. We conduct experiments to qualitatively evaluate the reconstruction accuracy of both appearance and structure, as well as real-world experiments to qualitatively validate the practical performance. Experiments show that the GaussianPlant achieves both high-fidelity appearance reconstruction via ApPs and accurate structural reconstruction via StPs, enabling the extraction of branch structure and leaf instances.",
    "arxiv_url": "https://arxiv.org/abs/2512.14087v1",
    "pdf_url": "https://arxiv.org/pdf/2512.14087v1",
    "published_date": "2025-12-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "3d reconstruction",
      "geometry",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.14087v1",
      "pdf": "https://arxiv.org/pdf/2512.14087v1"
    },
    "bibtex": ""
  },
  {
    "title": "ASAP-Textured Gaussians: Enhancing Textured Gaussians with Adaptive Sampling and Anisotropic Parameterization",
    "authors": [
      "Meng Wei",
      "Cheng Zhang",
      "Jianmin Zheng",
      "Hamid Rezatofighi",
      "Jianfei Cai"
    ],
    "abstract": "Recent advances have equipped 3D Gaussian Splatting with texture parameterizations to capture spatially varying attributes, improving the performance of both appearance modeling and downstream tasks. However, the added texture parameters introduce significant memory efficiency challenges. Rather than proposing new texture formulations, we take a step back to examine the characteristics of existing textured Gaussian methods and identify two key limitations in common: (1) Textures are typically defined in canonical space, leading to inefficient sampling that wastes textures' capacity on low-contribution regions; and (2) texture parameterization is uniformly assigned across all Gaussians, regardless of their visual complexity, resulting in over-parameterization. In this work, we address these issues through two simple yet effective strategies: adaptive sampling based on the Gaussian density distribution and error-driven anisotropic parameterization that allocates texture resources according to rendering error. Our proposed ASAP Textured Gaussians, short for Adaptive Sampling and Anisotropic Parameterization, significantly improve the quality efficiency tradeoff, achieving high-fidelity rendering with far fewer texture parameters.",
    "arxiv_url": "https://arxiv.org/abs/2512.14039v1",
    "pdf_url": "https://arxiv.org/pdf/2512.14039v1",
    "published_date": "2025-12-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.14039v1",
      "pdf": "https://arxiv.org/pdf/2512.14039v1"
    },
    "bibtex": ""
  },
  {
    "title": "Nexels: Neurally-Textured Surfels for Real-Time Novel View Synthesis with Sparse Geometries",
    "authors": [
      "Victor Rong",
      "Jan Held",
      "Victor Chu",
      "Daniel Rebain",
      "Marc Van Droogenbroeck",
      "Kiriakos N. Kutulakos",
      "Andrea Tagliasacchi",
      "David B. Lindell"
    ],
    "abstract": "Though Gaussian splatting has achieved impressive results in novel view synthesis, it requires millions of primitives to model highly textured scenes, even when the geometry of the scene is simple. We propose a representation that goes beyond point-based rendering and decouples geometry and appearance in order to achieve a compact representation. We use surfels for geometry and a combination of a global neural field and per-primitive colours for appearance. The neural field textures a fixed number of primitives for each pixel, ensuring that the added compute is low. Our representation matches the perceptual quality of 3D Gaussian splatting while using $9.7\\times$ fewer primitives and $5.5\\times$ less memory on outdoor scenes and using $31\\times$ fewer primitives and $3.7\\times$ less memory on indoor scenes. Our representation also renders twice as fast as existing textured primitives while improving upon their visual quality.",
    "arxiv_url": "https://arxiv.org/abs/2512.13796v1",
    "pdf_url": "https://arxiv.org/pdf/2512.13796v1",
    "published_date": "2025-12-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "compact",
      "geometry",
      "ar",
      "3d gaussian",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.13796v1",
      "pdf": "https://arxiv.org/pdf/2512.13796v1"
    },
    "bibtex": ""
  },
  {
    "title": "Computer vision training dataset generation for robotic environments using Gaussian splatting",
    "authors": [
      "Patryk Niżeniec",
      "Marcin Iwanowski"
    ],
    "abstract": "This paper introduces a novel pipeline for generating large-scale, highly realistic, and automatically labeled datasets for computer vision tasks in robotic environments. Our approach addresses the critical challenges of the domain gap between synthetic and real-world imagery and the time-consuming bottleneck of manual annotation. We leverage 3D Gaussian Splatting (3DGS) to create photorealistic representations of the operational environment and objects. These assets are then used in a game engine where physics simulations create natural arrangements. A novel, two-pass rendering technique combines the realism of splats with a shadow map generated from proxy meshes. This map is then algorithmically composited with the image to add both physically plausible shadows and subtle highlights, significantly enhancing realism. Pixel-perfect segmentation masks are generated automatically and formatted for direct use with object detection models like YOLO. Our experiments show that a hybrid training strategy, combining a small set of real images with a large volume of our synthetic data, yields the best detection and segmentation performance, confirming this as an optimal strategy for efficiently achieving robust and accurate models.",
    "arxiv_url": "https://arxiv.org/abs/2512.13411v1",
    "pdf_url": "https://arxiv.org/pdf/2512.13411v1",
    "published_date": "2025-12-15",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "shadow",
      "segmentation",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.13411v1",
      "pdf": "https://arxiv.org/pdf/2512.13411v1"
    },
    "bibtex": ""
  },
  {
    "title": "Light Field Based 6DoF Tracking of Previously Unobserved Objects",
    "authors": [
      "Nikolai Goncharov",
      "James L. Gray",
      "Donald G. Dansereau"
    ],
    "abstract": "Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.",
    "arxiv_url": "https://arxiv.org/abs/2512.13007v1",
    "pdf_url": "https://arxiv.org/pdf/2512.13007v1",
    "published_date": "2025-12-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/nagonch/LiFT-6DoF",
    "keywords": [
      "robotics",
      "tracking",
      "autonomous driving",
      "ar",
      "semantic",
      "reflection"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.13007v1",
      "pdf": "https://arxiv.org/pdf/2512.13007v1",
      "github": "https://github.com/nagonch/LiFT-6DoF"
    },
    "bibtex": ""
  },
  {
    "title": "Qonvolution: Towards Learning High-Frequency Signals with Queried Convolution",
    "authors": [
      "Abhinav Kumar",
      "Tristan Aumentado-Armstrong",
      "Lazar Valkov",
      "Gopal Sharma",
      "Alex Levinshtein",
      "Radek Grzeszczuk",
      "Suren Kumar"
    ],
    "abstract": "Accurately learning high-frequency signals is a challenge in computer vision and graphics, as neural networks often struggle with these signals due to spectral bias or optimization difficulties. While current techniques like Fourier encodings have made great strides in improving performance, there remains scope for improvement when presented with high-frequency information. This paper introduces Queried-Convolutions (Qonvolutions), a simple yet powerful modification using the neighborhood properties of convolution. Qonvolution convolves a low-frequency signal with queries (such as coordinates) to enhance the learning of intricate high-frequency signals. We empirically demonstrate that Qonvolutions enhance performance across a variety of high-frequency learning tasks crucial to both the computer vision and graphics communities, including 1D regression, 2D super-resolution, 2D image regression, and novel view synthesis (NVS). In particular, by combining Gaussian splatting with Qonvolutions for NVS, we showcase state-of-the-art performance on real-world complex scenes, even outperforming powerful radiance field models on image quality.",
    "arxiv_url": "https://arxiv.org/abs/2512.12898v1",
    "pdf_url": "https://arxiv.org/pdf/2512.12898v1",
    "published_date": "2025-12-15",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.12898v1",
      "pdf": "https://arxiv.org/pdf/2512.12898v1"
    },
    "bibtex": ""
  },
  {
    "title": "Fast 2DGS: Efficient Image Representation with Deep Gaussian Prior",
    "authors": [
      "Hao Wang",
      "Ashish Bastola",
      "Chaoyi Zhou",
      "Wenhui Zhu",
      "Xiwen Chen",
      "Xuanzhao Dong",
      "Siyu Huang",
      "Abolfazl Razi"
    ],
    "abstract": "As generative models become increasingly capable of producing high-fidelity visual content, the demand for efficient, interpretable, and editable image representations has grown substantially. Recent advances in 2D Gaussian Splatting (2DGS) have emerged as a promising solution, offering explicit control, high interpretability, and real-time rendering capabilities (>1000 FPS). However, high-quality 2DGS typically requires post-optimization. Existing methods adopt random or heuristics (e.g., gradient maps), which are often insensitive to image complexity and lead to slow convergence (>10s). More recent approaches introduce learnable networks to predict initial Gaussian configurations, but at the cost of increased computational and architectural complexity. To bridge this gap, we present Fast-2DGS, a lightweight framework for efficient Gaussian image representation. Specifically, we introduce Deep Gaussian Prior, implemented as a conditional network to capture the spatial distribution of Gaussian primitives under different complexities. In addition, we propose an attribute regression network to predict dense Gaussian properties. Experiments demonstrate that this disentangled architecture achieves high-quality reconstruction in a single forward pass, followed by minimal fine-tuning. More importantly, our approach significantly reduces computational cost without compromising visual quality, bringing 2DGS closer to industry-ready deployment.",
    "arxiv_url": "https://arxiv.org/abs/2512.12774v1",
    "pdf_url": "https://arxiv.org/pdf/2512.12774v1",
    "published_date": "2025-12-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "lightweight",
      "real-time rendering",
      "ar",
      "efficient",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.12774v1",
      "pdf": "https://arxiv.org/pdf/2512.12774v1"
    },
    "bibtex": ""
  },
  {
    "title": "Quantum Implicit Neural Representations for 3D Scene Reconstruction and Novel View Synthesis",
    "authors": [
      "Yeray Cordero",
      "Paula García-Molina",
      "Fernando Vilariño"
    ],
    "abstract": "Implicit neural representations (INRs) have become a powerful paradigm for continuous signal modeling and 3D scene reconstruction, yet classical networks suffer from a well-known spectral bias that limits their ability to capture high-frequency details. Quantum Implicit Representation Networks (QIREN) mitigate this limitation by employing parameterized quantum circuits with inherent Fourier structures, enabling compact and expressive frequency modeling beyond classical MLPs. In this paper, we present Quantum Neural Radiance Fields (Q-NeRF), the first hybrid quantum-classical framework for neural radiance field rendering. Q-NeRF integrates QIREN modules into the Nerfacto backbone, preserving its efficient sampling, pose refinement, and volumetric rendering strategies while replacing selected density and radiance prediction components with quantum-enhanced counterparts. We systematically evaluate three hybrid configurations on standard multi-view indoor datasets, comparing them to classical baselines using PSNR, SSIM, and LPIPS metrics. Results show that hybrid quantum-classical models achieve competitive reconstruction quality under limited computational resources, with quantum modules particularly effective in representing fine-scale, view-dependent appearance. Although current implementations rely on quantum circuit simulators constrained to few-qubit regimes, the results highlight the potential of quantum encodings to alleviate spectral bias in implicit representations. Q-NeRF provides a foundational step toward scalable quantum-enabled 3D scene reconstruction and a baseline for future quantum neural rendering research.",
    "arxiv_url": "https://arxiv.org/abs/2512.12683v1",
    "pdf_url": "https://arxiv.org/pdf/2512.12683v1",
    "published_date": "2025-12-14",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "neural rendering",
      "ar",
      "nerf",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.12683v1",
      "pdf": "https://arxiv.org/pdf/2512.12683v1"
    },
    "bibtex": ""
  },
  {
    "title": "From Particles to Fields: Reframing Photon Mapping with Continuous Gaussian Photon Fields",
    "authors": [
      "Jiachen Tao",
      "Benjamin Planche",
      "Van Nguyen Nguyen",
      "Junyi Wu",
      "Yuchun Liu",
      "Haoxuan Wang",
      "Zhongpai Gao",
      "Gengyu Zhang",
      "Meng Zheng",
      "Feiran Wang",
      "Anwesa Choudhuri",
      "Zhenghao Zhao",
      "Weitai Kang",
      "Terrence Chen",
      "Yan Yan",
      "Ziyan Wu"
    ],
    "abstract": "Accurately modeling light transport is essential for realistic image synthesis. Photon mapping provides physically grounded estimates of complex global illumination effects such as caustics and specular-diffuse interactions, yet its per-view radiance estimation remains computationally inefficient when rendering multiple views of the same scene. The inefficiency arises from independent photon tracing and stochastic kernel estimation at each viewpoint, leading to inevitable redundant computation. To accelerate multi-view rendering, we reformulate photon mapping as a continuous and reusable radiance function. Specifically, we introduce the Gaussian Photon Field (GPF), a learnable representation that encodes photon distributions as anisotropic 3D Gaussian primitives parameterized by position, rotation, scale, and spectrum. GPF is initialized from physically traced photons in the first SPPM iteration and optimized using multi-view supervision of final radiance, distilling photon-based light transport into a continuous field. Once trained, the field enables differentiable radiance evaluation along camera rays without repeated photon tracing or iterative refinement. Extensive experiments on scenes with complex light transport, such as caustics and specular-diffuse interactions, demonstrate that GPF attains photon-level accuracy while reducing computation by orders of magnitude, unifying the physical rigor of photon-based rendering with the efficiency of neural scene representations.",
    "arxiv_url": "https://arxiv.org/abs/2512.12459v1",
    "pdf_url": "https://arxiv.org/pdf/2512.12459v1",
    "published_date": "2025-12-13",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "global illumination",
      "illumination",
      "light transport",
      "ar",
      "mapping",
      "3d gaussian",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.12459v1",
      "pdf": "https://arxiv.org/pdf/2512.12459v1"
    },
    "bibtex": ""
  },
  {
    "title": "Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance",
    "authors": [
      "Jan U. Müller",
      "Robin Tim Landsgesell",
      "Leif Van Holland",
      "Patrick Stotko",
      "Reinhard Klein"
    ],
    "abstract": "The recent success of 3D Gaussian Splatting (3DGS) has reshaped novel view synthesis by enabling fast optimization and real-time rendering of high-quality radiance fields. However, it relies on simplified, order-dependent alpha blending and coarse approximations of the density integral within the rasterizer, thereby limiting its ability to render complex, overlapping semi-transparent objects. In this paper, we extend rasterization-based rendering of 3D Gaussian representations with a novel method for high-fidelity transmittance computation, entirely avoiding the need for ray tracing or per-pixel sample sorting. Building on prior work in moment-based order-independent transparency, our key idea is to characterize the density distribution along each camera ray with a compact and continuous representation based on statistical moments. To this end, we analytically derive and compute a set of per-pixel moments from all contributing 3D Gaussians. From these moments, a continuous transmittance function is reconstructed for each ray, which is then independently sampled within each Gaussian. As a result, our method bridges the gap between rasterization and physical accuracy by modeling light attenuation in complex translucent media, significantly improving overall reconstruction and rendering quality.",
    "arxiv_url": "https://arxiv.org/abs/2512.11800v1",
    "pdf_url": "https://arxiv.org/pdf/2512.11800v1",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ray tracing",
      "compact",
      "high-fidelity",
      "real-time rendering",
      "ar",
      "3d gaussian",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.11800v1",
      "pdf": "https://arxiv.org/pdf/2512.11800v1"
    },
    "bibtex": ""
  },
  {
    "title": "Fast and Explicit: Slice-to-Volume Reconstruction via 3D Gaussian Primitives with Analytic Point Spread Function Modeling",
    "authors": [
      "Maik Dannecker",
      "Steven Jia",
      "Nil Stolt-Ansó",
      "Nadine Girard",
      "Guillaume Auzias",
      "François Rousseau",
      "Daniel Rueckert"
    ],
    "abstract": "Recovering high-fidelity 3D images from sparse or degraded 2D images is a fundamental challenge in medical imaging, with broad applications ranging from 3D ultrasound reconstruction to MRI super-resolution. In the context of fetal MRI, high-resolution 3D reconstruction of the brain from motion-corrupted low-resolution 2D acquisitions is a prerequisite for accurate neurodevelopmental diagnosis. While implicit neural representations (INRs) have recently established state-of-the-art performance in self-supervised slice-to-volume reconstruction (SVR), they suffer from a critical computational bottleneck: accurately modeling the image acquisition physics requires expensive stochastic Monte Carlo sampling to approximate the point spread function (PSF). In this work, we propose a shift from neural network based implicit representations to Gaussian based explicit representations. By parameterizing the HR 3D image volume as a field of anisotropic Gaussian primitives, we leverage the property of Gaussians being closed under convolution and thus derive a \\textit{closed-form analytical solution} for the forward model. This formulation reduces the previously intractable acquisition integral to an exact covariance addition ($\\mathbfΣ_{obs} = \\mathbfΣ_{HR} + \\mathbfΣ_{PSF}$), effectively bypassing the need for compute-intensive stochastic sampling while ensuring exact gradient propagation. We demonstrate that our approach matches the reconstruction quality of self-supervised state-of-the-art SVR frameworks while delivering a 5$\\times$--10$\\times$ speed-up on neonatal and fetal data. With convergence often reached in under 30 seconds, our framework paves the way towards translation into clinical routine of real-time fetal 3D MRI. Code will be public at {https://github.com/m-dannecker/Gaussian-Primitives-for-Fast-SVR}.",
    "arxiv_url": "https://arxiv.org/abs/2512.11624v2",
    "pdf_url": "https://arxiv.org/pdf/2512.11624v2",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/m-dannecker/Gaussian-Primitives-for-Fast-SVR",
    "keywords": [
      "vr",
      "high-fidelity",
      "medical",
      "3d reconstruction",
      "ar",
      "3d gaussian",
      "fast",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.11624v2",
      "pdf": "https://arxiv.org/pdf/2512.11624v2",
      "github": "https://github.com/m-dannecker/Gaussian-Primitives-for-Fast-SVR"
    },
    "bibtex": ""
  },
  {
    "title": "Prior-Enhanced Gaussian Splatting for Dynamic Scene Reconstruction from Casual Video",
    "authors": [
      "Meng-Li Shih",
      "Ying-Huan Chen",
      "Yu-Lun Liu",
      "Brian Curless"
    ],
    "abstract": "We introduce a fully automatic pipeline for dynamic scene reconstruction from casually captured monocular RGB videos. Rather than designing a new scene representation, we enhance the priors that drive Dynamic Gaussian Splatting. Video segmentation combined with epipolar-error maps yields object-level masks that closely follow thin structures; these masks (i) guide an object-depth loss that sharpens the consistent video depth, and (ii) support skeleton-based sampling plus mask-guided re-identification to produce reliable, comprehensive 2-D tracks. Two additional objectives embed the refined priors in the reconstruction stage: a virtual-view depth loss removes floaters, and a scaffold-projection loss ties motion nodes to the tracks, preserving fine geometry and coherent motion. The resulting system surpasses previous monocular dynamic scene reconstruction methods and delivers visibly superior renderings",
    "arxiv_url": "https://arxiv.org/abs/2512.11356v1",
    "pdf_url": "https://arxiv.org/pdf/2512.11356v1",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "geometry",
      "segmentation",
      "gaussian splatting",
      "ar",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.11356v1",
      "pdf": "https://arxiv.org/pdf/2512.11356v1"
    },
    "bibtex": ""
  },
  {
    "title": "Lightweight 3D Gaussian Splatting Compression via Video Codec",
    "authors": [
      "Qi Yang",
      "Geert Van Der Auwera",
      "Zhu Li"
    ],
    "abstract": "Current video-based GS compression methods rely on using Parallel Linear Assignment Sorting (PLAS) to convert 3D GS into smooth 2D maps, which are computationally expensive and time-consuming, limiting the application of GS on lightweight devices. In this paper, we propose a Lightweight 3D Gaussian Splatting (GS) Compression method based on Video codec (LGSCV). First, a two-stage Morton scan is proposed to generate blockwise 2D maps that are friendly for canonical video codecs in which the coding units (CU) are square blocks. A 3D Morton scan is used to permute GS primitives, followed by a 2D Morton scan to map the ordered GS primitives to 2D maps in a blockwise style. However, although the blockwise 2D maps report close performance to the PLAS map in high-bitrate regions, they show a quality collapse at medium-to-low bitrates. Therefore, a principal component analysis (PCA) is used to reduce the dimensionality of spherical harmonics (SH), and a MiniPLAS, which is flexible and fast, is designed to permute the primitives within certain block sizes. Incorporating SH PCA and MiniPLAS leads to a significant gain in rate-distortion (RD) performance, especially at medium and low bitrates. MiniPLAS can also guide the setting of the codec CU size configuration and significantly reduce encoding time. Experimental results on the MPEG dataset demonstrate that the proposed LGSCV achieves over 20% RD gain compared with state-of-the-art methods, while reducing 2D map generation time to approximately 1 second and cutting encoding time by 50%. The code is available at https://github.com/Qi-Yangsjtu/LGSCV .",
    "arxiv_url": "https://arxiv.org/abs/2512.11186v1",
    "pdf_url": "https://arxiv.org/pdf/2512.11186v1",
    "published_date": "2025-12-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Qi-Yangsjtu/LGSCV",
    "keywords": [
      "lightweight",
      "compression",
      "ar",
      "3d gaussian",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.11186v1",
      "pdf": "https://arxiv.org/pdf/2512.11186v1",
      "github": "https://github.com/Qi-Yangsjtu/LGSCV"
    },
    "bibtex": ""
  },
  {
    "title": "GaussianHeadTalk: Wobble-Free 3D Talking Heads with Audio Driven Gaussian Splatting",
    "authors": [
      "Madhav Agarwal",
      "Mingtian Zhang",
      "Laura Sevilla-Lara",
      "Steven McDonagh"
    ],
    "abstract": "Speech-driven talking heads have recently emerged and enable interactive avatars. However, real-world applications are limited, as current methods achieve high visual fidelity but slow or fast yet temporally unstable. Diffusion methods provide realistic image generation, yet struggle with oneshot settings. Gaussian Splatting approaches are real-time, yet inaccuracies in facial tracking, or inconsistent Gaussian mappings, lead to unstable outputs and video artifacts that are detrimental to realistic use cases. We address this problem by mapping Gaussian Splatting using 3D Morphable Models to generate person-specific avatars. We introduce transformer-based prediction of model parameters, directly from audio, to drive temporal consistency. From monocular video and independent audio speech inputs, our method enables generation of real-time talking head videos where we report competitive quantitative and qualitative performance.",
    "arxiv_url": "https://arxiv.org/abs/2512.10939v1",
    "pdf_url": "https://arxiv.org/pdf/2512.10939v1",
    "published_date": "2025-12-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "tracking",
      "avatar",
      "ar",
      "mapping",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.10939v1",
      "pdf": "https://arxiv.org/pdf/2512.10939v1"
    },
    "bibtex": ""
  },
  {
    "title": "Sharp Monocular View Synthesis in Less Than a Second",
    "authors": [
      "Lars Mescheder",
      "Wei Dong",
      "Shiwei Li",
      "Xuyang Bai",
      "Marcel Santos",
      "Peiyun Hu",
      "Bruno Lecouat",
      "Mingmin Zhen",
      "Amaël Delaunoy",
      "Tian Fang",
      "Yanghai Tsin",
      "Stephan R. Richter",
      "Vladlen Koltun"
    ],
    "abstract": "We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25-34% and DISTS by 21-43% versus the best prior model, while lowering the synthesis time by three orders of magnitude. Code and weights are provided at https://github.com/apple/ml-sharp",
    "arxiv_url": "https://arxiv.org/abs/2512.10685v1",
    "pdf_url": "https://arxiv.org/pdf/2512.10685v1",
    "published_date": "2025-12-11",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "https://github.com/apple/ml-sharp",
    "keywords": [
      "3d gaussian",
      "ar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.10685v1",
      "pdf": "https://arxiv.org/pdf/2512.10685v1",
      "github": "https://github.com/apple/ml-sharp"
    },
    "bibtex": ""
  },
  {
    "title": "DeMapGS: Simultaneous Mesh Deformation and Surface Attribute Mapping via Gaussian Splatting",
    "authors": [
      "Shuyi Zhou",
      "Shengze Zhong",
      "Kenshi Takayama",
      "Takafumi Taketomi",
      "Takeshi Oishi"
    ],
    "abstract": "We propose DeMapGS, a structured Gaussian Splatting framework that jointly optimizes deformable surfaces and surface-attached 2D Gaussian splats. By anchoring splats to a deformable template mesh, our method overcomes topological inconsistencies and enhances editing flexibility, addressing limitations of prior Gaussian Splatting methods that treat points independently. The unified representation in our method supports extraction of high-fidelity diffuse, normal, and displacement maps, enabling the reconstructed mesh to inherit the photorealistic rendering quality of Gaussian Splatting. To support robust optimization, we introduce a gradient diffusion strategy that propagates supervision across the surface, along with an alternating 2D/3D rendering scheme to handle concave regions. Experiments demonstrate that DeMapGS achieves state-of-the-art mesh reconstruction quality and enables downstream applications for Gaussian splats such as editing and cross-object manipulation through a shared parametric surface.",
    "arxiv_url": "https://arxiv.org/abs/2512.10572v1",
    "pdf_url": "https://arxiv.org/pdf/2512.10572v1",
    "published_date": "2025-12-11",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "deformation",
      "ar",
      "mapping",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.10572v1",
      "pdf": "https://arxiv.org/pdf/2512.10572v1"
    },
    "bibtex": ""
  },
  {
    "title": "Neural Hamiltonian Deformation Fields for Dynamic Scene Rendering",
    "authors": [
      "Hai-Long Qin",
      "Sixian Wang",
      "Guo Lu",
      "Jincheng Dai"
    ],
    "abstract": "Representing and rendering dynamic scenes with complex motions remains challenging in computer vision and graphics. Recent dynamic view synthesis methods achieve high-quality rendering but often produce physically implausible motions. We introduce NeHaD, a neural deformation field for dynamic Gaussian Splatting governed by Hamiltonian mechanics. Our key observation is that existing methods using MLPs to predict deformation fields introduce inevitable biases, resulting in unnatural dynamics. By incorporating physics priors, we achieve robust and realistic dynamic scene rendering. Hamiltonian mechanics provides an ideal framework for modeling Gaussian deformation fields due to their shared phase-space structure, where primitives evolve along energy-conserving trajectories. We employ Hamiltonian neural networks to implicitly learn underlying physical laws governing deformation. Meanwhile, we introduce Boltzmann equilibrium decomposition, an energy-aware mechanism that adaptively separates static and dynamic Gaussians based on their spatial-temporal energy states for flexible rendering. To handle real-world dissipation, we employ second-order symplectic integration and local rigidity regularization as physics-informed constraints for robust dynamics modeling. Additionally, we extend NeHaD to adaptive streaming through scale-aware mipmapping and progressive optimization. Extensive experiments demonstrate that NeHaD achieves physically plausible results with a rendering quality-efficiency trade-off. To our knowledge, this is the first exploration leveraging Hamiltonian mechanics for neural Gaussian deformation, enabling physically realistic dynamic scene rendering with streaming capabilities.",
    "arxiv_url": "https://arxiv.org/abs/2512.10424v1",
    "pdf_url": "https://arxiv.org/pdf/2512.10424v1",
    "published_date": "2025-12-11",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "deformation",
      "gaussian splatting",
      "ar",
      "mapping",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.10424v1",
      "pdf": "https://arxiv.org/pdf/2512.10424v1"
    },
    "bibtex": ""
  },
  {
    "title": "Breaking the Vicious Cycle: Coherent 3D Gaussian Splatting from Sparse and Motion-Blurred Views",
    "authors": [
      "Zhankuo Xu",
      "Chaoran Feng",
      "Yingtao Li",
      "Jianbin Zhao",
      "Jiashu Yang",
      "Wangbo Yu",
      "Li Yuan",
      "Yonghong Tian"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a state-of-the-art method for novel view synthesis. However, its performance heavily relies on dense, high-quality input imagery, an assumption that is often violated in real-world applications, where data is typically sparse and motion-blurred. These two issues create a vicious cycle: sparse views ignore the multi-view constraints necessary to resolve motion blur, while motion blur erases high-frequency details crucial for aligning the limited views. Thus, reconstruction often fails catastrophically, with fragmented views and a low-frequency bias. To break this cycle, we introduce CoherentGS, a novel framework for high-fidelity 3D reconstruction from sparse and blurry images. Our key insight is to address these compound degradations using a dual-prior strategy. Specifically, we combine two pre-trained generative models: a specialized deblurring network for restoring sharp details and providing photometric guidance, and a diffusion model that offers geometric priors to fill in unobserved regions of the scene. This dual-prior strategy is supported by several key techniques, including a consistency-guided camera exploration module that adaptively guides the generative process, and a depth regularization loss that ensures geometric plausibility. We evaluate CoherentGS through both quantitative and qualitative experiments on synthetic and real-world scenes, using as few as 3, 6, and 9 input views. Our results demonstrate that CoherentGS significantly outperforms existing methods, setting a new state-of-the-art for this challenging task. The code and video demos are available at https://potatobigroom.github.io/CoherentGS/.",
    "arxiv_url": "https://arxiv.org/abs/2512.10369v2",
    "pdf_url": "https://arxiv.org/pdf/2512.10369v2",
    "published_date": "2025-12-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "high-fidelity",
      "3d reconstruction",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.10369v2",
      "pdf": "https://arxiv.org/pdf/2512.10369v2",
      "project": "https://potatobigroom.github.io/CoherentGS"
    },
    "bibtex": ""
  },
  {
    "title": "Physically Aware 360$^\\circ$ View Generation from a Single Image using Disentangled Scene Embeddings",
    "authors": [
      "Karthikeya KV",
      "Narendra Bandaru"
    ],
    "abstract": "We introduce Disentangled360, an innovative 3D-aware technology that integrates the advantages of direction disentangled volume rendering with single-image 360° unique view synthesis for applications in medical imaging and natural scene reconstruction. In contrast to current techniques that either oversimplify anisotropic light behavior or lack generalizability across various contexts, our framework distinctly differentiates between isotropic and anisotropic contributions inside a Gaussian Splatting backbone. We implement a dual-branch conditioning framework, one optimized for CT intensity driven scattering in volumetric data and the other for real-world RGB scenes through normalized camera embeddings. To address scale ambiguity and maintain structural realism, we present a hybrid pose agnostic anchoring method that adaptively samples scene depth and material transitions, functioning as stable pivots during scene distillation. Our design integrates preoperative radiography simulation and consumer-grade 360° rendering into a singular inference pipeline, facilitating rapid, photorealistic view synthesis with inherent directionality. Evaluations on the Mip-NeRF 360, RealEstate10K, and DeepDRR datasets indicate superior SSIM and LPIPS performance, while runtime assessments confirm its viability for interactive applications. Disentangled360 facilitates mixed-reality medical supervision, robotic perception, and immersive content creation, eliminating the necessity for scene-specific finetuning or expensive photon simulations.",
    "arxiv_url": "https://arxiv.org/abs/2512.10293v1",
    "pdf_url": "https://arxiv.org/pdf/2512.10293v1",
    "published_date": "2025-12-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "medical",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.10293v1",
      "pdf": "https://arxiv.org/pdf/2512.10293v1"
    },
    "bibtex": ""
  },
  {
    "title": "Long-LRM++: Preserving Fine Details in Feed-Forward Wide-Coverage Reconstruction",
    "authors": [
      "Chen Ziwen",
      "Hao Tan",
      "Peng Wang",
      "Zexiang Xu",
      "Li Fuxin"
    ],
    "abstract": "Recent advances in generalizable Gaussian splatting (GS) have enabled feed-forward reconstruction of scenes from tens of input views. Long-LRM notably scales this paradigm to 32 input images at $950\\times540$ resolution, achieving 360° scene-level reconstruction in a single forward pass. However, directly predicting millions of Gaussian parameters at once remains highly error-sensitive: small inaccuracies in positions or other attributes lead to noticeable blurring, particularly in fine structures such as text. In parallel, implicit representation methods such as LVSM and LaCT have demonstrated significantly higher rendering fidelity by compressing scene information into model weights rather than explicit Gaussians, and decoding RGB frames using the full transformer or TTT backbone. However, this computationally intensive decompression process for every rendered frame makes real-time rendering infeasible. These observations raise key questions: Is the deep, sequential \"decompression\" process necessary? Can we retain the benefits of implicit representations while enabling real-time performance? We address these questions with Long-LRM++, a model that adopts a semi-explicit scene representation combined with a lightweight decoder. Long-LRM++ matches the rendering quality of LaCT on DL3DV while achieving real-time 14 FPS rendering on an A100 GPU, overcoming the speed limitations of prior implicit methods. Our design also scales to 64 input views at the $950\\times540$ resolution, demonstrating strong generalization to increased input lengths. Additionally, Long-LRM++ delivers superior novel-view depth prediction on ScanNetv2 compared to direct depth rendering from Gaussians. Extensive ablation studies validate the effectiveness of each component in the proposed framework.",
    "arxiv_url": "https://arxiv.org/abs/2512.10267v1",
    "pdf_url": "https://arxiv.org/pdf/2512.10267v1",
    "published_date": "2025-12-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "compression",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.10267v1",
      "pdf": "https://arxiv.org/pdf/2512.10267v1"
    },
    "bibtex": ""
  },
  {
    "title": "TraceFlow: Dynamic 3D Reconstruction of Specular Scenes Driven by Ray Tracing",
    "authors": [
      "Jiachen Tao",
      "Junyi Wu",
      "Haoxuan Wang",
      "Zongxin Yang",
      "Dawen Cai",
      "Yan Yan"
    ],
    "abstract": "We present TraceFlow, a novel framework for high-fidelity rendering of dynamic specular scenes by addressing two key challenges: precise reflection direction estimation and physically accurate reflection modeling. To achieve this, we propose a Residual Material-Augmented 2D Gaussian Splatting representation that models dynamic geometry and material properties, allowing accurate reflection ray computation. Furthermore, we introduce a Dynamic Environment Gaussian and a hybrid rendering pipeline that decomposes rendering into diffuse and specular components, enabling physically grounded specular synthesis via rasterization and ray tracing. Finally, we devise a coarse-to-fine training strategy to improve optimization stability and promote physically meaningful decomposition. Extensive experiments on dynamic scene benchmarks demonstrate that TraceFlow outperforms prior methods both quantitatively and qualitatively, producing sharper and more realistic specular reflections in complex dynamic environments.",
    "arxiv_url": "https://arxiv.org/abs/2512.10095v1",
    "pdf_url": "https://arxiv.org/pdf/2512.10095v1",
    "published_date": "2025-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ray tracing",
      "dynamic",
      "high-fidelity",
      "3d reconstruction",
      "geometry",
      "ar",
      "reflection",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.10095v1",
      "pdf": "https://arxiv.org/pdf/2512.10095v1"
    },
    "bibtex": ""
  },
  {
    "title": "GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures",
    "authors": [
      "Patrick Noras",
      "Jun Myeong Choi",
      "Didier Stricker",
      "Pieter Peers",
      "Roni Sengupta"
    ],
    "abstract": "Recent advances in Gaussian Splatting-based inverse rendering extend Gaussian primitives with shading parameters and physically grounded light transport, enabling high-quality material recovery from dense multi-view captures. However, these methods degrade sharply under sparse-view settings, where limited observations lead to severe ambiguity between geometry, reflectance, and lighting. We introduce GAINS (Gaussian-based Inverse rendering from Sparse multi-view captures), a two-stage inverse rendering framework that leverages learning-based priors to stabilize geometry and material estimation. GAINS first refines geometry using monocular depth/normal and diffusion priors, then employs segmentation, intrinsic image decomposition (IID), and diffusion priors to regularize material recovery. Extensive experiments on synthetic and real-world datasets show that GAINS significantly improves material parameter accuracy, relighting quality, and novel-view synthesis compared to state-of-the-art Gaussian-based inverse rendering methods, especially under sparse-view settings. Project page: https://patrickbail.github.io/gains/",
    "arxiv_url": "https://arxiv.org/abs/2512.09925v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09925v1",
    "published_date": "2025-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "light transport",
      "sparse-view",
      "geometry",
      "segmentation",
      "ar",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.09925v1",
      "pdf": "https://arxiv.org/pdf/2512.09925v1",
      "project": "https://patrickbail.github.io/gains"
    },
    "bibtex": ""
  },
  {
    "title": "Splatent: Splatting Diffusion Latents for Novel View Synthesis",
    "authors": [
      "Or Hirschorn",
      "Omer Sela",
      "Inbar Huberman-Spiegelglas",
      "Netalee Efrat",
      "Eli Alshan",
      "Ianir Ideses",
      "Frederic Devernay",
      "Yochai Zvik",
      "Lior Fritz"
    ],
    "abstract": "Radiance field representations have recently been explored in the latent space of VAEs that are commonly used by diffusion models. This direction offers efficient rendering and seamless integration with diffusion-based pipelines. However, these methods face a fundamental limitation: The VAE latent space lacks multi-view consistency, leading to blurred textures and missing details during 3D reconstruction. Existing approaches attempt to address this by fine-tuning the VAE, at the cost of reconstruction quality, or by relying on pre-trained diffusion models to recover fine-grained details, at the risk of some hallucinations. We present Splatent, a diffusion-based enhancement framework designed to operate on top of 3D Gaussian Splatting (3DGS) in the latent space of VAEs. Our key insight departs from the conventional 3D-centric view: rather than reconstructing fine-grained details in 3D space, we recover them in 2D from input views through multi-view attention mechanisms. This approach preserves the reconstruction quality of pretrained VAEs while achieving faithful detail recovery. Evaluated across multiple benchmarks, Splatent establishes a new state-of-the-art for VAE latent radiance field reconstruction. We further demonstrate that integrating our method with existing feed-forward frameworks, consistently improves detail preservation, opening new possibilities for high-quality sparse-view 3D reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2512.09923v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09923v1",
    "published_date": "2025-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "face",
      "3d reconstruction",
      "ar",
      "efficient rendering",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.09923v1",
      "pdf": "https://arxiv.org/pdf/2512.09923v1"
    },
    "bibtex": ""
  },
  {
    "title": "YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos",
    "authors": [
      "Ryan Meegan",
      "Adam D'Souza",
      "Bryan Bo Cao",
      "Shubham Jain",
      "Kristin Dana"
    ],
    "abstract": "Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning. However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive. We address the problem of visual navigation when exploration videos of a large environment are available. The videos serve as a visual reference, allowing a robot to retrace the explored trajectories without relying on metric maps. Our proposed method, YOPO-Nav (You Only Pass Once), encodes an environment into a compact spatial representation composed of interconnected local 3D Gaussian Splatting (3DGS) models. During navigation, the framework aligns the robot's current visual observation with this representation and predicts actions that guide it back toward the demonstrated trajectory. YOPO-Nav employs a hierarchical design: a visual place recognition (VPR) module provides coarse localization, while the local 3DGS models refine the goal and intermediate poses to generate control actions. To evaluate our approach, we introduce the YOPO-Campus dataset, comprising 4 hours of egocentric video and robot controller inputs from over 6 km of human-teleoperated robot trajectories. We benchmark recent visual navigation methods on trajectories from YOPO-Campus using a Clearpath Jackal robot. Experimental results show YOPO-Nav provides excellent performance in image-goal navigation for real-world scenes on a physical robot. The dataset and code will be made publicly available for visual navigation and scene representation research.",
    "arxiv_url": "https://arxiv.org/abs/2512.09903v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09903v1",
    "published_date": "2025-12-10",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "localization",
      "recognition",
      "ar",
      "human",
      "3d gaussian",
      "mapping",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.09903v1",
      "pdf": "https://arxiv.org/pdf/2512.09903v1"
    },
    "bibtex": ""
  },
  {
    "title": "ReMoSPLAT: Reactive Mobile Manipulation Control on a Gaussian Splat",
    "authors": [
      "Nicolas Marticorena",
      "Tobias Fischer",
      "Niko Suenderhauf"
    ],
    "abstract": "Reactive control can gracefully coordinate the motion of the base and the arm of a mobile manipulator. However, incorporating an accurate representation of the environment to avoid obstacles without involving costly planning remains a challenge. In this work, we present ReMoSPLAT, a reactive controller based on a quadratic program formulation for mobile manipulation that leverages a Gaussian Splat representation for collision avoidance. By integrating additional constraints and costs into the optimisation formulation, a mobile manipulator platform can reach its intended end effector pose while avoiding obstacles, even in cluttered scenes. We investigate the trade-offs of two methods for efficiently calculating robot-obstacle distances, comparing a purely geometric approach with a rasterisation-based approach. Our experiments in simulation on both synthetic and real-world scans demonstrate the feasibility of our method, showing that the proposed approach achieves performance comparable to controllers that rely on perfect ground-truth information.",
    "arxiv_url": "https://arxiv.org/abs/2512.09656v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09656v1",
    "published_date": "2025-12-10",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.09656v1",
      "pdf": "https://arxiv.org/pdf/2512.09656v1"
    },
    "bibtex": ""
  },
  {
    "title": "Super4DR: 4D Radar-centric Self-supervised Odometry and Gaussian-based Map Optimization",
    "authors": [
      "Zhiheng Li",
      "Weihua Wang",
      "Qiang Shen",
      "Yichen Zhao",
      "Zheng Fang"
    ],
    "abstract": "Conventional SLAM systems using visual or LiDAR data often struggle in poor lighting and severe weather. Although 4D radar is suited for such environments, its sparse and noisy point clouds hinder accurate odometry estimation, while the radar maps suffer from obscure and incomplete structures. Thus, we propose Super4DR, a 4D radar-centric framework for learning-based odometry estimation and gaussian-based map optimization. First, we design a cluster-aware odometry network that incorporates object-level cues from the clustered radar points for inter-frame matching, alongside a hierarchical self-supervision mechanism to overcome outliers through spatio-temporal consistency, knowledge transfer, and feature contrast. Second, we propose using 3D gaussians as an intermediate representation, coupled with a radar-specific growth strategy, selective separation, and multi-view regularization, to recover blurry map areas and those undetected based on image texture. Experiments show that Super4DR achieves a 67% performance gain over prior self-supervised methods, nearly matches supervised odometry, and narrows the map quality disparity with LiDAR while enabling multi-modal image rendering.",
    "arxiv_url": "https://arxiv.org/abs/2512.09608v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09608v1",
    "published_date": "2025-12-10",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "3d gaussian",
      "slam",
      "lighting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.09608v1",
      "pdf": "https://arxiv.org/pdf/2512.09608v1"
    },
    "bibtex": ""
  },
  {
    "title": "D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM",
    "authors": [
      "Siting Zhu",
      "Yuxiang Huang",
      "Wenhua Wu",
      "Chaokang Jiang",
      "Yongbo Chen",
      "I-Ming Chen",
      "Hesheng Wang"
    ],
    "abstract": "Recent advances in Dense Simultaneous Localization and Mapping (SLAM) have demonstrated remarkable performance in static environments. However, dense SLAM in dynamic environments remains challenging. Most methods directly remove dynamic objects and focus solely on static scene reconstruction, which ignores the motion information contained in these dynamic objects. In this paper, we present D$^2$GSLAM, a novel dynamic SLAM system utilizing Gaussian representation, which simultaneously performs accurate dynamic reconstruction and robust tracking within dynamic environments. Our system is composed of four key components: (i) We propose a geometric-prompt dynamic separation method to distinguish between static and dynamic elements of the scene. This approach leverages the geometric consistency of Gaussian representation and scene geometry to obtain coarse dynamic regions. The regions then serve as prompts to guide the refinement of the coarse mask for achieving accurate motion mask. (ii) To facilitate accurate and efficient mapping of the dynamic scene, we introduce dynamic-static composite representation that integrates static 3D Gaussians with dynamic 4D Gaussians. This representation allows for modeling the transitions between static and dynamic states of objects in the scene for composite mapping and optimization. (iii) We employ a progressive pose refinement strategy that leverages both the multi-view consistency of static scene geometry and motion information from dynamic objects to achieve accurate camera tracking. (iv) We introduce a motion consistency loss, which leverages the temporal continuity in object motions for accurate dynamic modeling. Our D$^2$GSLAM demonstrates superior performance on dynamic scenes in terms of mapping and tracking accuracy, while also showing capability in accurate dynamic modeling.",
    "arxiv_url": "https://arxiv.org/abs/2512.09411v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09411v1",
    "published_date": "2025-12-10",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "localization",
      "dynamic",
      "tracking",
      "motion",
      "geometry",
      "ar",
      "mapping",
      "3d gaussian",
      "slam",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.09411v1",
      "pdf": "https://arxiv.org/pdf/2512.09411v1"
    },
    "bibtex": ""
  },
  {
    "title": "Relightable and Dynamic Gaussian Avatar Reconstruction from Monocular Video",
    "authors": [
      "Seonghwa Choi",
      "Moonkyeong Choi",
      "Mingyu Jang",
      "Jaekyung Kim",
      "Jianfei Cai",
      "Wen-Huang Cheng",
      "Sanghoon Lee"
    ],
    "abstract": "Modeling relightable and animatable human avatars from monocular video is a long-standing and challenging task. Recently, Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) methods have been employed to reconstruct the avatars. However, they often produce unsatisfactory photo-realistic results because of insufficient geometrical details related to body motion, such as clothing wrinkles. In this paper, we propose a 3DGS-based human avatar modeling framework, termed as Relightable and Dynamic Gaussian Avatar (RnD-Avatar), that presents accurate pose-variant deformation for high-fidelity geometrical details. To achieve this, we introduce dynamic skinning weights that define the human avatar's articulation based on pose while also learning additional deformations induced by body motion. We also introduce a novel regularization to capture fine geometric details under sparse visual cues. Furthermore, we present a new multi-view dataset with varied lighting conditions to evaluate relight. Our framework enables realistic rendering of novel poses and views while supporting photo-realistic lighting effects under arbitrary lighting conditions. Our method achieves state-of-the-art performance in novel view synthesis, novel pose rendering, and relighting.",
    "arxiv_url": "https://arxiv.org/abs/2512.09335v2",
    "pdf_url": "https://arxiv.org/pdf/2512.09335v2",
    "published_date": "2025-12-10",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "dynamic",
      "high-fidelity",
      "body",
      "deformation",
      "avatar",
      "motion",
      "ar",
      "nerf",
      "relightable",
      "human",
      "3d gaussian",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.09335v2",
      "pdf": "https://arxiv.org/pdf/2512.09335v2"
    },
    "bibtex": ""
  },
  {
    "title": "MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification",
    "authors": [
      "Sangwoon Kwak",
      "Weeyoung Kwon",
      "Jun Young Jeong",
      "Geonho Kim",
      "Won-Sik Cheong",
      "Jihyong Oh"
    ],
    "abstract": "Recent advances in 4D Gaussian Splatting (4DGS) have extended the high-speed rendering capability of 3D Gaussian Splatting (3DGS) into the temporal domain, enabling real-time rendering of dynamic scenes. However, one of the major remaining challenges lies in modeling long-range motion-contained dynamic videos, where a naive extension of existing methods leads to severe memory explosion, temporal flickering, and failure to handle appearing or disappearing occlusions over time. To address these challenges, we propose a novel 4DGS framework characterized by an Anchor Relay-based Bidirectional Blending (ARBB) mechanism, named MoRel, which enables temporally consistent and memory-efficient modeling of long-range dynamic scenes. Our method progressively constructs locally canonical anchor spaces at key-frame time index and models inter-frame deformations at the anchor level, enhancing temporal coherence. By learning bidirectional deformations between KfA and adaptively blending them through learnable opacity control, our approach mitigates temporal discontinuities and flickering artifacts. We further introduce a Feature-variance-guided Hierarchical Densification (FHD) scheme that effectively densifies KfA's while keeping rendering quality, based on an assigned level of feature-variance. To effectively evaluate our model's capability to handle real-world long-range 4D motion, we newly compose long-range 4D motion-contained dataset, called SelfCap$_{\\text{LR}}$. It has larger average dynamic motion magnitude, captured at spatially wider spaces, compared to previous dynamic video datasets. Overall, our MoRel achieves temporally coherent and flicker-free long-range 4D reconstruction while maintaining bounded memory usage, demonstrating both scalability and efficiency in dynamic Gaussian-based representations.",
    "arxiv_url": "https://arxiv.org/abs/2512.09270v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09270v1",
    "published_date": "2025-12-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "deformation",
      "real-time rendering",
      "motion",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.09270v1",
      "pdf": "https://arxiv.org/pdf/2512.09270v1"
    },
    "bibtex": ""
  },
  {
    "title": "GTAvatar: Bridging Gaussian Splatting and Texture Mapping for Relightable and Editable Gaussian Avatars",
    "authors": [
      "Kelian Baert",
      "Mae Younes",
      "Francois Bourel",
      "Marc Christie",
      "Adnane Boukhayma"
    ],
    "abstract": "Recent advancements in Gaussian Splatting have enabled increasingly accurate reconstruction of photorealistic head avatars, opening the door to numerous applications in visual effects, videoconferencing, and virtual reality. This, however, comes with the lack of intuitive editability offered by traditional triangle mesh-based methods. In contrast, we propose a method that combines the accuracy and fidelity of 2D Gaussian Splatting with the intuitiveness of UV texture mapping. By embedding each canonical Gaussian primitive's local frame into a patch in the UV space of a template mesh in a computationally efficient manner, we reconstruct continuous editable material head textures from a single monocular video on a conventional UV domain. Furthermore, we leverage an efficient physically based reflectance model to enable relighting and editing of these intrinsic material maps. Through extensive comparisons with state-of-the-art methods, we demonstrate the accuracy of our reconstructions, the quality of our relighting results, and the ability to provide intuitive controls for modifying an avatar's appearance and geometry via texture mapping without additional optimization.",
    "arxiv_url": "https://arxiv.org/abs/2512.09162v1",
    "pdf_url": "https://arxiv.org/pdf/2512.09162v1",
    "published_date": "2025-12-09",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "head",
      "avatar",
      "geometry",
      "ar",
      "efficient",
      "mapping",
      "relightable",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.09162v1",
      "pdf": "https://arxiv.org/pdf/2512.09162v1"
    },
    "bibtex": ""
  },
  {
    "title": "OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics",
    "authors": [
      "Jisang Yoo",
      "Gyeongjin Kang",
      "Hyun-kyu Ko",
      "Hyeonwoo Yu",
      "Eunbyung Park"
    ],
    "abstract": "Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems. With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction. Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments. In this work, we present OpenMonoGS-SLAM, the first monocular SLAM framework that unifies 3D Gaussian Splatting (3DGS) with open-set semantic understanding. To achieve our goal, we leverage recent advances in Visual Foundation Models (VFMs), including MASt3R for visual geometry and SAM and CLIP for open-vocabulary semantics. These models provide robust generalization across diverse tasks, enabling accurate monocular camera tracking and mapping, as well as a rich understanding of semantics in open-world environments. Our method operates without any depth input or 3D semantic ground truth, relying solely on self-supervised learning objectives. Furthermore, we propose a memory mechanism specifically designed to manage high-dimensional semantic features, which effectively constructs Gaussian semantic feature maps, leading to strong overall performance. Experimental results demonstrate that our approach achieves performance comparable to or surpassing existing baselines in both closed-set and open-set segmentation tasks, all without relying on supplementary sensors such as depth maps or semantic annotations.",
    "arxiv_url": "https://arxiv.org/abs/2512.08625v1",
    "pdf_url": "https://arxiv.org/pdf/2512.08625v1",
    "published_date": "2025-12-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "understanding",
      "localization",
      "vr",
      "tracking",
      "geometry",
      "segmentation",
      "ar",
      "semantic",
      "3d gaussian",
      "slam",
      "mapping",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.08625v1",
      "pdf": "https://arxiv.org/pdf/2512.08625v1"
    },
    "bibtex": ""
  },
  {
    "title": "On-the-fly Large-scale 3D Reconstruction from Multi-Camera Rigs",
    "authors": [
      "Yijia Guo",
      "Tong Hu",
      "Zhiwei Li",
      "Liwen Hu",
      "Keming Qian",
      "Xitong Lin",
      "Shengbo Chen",
      "Tiejun Huang",
      "Lei Ma"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled efficient free-viewpoint rendering and photorealistic scene reconstruction. While on-the-fly extensions of 3DGS have shown promise for real-time reconstruction from monocular RGB streams, they often fail to achieve complete 3D coverage due to the limited field of view (FOV). Employing a multi-camera rig fundamentally addresses this limitation. In this paper, we present the first on-the-fly 3D reconstruction framework for multi-camera rigs. Our method incrementally fuses dense RGB streams from multiple overlapping cameras into a unified Gaussian representation, achieving drift-free trajectory estimation and efficient online reconstruction. We propose a hierarchical camera initialization scheme that enables coarse inter-camera alignment without calibration, followed by a lightweight multi-camera bundle adjustment that stabilizes trajectories while maintaining real-time performance. Furthermore, we introduce a redundancy-free Gaussian sampling strategy and a frequency-aware optimization scheduler to reduce the number of Gaussian primitives and the required optimization iterations, thereby maintaining both efficiency and reconstruction fidelity. Our method reconstructs hundreds of meters of 3D scenes within just 2 minutes using only raw multi-camera video streams, demonstrating unprecedented speed, robustness, and Fidelity for on-the-fly 3D scene reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2512.08498v1",
    "pdf_url": "https://arxiv.org/pdf/2512.08498v1",
    "published_date": "2025-12-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "3d reconstruction",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.08498v1",
      "pdf": "https://arxiv.org/pdf/2512.08498v1"
    },
    "bibtex": ""
  },
  {
    "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform",
    "authors": [
      "Yuning Gong",
      "Yifei Liu",
      "Yifan Zhan",
      "Muyao Niu",
      "Xueying Li",
      "Yuanjun Liao",
      "Jiaming Chen",
      "Yuanyuan Gao",
      "Jiaqi Chen",
      "Minming Chen",
      "Li Zhou",
      "Yuning Zhang",
      "Wei Wang",
      "Xiaoqing Hou",
      "Huaxi Huang",
      "Shixiang Tang",
      "Le Ma",
      "Dingwen Zhang",
      "Xue Yang",
      "Junchi Yan",
      "Yanchi Zhang",
      "Yinqiang Zheng",
      "Xiao Sun",
      "Zhihang Zhong"
    ],
    "abstract": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.",
    "arxiv_url": "https://arxiv.org/abs/2512.08478v1",
    "pdf_url": "https://arxiv.org/pdf/2512.08478v1",
    "published_date": "2025-12-09",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "lightweight",
      "avatar",
      "neural rendering",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.08478v1",
      "pdf": "https://arxiv.org/pdf/2512.08478v1"
    },
    "bibtex": ""
  },
  {
    "title": "HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting",
    "authors": [
      "Chang Liu",
      "Hongliang Yuan",
      "Lianghao Zhang",
      "Sichao Wang",
      "Jianwei Guo",
      "Shi-Sheng Huang"
    ],
    "abstract": "Rendering complex reflection of real-world scenes using 3D Gaussian splatting has been a quite promising solution for photorealistic novel view synthesis, but still faces bottlenecks especially in rendering speed and memory storage. This paper proposes a new Hybrid Splatting(HybridSplat) mechanism for Gaussian primitives. Our key idea is a new reflection-baked Gaussian tracing, which bakes the view-dependent reflection within each Gaussian primitive while rendering the reflection using tile-based Gaussian splatting. Then we integrate the reflective Gaussian primitives with base Gaussian primitives using a unified hybrid splatting framework for high-fidelity scene reconstruction. Moreover, we further introduce a pipeline-level acceleration for the hybrid splatting, and reflection-sensitive Gaussian pruning to reduce the model size, thus achieving much faster rendering speed and lower memory storage while preserving the reflection rendering quality. By extensive evaluation, our HybridSplat accelerates about 7x rendering speed across complex reflective scenes from Ref-NeRF, NeRF-Casting with 4x fewer Gaussian primitives than similar ray-tracing based Gaussian splatting baselines, serving as a new state-of-the-art method especially for complex reflective scenes.",
    "arxiv_url": "https://arxiv.org/abs/2512.08334v2",
    "pdf_url": "https://arxiv.org/pdf/2512.08334v2",
    "published_date": "2025-12-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "acceleration",
      "face",
      "ar",
      "nerf",
      "reflection",
      "3d gaussian",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.08334v2",
      "pdf": "https://arxiv.org/pdf/2512.08334v2"
    },
    "bibtex": ""
  },
  {
    "title": "Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation",
    "authors": [
      "Srijan Dokania",
      "Dharini Raghavan"
    ],
    "abstract": "We introduce Zero-Splat TeleAssist, a zero-shot sensor-fusion pipeline that transforms commodity CCTV streams into a shared, 6-DoF world model for multilateral teleoperation. By integrating vision-language segmentation, monocular depth, weighted-PCA pose extraction, and 3D Gaussian Splatting (3DGS), TeleAssist provides every operator with real-time global positions and orientations of multiple robots without fiducials or depth sensors in an interaction-centric teleoperation setup.",
    "arxiv_url": "https://arxiv.org/abs/2512.08271v1",
    "pdf_url": "https://arxiv.org/pdf/2512.08271v1",
    "published_date": "2025-12-09",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "ar",
      "semantic",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.08271v1",
      "pdf": "https://arxiv.org/pdf/2512.08271v1"
    },
    "bibtex": ""
  },
  {
    "title": "Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes",
    "authors": [
      "Shai Krakovsky",
      "Gal Fiebelman",
      "Sagie Benaim",
      "Hadar Averbuch-Elor"
    ],
    "abstract": "Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2512.07807v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07807v1",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "semantic",
      "human",
      "3d gaussian",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07807v1",
      "pdf": "https://arxiv.org/pdf/2512.07807v1"
    },
    "bibtex": ""
  },
  {
    "title": "Multi-view Pyramid Transformer: Look Coarser to See Broader",
    "authors": [
      "Gyeongjin Kang",
      "Seungkwon Yang",
      "Seungtae Nam",
      "Younggeun Lee",
      "Jungwoo Kim",
      "Eunbyung Park"
    ],
    "abstract": "We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details,\" MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.",
    "arxiv_url": "https://arxiv.org/abs/2512.07806v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07806v1",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "ar",
      "3d gaussian",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07806v1",
      "pdf": "https://arxiv.org/pdf/2512.07806v1"
    },
    "bibtex": ""
  },
  {
    "title": "From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images",
    "authors": [
      "Fei Yu",
      "Yu Liu",
      "Luyang Tang",
      "Mingchao Sun",
      "Zengye Ge",
      "Rui Bu",
      "Yuchao Jin",
      "Haisen Zhao",
      "He Sun",
      "Yangyan Li",
      "Mu Xu",
      "Wenzheng Chen",
      "Baoquan Chen"
    ],
    "abstract": "City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax. This requires inferring nearly $90^\\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail. To address this problem, we propose two design choices tailored for city structures and satellite inputs. First, we model city geometry as a 2.5D height map, implemented as a Z-monotonic signed distance field (SDF) that matches urban building layouts from top-down viewpoints. This stabilizes geometry optimization under sparse, off-nadir satellite views and yields a watertight mesh with crisp roofs and clean, vertically extruded facades. Second, we paint the mesh appearance from satellite images via differentiable rendering techniques. While the satellite inputs may contain long-range, blurry captures, we further train a generative texture restoration network to enhance the appearance, recovering high-frequency, plausible texture details from degraded inputs. Our method's scalability and robustness are demonstrated through extensive experiments on large-scale urban reconstruction. For example, in our teaser figure, we reconstruct a $4\\,\\mathrm{km}^2$ real-world region from only a few satellite images, achieving state-of-the-art performance in synthesizing photorealistic ground views. The resulting models are not only visually compelling but also serve as high-fidelity, application-ready assets for downstream tasks like urban planning and simulation. Project page can be found at https://pku-vcl-geometry.github.io/Orbit2Ground/.",
    "arxiv_url": "https://arxiv.org/abs/2512.07527v2",
    "pdf_url": "https://arxiv.org/pdf/2512.07527v2",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "3d reconstruction",
      "geometry",
      "ar",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07527v2",
      "pdf": "https://arxiv.org/pdf/2512.07527v2",
      "project": "https://pku-vcl-geometry.github.io/Orbit2Ground"
    },
    "bibtex": ""
  },
  {
    "title": "Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects",
    "authors": [
      "Shuohan Tao",
      "Boyao Zhou",
      "Hanzhang Tu",
      "Yuwang Wang",
      "Yebin Liu"
    ],
    "abstract": "3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.",
    "arxiv_url": "https://arxiv.org/abs/2512.07381v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07381v1",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "dynamic",
      "face",
      "deformation",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07381v1",
      "pdf": "https://arxiv.org/pdf/2512.07381v1"
    },
    "bibtex": ""
  },
  {
    "title": "Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting",
    "authors": [
      "Shilong Jin",
      "Haoran Duan",
      "Litao Hua",
      "Wentao Huang",
      "Yuan Zhou"
    ],
    "abstract": "Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.",
    "arxiv_url": "https://arxiv.org/abs/2512.07345v2",
    "pdf_url": "https://arxiv.org/pdf/2512.07345v2",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "semantic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07345v2",
      "pdf": "https://arxiv.org/pdf/2512.07345v2"
    },
    "bibtex": ""
  },
  {
    "title": "AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing",
    "authors": [
      "Ziming Hong",
      "Tianyu Huang",
      "Runnan Chen",
      "Shanshan Ye",
      "Mingming Gong",
      "Bo Han",
      "Tongliang Liu"
    ],
    "abstract": "Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.",
    "arxiv_url": "https://arxiv.org/abs/2512.07247v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07247v1",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07247v1",
      "pdf": "https://arxiv.org/pdf/2512.07247v1"
    },
    "bibtex": ""
  },
  {
    "title": "STRinGS: Selective Text Refinement in Gaussian Splatting",
    "authors": [
      "Abhinav Raundhal",
      "Gaurav Behera",
      "P J Narayanan",
      "Ravi Kiran Sarvadevabhatla",
      "Makarand Tapaswi"
    ],
    "abstract": "Text as signs, labels, or instructions is a critical element of real-world scenes as they can convey important contextual information. 3D representations such as 3D Gaussian Splatting (3DGS) struggle to preserve fine-grained text details, while achieving high visual fidelity. Small errors in textual element reconstruction can lead to significant semantic loss. We propose STRinGS, a text-aware, selective refinement framework to address this issue for 3DGS reconstruction. Our method treats text and non-text regions separately, refining text regions first and merging them with non-text regions later for full-scene optimization. STRinGS produces sharp, readable text even in challenging configurations. We introduce a text readability measure OCR Character Error Rate (CER) to evaluate the efficacy on text regions. STRinGS results in a 63.6% relative improvement over 3DGS at just 7K iterations. We also introduce a curated dataset STRinGS-360 with diverse text scenarios to evaluate text readability in 3D reconstruction. Our method and dataset together push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods.",
    "arxiv_url": "https://arxiv.org/abs/2512.07230v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07230v1",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "gaussian splatting",
      "ar",
      "semantic",
      "3d gaussian",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07230v1",
      "pdf": "https://arxiv.org/pdf/2512.07230v1"
    },
    "bibtex": ""
  },
  {
    "title": "SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting",
    "authors": [
      "Seokhyun Youn",
      "Soohyun Lee",
      "Geonho Kim",
      "Weeyoung Kwon",
      "Sung-Ho Bae",
      "Jihyong Oh"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.",
    "arxiv_url": "https://arxiv.org/abs/2512.07197v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07197v1",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "compact",
      "dynamic",
      "high-fidelity",
      "survey",
      "compression",
      "3d reconstruction",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07197v1",
      "pdf": "https://arxiv.org/pdf/2512.07197v1"
    },
    "bibtex": ""
  },
  {
    "title": "MuSASplat: Efficient Sparse-View 3D Gaussian Splats via Lightweight Multi-Scale Adaptation",
    "authors": [
      "Muyu Xu",
      "Fangneng Zhan",
      "Xiaoqin Zhang",
      "Ling Shao",
      "Shijian Lu"
    ],
    "abstract": "Sparse-view 3D Gaussian splatting seeks to render high-quality novel views of 3D scenes from a limited set of input images. While recent pose-free feed-forward methods leveraging pre-trained 3D priors have achieved impressive results, most of them rely on full fine-tuning of large Vision Transformer (ViT) backbones and incur substantial GPU costs. In this work, we introduce MuSASplat, a novel framework that dramatically reduces the computational burden of training pose-free feed-forward 3D Gaussian splats models with little compromise of rendering quality. Central to our approach is a lightweight Multi-Scale Adapter that enables efficient fine-tuning of ViT-based architectures with only a small fraction of training parameters. This design avoids the prohibitive GPU overhead associated with previous full-model adaptation techniques while maintaining high fidelity in novel view synthesis, even with very sparse input views. In addition, we introduce a Feature Fusion Aggregator that integrates features across input views effectively and efficiently. Unlike widely adopted memory banks, the Feature Fusion Aggregator ensures consistent geometric integration across input views and meanwhile mitigates the memory usage, training complexity, and computational costs significantly. Extensive experiments across diverse datasets show that MuSASplat achieves state-of-the-art rendering quality but has significantly reduced parameters and training resource requirements as compared with existing methods.",
    "arxiv_url": "https://arxiv.org/abs/2512.07165v1",
    "pdf_url": "https://arxiv.org/pdf/2512.07165v1",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "sparse-view",
      "lightweight",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07165v1",
      "pdf": "https://arxiv.org/pdf/2512.07165v1"
    },
    "bibtex": ""
  },
  {
    "title": "COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision",
    "authors": [
      "Jaeyoon Lee",
      "Hojoon Jung",
      "Sungtae Hwang",
      "Jihyong Oh",
      "Jongwon Choi"
    ],
    "abstract": "We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework.",
    "arxiv_url": "https://arxiv.org/abs/2512.07107v2",
    "pdf_url": "https://arxiv.org/pdf/2512.07107v2",
    "published_date": "2025-12-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "face",
      "geometry",
      "ar",
      "relightable",
      "3d gaussian",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07107v2",
      "pdf": "https://arxiv.org/pdf/2512.07107v2"
    },
    "bibtex": ""
  },
  {
    "title": "RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting",
    "authors": [
      "Hoang-Nhat Tran",
      "Francesco Di Sario",
      "Gabriele Spadaro",
      "Giuseppe Valenzise",
      "Enzo Tartaglione"
    ],
    "abstract": "Recent advances in neural scene representations have transformed immersive multimedia, with 3D Gaussian Splatting (3DGS) enabling real-time photorealistic rendering. Despite its efficiency, 3DGS suffers from large memory requirements and costly training procedures, motivating efforts toward compression. Existing approaches, however, operate at fixed rates, limiting adaptability to varying bandwidth and device constraints. In this work, we propose a flexible compression scheme for 3DGS that supports interpolation at any rate between predefined bounds. Our method is computationally lightweight, requires no retraining for any rate, and preserves rendering quality across a broad range of operating points. Experiments demonstrate that the approach achieves efficient, high-quality compression while offering dynamic rate control, making it suitable for practical deployment in immersive applications. The code is available at https://github.com/inspiros/RAVE.",
    "arxiv_url": "https://arxiv.org/abs/2512.07052v2",
    "pdf_url": "https://arxiv.org/pdf/2512.07052v2",
    "published_date": "2025-12-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/inspiros/RAVE",
    "keywords": [
      "dynamic",
      "lightweight",
      "compression",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.07052v2",
      "pdf": "https://arxiv.org/pdf/2512.07052v2",
      "github": "https://github.com/inspiros/RAVE"
    },
    "bibtex": ""
  },
  {
    "title": "MeshSplatting: Differentiable Rendering with Opaque Meshes",
    "authors": [
      "Jan Held",
      "Sanghyun Son",
      "Renaud Vandeghen",
      "Daniel Rebain",
      "Matheus Gadelha",
      "Yi Zhou",
      "Anthony Cioppa",
      "Ming C. Lin",
      "Marc Van Droogenbroeck",
      "Andrea Tagliasacchi"
    ],
    "abstract": "Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.",
    "arxiv_url": "https://arxiv.org/abs/2512.06818v1",
    "pdf_url": "https://arxiv.org/pdf/2512.06818v1",
    "published_date": "2025-12-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "face",
      "real-time rendering",
      "neural rendering",
      "geometry",
      "ar",
      "nerf",
      "3d gaussian",
      "efficient",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.06818v1",
      "pdf": "https://arxiv.org/pdf/2512.06818v1",
      "project": "https://meshsplatting.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting",
    "authors": [
      "Longjie Zhao",
      "Ziming Hong",
      "Zhenyang Ren",
      "Runnan Chen",
      "Mingming Gong",
      "Tongliang Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has enabled the creation of digital assets and downstream applications, underscoring the need for robust copyright protection via digital watermarking. However, existing 3DGS watermarking methods remain highly vulnerable to diffusion-based editing, which can easily erase embedded provenance. This challenge highlights the urgent need for 3DGS watermarking techniques that are intrinsically resilient to diffusion-based editing. In this paper, we introduce RDSplat, a Robust watermarking paradigm against Diffusion editing for 3D Gaussian Splatting. RDSplat embeds watermarks into 3DGS components that diffusion-based editing inherently preserve, achieved through (i) proactively targeting low-frequency Gaussians and (ii) adversarial training with a diffusion proxy. Specifically, we introduce a multi-domain framework that operates natively in 3DGS space and embeds watermarks into diffusion-editing-preserved low-frequency Gaussians via coordinated covariance regularization and 2D filtering. In addition, we exploit the low-pass filtering behavior of diffusion-based editing by using Gaussian blur as an efficient training surrogate, enabling adversarial fine-tuning that further enhances watermark robustness against diffusion-based editing. Empirically, comprehensive quantitative and qualitative evaluations on three benchmark datasets demonstrate that RDSplat not only maintains superior robustness under diffusion-based editing, but also preserves watermark invisibility, achieving state-of-the-art performance.",
    "arxiv_url": "https://arxiv.org/abs/2512.06774v1",
    "pdf_url": "https://arxiv.org/pdf/2512.06774v1",
    "published_date": "2025-12-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.06774v1",
      "pdf": "https://arxiv.org/pdf/2512.06774v1"
    },
    "bibtex": ""
  },
  {
    "title": "EMGauss: Continuous Slice-to-3D Reconstruction via Dynamic Gaussian Modeling in Volume Electron Microscopy",
    "authors": [
      "Yumeng He",
      "Zanwei Zhou",
      "Yekun Zheng",
      "Chen Liang",
      "Yunbo Wang",
      "Xiaokang Yang"
    ],
    "abstract": "Volume electron microscopy (vEM) enables nanoscale 3D imaging of biological structures but remains constrained by acquisition trade-offs, leading to anisotropic volumes with limited axial resolution. Existing deep learning methods seek to restore isotropy by leveraging lateral priors, yet their assumptions break down for morphologically anisotropic structures. We present EMGauss, a general framework for 3D reconstruction from planar scanned 2D slices with applications in vEM, which circumvents the inherent limitations of isotropy-based approaches. Our key innovation is to reframe slice-to-3D reconstruction as a 3D dynamic scene rendering problem based on Gaussian splatting, where the progression of axial slices is modeled as the temporal evolution of 2D Gaussian point clouds. To enhance fidelity in data-sparse regimes, we incorporate a Teacher-Student bootstrapping mechanism that uses high-confidence predictions on unobserved slices as pseudo-supervisory signals. Compared with diffusion- and GAN-based reconstruction methods, EMGauss substantially improves interpolation quality, enables continuous slice synthesis, and eliminates the need for large-scale pretraining. Beyond vEM, it potentially provides a generalizable slice-to-3D solution across diverse imaging domains.",
    "arxiv_url": "https://arxiv.org/abs/2512.06684v1",
    "pdf_url": "https://arxiv.org/pdf/2512.06684v1",
    "published_date": "2025-12-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.06684v1",
      "pdf": "https://arxiv.org/pdf/2512.06684v1"
    },
    "bibtex": ""
  },
  {
    "title": "AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars",
    "authors": [
      "Ramazan Fazylov",
      "Sergey Zagoruyko",
      "Aleksandr Parkin",
      "Stamatis Lefkimmiatis",
      "Ivan Laptev"
    ],
    "abstract": "The generation of high-fidelity, animatable 3D human avatars remains a core challenge in computer graphics and vision, with applications in VR, telepresence, and entertainment. Existing approaches based on implicit representations like NeRFs suffer from slow rendering and dynamic inconsistencies, while 3D Gaussian Splatting (3DGS) methods are typically limited to static head generation, lacking dynamic control. We bridge this gap by introducing AGORA, a novel framework that extends 3DGS within a generative adversarial network to produce animatable avatars. Our key contribution is a lightweight, FLAME-conditioned deformation branch that predicts per-Gaussian residuals, enabling identity-preserving, fine-grained expression control while allowing real-time inference. Expression fidelity is enforced via a dual-discriminator training scheme leveraging synthetic renderings of the parametric mesh. AGORA generates avatars that are not only visually realistic but also precisely controllable. Quantitatively, we outperform state-of-the-art NeRF-based methods on expression accuracy while rendering at 250+ FPS on a single GPU, and, notably, at $\\sim$9 FPS under CPU-only inference - representing, to our knowledge, the first demonstration of practical CPU-only animatable 3DGS avatar synthesis. This work represents a significant step toward practical, high-performance digital humans. Project website: https://ramazan793.github.io/AGORA/",
    "arxiv_url": "https://arxiv.org/abs/2512.06438v2",
    "pdf_url": "https://arxiv.org/pdf/2512.06438v2",
    "published_date": "2025-12-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "dynamic",
      "vr",
      "high-fidelity",
      "lightweight",
      "deformation",
      "avatar",
      "ar",
      "nerf",
      "human",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.06438v2",
      "pdf": "https://arxiv.org/pdf/2512.06438v2",
      "project": "https://ramazan793.github.io/AGORA"
    },
    "bibtex": ""
  },
  {
    "title": "TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting",
    "authors": [
      "Quan Tran",
      "Tuan Dang"
    ],
    "abstract": "3D Gaussian Splatting is crucial for real-time novel view synthesis due to its efficiency and ability to render photorealistic images. However, building a 3D Gaussian is guided solely by photometric loss, which can result in inconsistencies in reconstruction. This under-constrained process often results in \"floater\" artifacts and unstructured geometry, preventing the extraction of high-fidelity surfaces. To address this issue, our paper introduces a novel method that improves reconstruction by enforcing global geometry consistency through constrained multi-view triangulation. Our approach aims to achieve a consensus on 3D representation in the physical world by utilizing various estimated views. We optimize this process by penalizing the deviation of a rendered 3D point from a robust consensus point, which is re-triangulated from a bundle of neighboring views in a self-supervised fashion. We demonstrate the effectiveness of our method across multiple datasets, achieving state-of-the-art results. On the DTU dataset, our method attains a mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods. We will make our code open-source to facilitate community validation and ensure reproducibility.",
    "arxiv_url": "https://arxiv.org/abs/2512.06269v1",
    "pdf_url": "https://arxiv.org/pdf/2512.06269v1",
    "published_date": "2025-12-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "geometry",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.06269v1",
      "pdf": "https://arxiv.org/pdf/2512.06269v1"
    },
    "bibtex": ""
  },
  {
    "title": "Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation",
    "authors": [
      "Su Sun",
      "Cheng Zhao",
      "Himangi Mittal",
      "Gaurav Mittal",
      "Rohith Kukkala",
      "Yingjie Victor Chen",
      "Mei Chen"
    ],
    "abstract": "Generating dynamic 4D objects from sparse inputs is difficult because it demands joint preservation of appearance and motion coherence across views and time while suppressing artifacts and temporal drift. We hypothesize that the view discrepancy arises from supervision limited to pixel- or latent-space video-diffusion losses, which lack explicitly temporally aware, feature-level tracking guidance. We present \\emph{Track4DGen}, a two-stage framework that couples a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor. The central idea is to explicitly inject tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D-GS. In Stage One, we enforce dense, feature-level point correspondences inside the diffusion generator, producing temporally consistent features that curb appearance drift and enhance cross-view coherence. In Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion encoding that concatenates co-located diffusion features (carrying Stage-One tracking priors) with Hex-plane features, and augment them with 4D Spherical Harmonics for higher-fidelity dynamics modeling. \\emph{Track4DGen} surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. Lastly, we curate \\emph{Sketchfab28}, a high-quality dataset for benchmarking object-centric 4D generation and fostering future research.",
    "arxiv_url": "https://arxiv.org/abs/2512.06158v1",
    "pdf_url": "https://arxiv.org/pdf/2512.06158v1",
    "published_date": "2025-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "tracking",
      "animation",
      "gaussian splatting",
      "ar",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.06158v1",
      "pdf": "https://arxiv.org/pdf/2512.06158v1"
    },
    "bibtex": ""
  },
  {
    "title": "Curvature-Regularized Variational Autoencoder for 3D Scene Reconstruction from Sparse Depth",
    "authors": [
      "Maryam Yousefi",
      "Soodeh Bakhshandeh"
    ],
    "abstract": "When depth sensors provide only 5% of needed measurements, reconstructing complete 3D scenes becomes difficult. Autonomous vehicles and robots cannot tolerate the geometric errors that sparse reconstruction introduces. We propose curvature regularization through a discrete Laplacian operator, achieving 18.1% better reconstruction accuracy than standard variational autoencoders. Our contribution challenges an implicit assumption in geometric deep learning: that combining multiple geometric constraints improves performance. A single well-designed regularization term not only matches but exceeds the effectiveness of complex multi-term formulations. The discrete Laplacian offers stable gradients and noise suppression with just 15% training overhead and zero inference cost. Code and models are available at https://github.com/Maryousefi/GeoVAE-3D.",
    "arxiv_url": "https://arxiv.org/abs/2512.05783v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05783v1",
    "published_date": "2025-12-05",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "https://github.com/Maryousefi/GeoVAE-3D",
    "keywords": [
      "head",
      "ar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05783v1",
      "pdf": "https://arxiv.org/pdf/2512.05783v1",
      "github": "https://github.com/Maryousefi/GeoVAE-3D"
    },
    "bibtex": ""
  },
  {
    "title": "TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression",
    "authors": [
      "Cheng-Yuan Ho",
      "He-Bi Yang",
      "Jui-Chiu Chiang",
      "Yu-Lun Liu",
      "Wen-Hsiao Peng"
    ],
    "abstract": "Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenes, commonly referred to as 4DGS or dynamic 3DGS, has attracted increasing attention. However, designing more compact and efficient deformation schemes together with rate-distortion-optimized compression strategies for dynamic 3DGS representations remains an underexplored area. Prior methods either rely on space-time 4DGS with overspecified, short-lived Gaussian primitives or on canonical 3DGS with deformation that lacks explicit temporal control. To address this, we present TED-4DGS, a temporally activated and embedding-based deformation scheme for rate-distortion-optimized 4DGS compression that unifies the strengths of both families. TED-4DGS is built on a sparse anchor-based 3DGS representation. Each canonical anchor is assigned learnable temporal-activation parameters to specify its appearance and disappearance transitions over time, while a lightweight per-anchor temporal embedding queries a shared deformation bank to produce anchor-specific deformation. For rate-distortion compression, we incorporate an implicit neural representation (INR)-based hyperprior to model anchor attribute distributions, along with a channel-wise autoregressive model to capture intra-anchor correlations. With these novel elements, our scheme achieves state-of-the-art rate-distortion performance on several real-world datasets. To the best of our knowledge, this work represents one of the first attempts to pursue a rate-distortion-optimized compression framework for dynamic 3DGS representations.",
    "arxiv_url": "https://arxiv.org/abs/2512.05446v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05446v1",
    "published_date": "2025-12-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "compact",
      "dynamic",
      "lightweight",
      "deformation",
      "compression",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05446v1",
      "pdf": "https://arxiv.org/pdf/2512.05446v1"
    },
    "bibtex": ""
  },
  {
    "title": "SplatPainter: Interactive Authoring of 3D Gaussians from 2D Edits via Test-Time Training",
    "authors": [
      "Yang Zheng",
      "Hao Tan",
      "Kai Zhang",
      "Peng Wang",
      "Leonidas Guibas",
      "Gordon Wetzstein",
      "Wang Yifan"
    ],
    "abstract": "The rise of 3D Gaussian Splatting has revolutionized photorealistic 3D asset creation, yet a critical gap remains for their interactive refinement and editing. Existing approaches based on diffusion or optimization are ill-suited for this task, as they are often prohibitively slow, destructive to the original asset's identity, or lack the precision for fine-grained control. To address this, we introduce \\ourmethod, a state-aware feedforward model that enables continuous editing of 3D Gaussian assets from user-provided 2D view(s). Our method directly predicts updates to the attributes of a compact, feature-rich Gaussian representation and leverages Test-Time Training to create a state-aware, iterative workflow. The versatility of our approach allows a single architecture to perform diverse tasks, including high-fidelity local detail refinement, local paint-over, and consistent global recoloring, all at interactive speeds, paving the way for fluid and intuitive 3D content authoring.",
    "arxiv_url": "https://arxiv.org/abs/2512.05354v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05354v1",
    "published_date": "2025-12-05",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "high-fidelity",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05354v1",
      "pdf": "https://arxiv.org/pdf/2512.05354v1"
    },
    "bibtex": ""
  },
  {
    "title": "Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting",
    "authors": [
      "Hao-Jen Chien",
      "Yi-Chuan Huang",
      "Chung-Ho Wu",
      "Wei-Lun Chao",
      "Yu-Lun Liu"
    ],
    "abstract": "Synthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model's time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: https://chien90190.github.io/splannequin/",
    "arxiv_url": "https://arxiv.org/abs/2512.05113v2",
    "pdf_url": "https://arxiv.org/pdf/2512.05113v2",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "dynamic",
      "high-fidelity",
      "gaussian splatting",
      "ar",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05113v2",
      "pdf": "https://arxiv.org/pdf/2512.05113v2",
      "project": "https://chien90190.github.io/splannequin"
    },
    "bibtex": ""
  },
  {
    "title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
    "authors": [
      "Xianfeng Wu",
      "Yajing Bai",
      "Minghan Li",
      "Xianzu Wu",
      "Xueqi Zhao",
      "Zhongyuan Lai",
      "Wenyu Liu",
      "Xinggang Wang"
    ],
    "abstract": "Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT",
    "arxiv_url": "https://arxiv.org/abs/2512.05060v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05060v1",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/hustvl/4DLangVGGT",
    "keywords": [
      "4d",
      "understanding",
      "dynamic",
      "geometry",
      "ar",
      "semantic",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05060v1",
      "pdf": "https://arxiv.org/pdf/2512.05060v1",
      "github": "https://github.com/hustvl/4DLangVGGT"
    },
    "bibtex": ""
  },
  {
    "title": "RobustSplat++: Decoupling Densification, Dynamics, and Illumination for In-the-Wild 3DGS",
    "authors": [
      "Chuanyu Fu",
      "Guanying Chen",
      "Yuqi Zhang",
      "Kunbin Yao",
      "Yuan Xiong",
      "Chuan Huang",
      "Shuguang Cui",
      "Yasuyuki Matsushita",
      "Xiaochun Cao"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling in-the-wild scenes affected by transient objects and illuminations, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances and illumination variations. To address this, we propose RobustSplat++, a robust solution based on several critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Third, we incorporate the delayed Gaussian growth strategy and mask bootstrapping with appearance modeling to handling in-the-wild scenes including transients and illuminations. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method.",
    "arxiv_url": "https://arxiv.org/abs/2512.04815v1",
    "pdf_url": "https://arxiv.org/pdf/2512.04815v1",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "dynamic",
      "ar",
      "semantic",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.04815v1",
      "pdf": "https://arxiv.org/pdf/2512.04815v1"
    },
    "bibtex": ""
  },
  {
    "title": "Bridging Simulation and Reality: Cross-Domain Transfer with Semantic 2D Gaussian Splatting",
    "authors": [
      "Jian Tang",
      "Pu Pang",
      "Haowen Sun",
      "Chengzhong Ma",
      "Xingyu Chen",
      "Hua Huang",
      "Xuguang Lan"
    ],
    "abstract": "Cross-domain transfer in robotic manipulation remains a longstanding challenge due to the significant domain gap between simulated and real-world environments. Existing methods such as domain randomization, adaptation, and sim-real calibration often require extensive tuning or fail to generalize to unseen scenarios. To address this issue, we observe that if domain-invariant features are utilized during policy training in simulation, and the same features can be extracted and provided as the input to policy during real-world deployment, the domain gap can be effectively bridged, leading to significantly improved policy generalization. Accordingly, we propose Semantic 2D Gaussian Splatting (S2GS), a novel representation method that extracts object-centric, domain-invariant spatial features. S2GS constructs multi-view 2D semantic fields and projects them into a unified 3D space via feature-level Gaussian splatting. A semantic filtering mechanism removes irrelevant background content, ensuring clean and consistent inputs for policy learning. To evaluate the effectiveness of S2GS, we adopt Diffusion Policy as the downstream learning algorithm and conduct experiments in the ManiSkill simulation environment, followed by real-world deployment. Results demonstrate that S2GS significantly improves sim-to-real transferability, maintaining high and stable task performance in real-world scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2512.04731v1",
    "pdf_url": "https://arxiv.org/pdf/2512.04731v1",
    "published_date": "2025-12-04",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "semantic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.04731v1",
      "pdf": "https://arxiv.org/pdf/2512.04731v1"
    },
    "bibtex": ""
  },
  {
    "title": "Gaussian Entropy Fields: Driving Adaptive Sparsity in 3D Gaussian Optimization",
    "authors": [
      "Hong Kuang",
      "Jianchen Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a leading technique for novel view synthesis, demonstrating exceptional rendering efficiency. \\replaced[]{Well-reconstructed surfaces can be characterized by low configurational entropy, where dominant primitives clearly define surface geometry while redundant components are suppressed.}{The key insight is that well-reconstructed surfaces naturally exhibit low configurational entropy, where dominant primitives clearly define surface geometry while suppressing redundant components.} Three complementary technical contributions are introduced: (1) entropy-driven surface modeling via entropy minimization for low configurational entropy in primitive distributions; (2) adaptive spatial regularization using the Surface Neighborhood Redundancy Index (SNRI) and image entropy-guided weighting; (3) multi-scale geometric preservation through competitive cross-scale entropy alignment. Extensive experiments demonstrate that GEF achieves competitive geometric precision on DTU and T\\&T benchmarks, while delivering superior rendering quality compared to existing methods on Mip-NeRF 360. Notably, superior Chamfer Distance (0.64) on DTU and F1 score (0.44) on T\\&T are obtained, alongside the best SSIM (0.855) and LPIPS (0.136) among baselines on Mip-NeRF 360, validating the framework's ability to enhance surface reconstruction accuracy without compromising photometric fidelity.",
    "arxiv_url": "https://arxiv.org/abs/2512.04542v1",
    "pdf_url": "https://arxiv.org/pdf/2512.04542v1",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "ar",
      "nerf",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.04542v1",
      "pdf": "https://arxiv.org/pdf/2512.04542v1"
    },
    "bibtex": ""
  },
  {
    "title": "UTrice: Unifying Primitives in Differentiable Ray Tracing and Rasterization via Triangles for Particle-Based 3D Scenes",
    "authors": [
      "Changhe Liu",
      "Ehsan Javanmardi",
      "Naren Bao",
      "Alex Orsholits",
      "Manabu Tsukada"
    ],
    "abstract": "Ray tracing 3D Gaussian particles enables realistic effects such as depth of field, refractions, and flexible camera modeling for novel-view synthesis. However, existing methods trace Gaussians through proxy geometry, which requires constructing complex intermediate meshes and performing costly intersection tests. This limitation arises because Gaussian-based particles are not well suited as unified primitives for both ray tracing and rasterization. In this work, we propose a differentiable triangle-based ray tracing pipeline that directly treats triangles as rendering primitives without relying on any proxy geometry. Our results show that the proposed method achieves significantly higher rendering quality than existing ray tracing approaches while maintaining real-time rendering performance. Moreover, our pipeline can directly render triangles optimized by the rasterization-based method Triangle Splatting, thus unifying the primitives used in novel-view synthesis.",
    "arxiv_url": "https://arxiv.org/abs/2512.04421v1",
    "pdf_url": "https://arxiv.org/pdf/2512.04421v1",
    "published_date": "2025-12-04",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ray tracing",
      "real-time rendering",
      "geometry",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.04421v1",
      "pdf": "https://arxiv.org/pdf/2512.04421v1"
    },
    "bibtex": ""
  },
  {
    "title": "SyncTrack4D: Cross-Video Motion Alignment and Video Synchronization for Multi-Video 4D Gaussian Splatting",
    "authors": [
      "Yonghan Lee",
      "Tsung-Wei Huang",
      "Shiv Gehlot",
      "Jaehoon Choi",
      "Guan-Ming Su",
      "Dinesh Manocha"
    ],
    "abstract": "Modeling dynamic 3D scenes is challenging due to their high-dimensional nature, which requires aggregating information from multiple views to reconstruct time-evolving 3D geometry and motion. We present a novel multi-video 4D Gaussian Splatting (4DGS) approach designed to handle real-world, unsynchronized video sets. Our approach, SyncTrack4D, directly leverages dense 4D track representation of dynamic scene parts as cues for simultaneous cross-video synchronization and 4DGS reconstruction. We first compute dense per-video 4D feature tracks and cross-video track correspondences by Fused Gromov-Wasserstein optimal transport approach. Next, we perform global frame-level temporal alignment to maximize overlapping motion of matched 4D tracks. Finally, we achieve sub-frame synchronization through our multi-video 4D Gaussian splatting built upon a motion-spline scaffold representation. The final output is a synchronized 4DGS representation with dense, explicit 3D trajectories, and temporal offsets for each video. We evaluate our approach on the Panoptic Studio and SyncNeRF Blender, demonstrating sub-frame synchronization accuracy with an average temporal error below 0.26 frames, and high-fidelity 4D reconstruction reaching 26.3 PSNR scores on the Panoptic Studio dataset. To the best of our knowledge, our work is the first general 4D Gaussian Splatting approach for unsynchronized video sets, without assuming the existence of predefined scene objects or prior models.",
    "arxiv_url": "https://arxiv.org/abs/2512.04315v1",
    "pdf_url": "https://arxiv.org/pdf/2512.04315v1",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "high-fidelity",
      "motion",
      "geometry",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.04315v1",
      "pdf": "https://arxiv.org/pdf/2512.04315v1"
    },
    "bibtex": ""
  },
  {
    "title": "Mind-to-Face: Neural-Driven Photorealistic Avatar Synthesis via EEG Decoding",
    "authors": [
      "Haolin Xiong",
      "Tianwen Fu",
      "Pratusha Bhuvana Prasad",
      "Yunxuan Cai",
      "Haiwei Chen",
      "Wenbin Teng",
      "Hanyuan Xiao",
      "Yajie Zhao"
    ],
    "abstract": "Current expressive avatar systems rely heavily on visual cues, failing when faces are occluded or when emotions remain internal. We present Mind-to-Face, the first framework that decodes non-invasive electroencephalogram (EEG) signals directly into high-fidelity facial expressions. We build a dual-modality recording setup to obtain synchronized EEG and multi-view facial video during emotion-eliciting stimuli, enabling precise supervision for neural-to-visual learning. Our model uses a CNN-Transformer encoder to map EEG signals into dense 3D position maps, capable of sampling over 65k vertices, capturing fine-scale geometry and subtle emotional dynamics, and renders them through a modified 3D Gaussian Splatting pipeline for photorealistic, view-consistent results. Through extensive evaluation, we show that EEG alone can reliably predict dynamic, subject-specific facial expressions, including subtle emotional responses, demonstrating that neural signals contain far richer affective and geometric information than previously assumed. Mind-to-Face establishes a new paradigm for neural-driven avatars, enabling personalized, emotion-aware telepresence and cognitive interaction in immersive environments.",
    "arxiv_url": "https://arxiv.org/abs/2512.04313v1",
    "pdf_url": "https://arxiv.org/pdf/2512.04313v1",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "high-fidelity",
      "face",
      "avatar",
      "motion",
      "geometry",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.04313v1",
      "pdf": "https://arxiv.org/pdf/2512.04313v1"
    },
    "bibtex": ""
  },
  {
    "title": "C3G: Learning Compact 3D Representations with 2K Gaussians",
    "authors": [
      "Honggyu An",
      "Jaewoo Jung",
      "Mungyeom Kim",
      "Sunghwan Hong",
      "Chaehyun Kim",
      "Kazumi Fukuda",
      "Minkyeong Jeon",
      "Jisang Han",
      "Takuya Narihira",
      "Hyuna Ko",
      "Junsu Kim",
      "Yuki Mitsufuji",
      "Seungryong Kim"
    ],
    "abstract": "Reconstructing and understanding 3D scenes from unposed sparse views in a feed-forward manner remains as a challenging task in 3D computer vision. Recent approaches use per-pixel 3D Gaussian Splatting for reconstruction, followed by a 2D-to-3D feature lifting stage for scene understanding. However, they generate excessive redundant Gaussians, causing high memory overhead and sub-optimal multi-view feature aggregation, leading to degraded novel view synthesis and scene understanding performance. We propose C3G, a novel feed-forward framework that estimates compact 3D Gaussians only at essential spatial locations, minimizing redundancy while enabling effective feature lifting. We introduce learnable tokens that aggregate multi-view features through self-attention to guide Gaussian generation, ensuring each Gaussian integrates relevant visual features across views. We then exploit the learned attention patterns for Gaussian decoding to efficiently lift features. Extensive experiments on pose-free novel view synthesis, 3D open-vocabulary segmentation, and view-invariant feature aggregation demonstrate our approach's effectiveness. Results show that a compact yet geometrically meaningful representation is sufficient for high-quality scene reconstruction and understanding, achieving superior memory efficiency and feature fidelity compared to existing methods.",
    "arxiv_url": "https://arxiv.org/abs/2512.04021v1",
    "pdf_url": "https://arxiv.org/pdf/2512.04021v1",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "compact",
      "understanding",
      "sparse view",
      "segmentation",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.04021v1",
      "pdf": "https://arxiv.org/pdf/2512.04021v1"
    },
    "bibtex": ""
  },
  {
    "title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
    "authors": [
      "Melis Ocal",
      "Xiaoyan Xing",
      "Yue Li",
      "Ngo Anh Vien",
      "Sezer Karaoglu",
      "Theo Gevers"
    ],
    "abstract": "3D stylization is central to game development, virtual reality, and digital arts, where the demand for diverse assets calls for scalable methods that support fast, high-fidelity manipulation. Existing text-to-3D stylization methods typically distill from 2D image editors, requiring time-intensive per-asset optimization and exhibiting multi-view inconsistency due to the limitations of current text-to-image models, which makes them impractical for large-scale production. In this paper, we introduce GaussianBlender, a pioneering feed-forward framework for text-driven 3D stylization that performs edits instantly at inference. Our method learns structured, disentangled latent spaces with controlled information sharing for geometry and appearance from spatially-grouped 3D Gaussians. A latent diffusion model then applies text-conditioned edits on these learned representations. Comprehensive evaluations show that GaussianBlender not only delivers instant, high-fidelity, geometry-preserving, multi-view consistent stylization, but also surpasses methods that require per-instance test-time optimization - unlocking practical, democratized 3D stylization at scale.",
    "arxiv_url": "https://arxiv.org/abs/2512.03683v1",
    "pdf_url": "https://arxiv.org/pdf/2512.03683v1",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "geometry",
      "ar",
      "3d gaussian",
      "fast"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.03683v1",
      "pdf": "https://arxiv.org/pdf/2512.03683v1"
    },
    "bibtex": ""
  },
  {
    "title": "ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation",
    "authors": [
      "Yaokun Li",
      "Shuaixian Wang",
      "Mantang Guo",
      "Jiehui Huang",
      "Taojun Ding",
      "Mu Hu",
      "Kaixuan Wang",
      "Shaojie Shen",
      "Guang Tan"
    ],
    "abstract": "We propose ReCamDriving, a purely vision-based, camera-controlled novel-trajectory video generation framework. While repair-based methods fail to restore complex artifacts and LiDAR-based approaches rely on sparse and incomplete cues, ReCamDriving leverages dense and scene-complete 3DGS renderings for explicit geometric guidance, achieving precise camera-controllable generation. To mitigate overfitting to restoration behaviors when conditioned on 3DGS renderings, ReCamDriving adopts a two-stage training paradigm: the first stage uses camera poses for coarse control, while the second stage incorporates 3DGS renderings for fine-grained viewpoint and geometric guidance. Furthermore, we present a 3DGS-based cross-trajectory data curation strategy to eliminate the train-test gap in camera transformation patterns, enabling scalable multi-trajectory supervision from monocular videos. Based on this strategy, we construct the ParaDrive dataset, containing over 110K parallel-trajectory video pairs. Extensive experiments demonstrate that ReCamDriving achieves state-of-the-art camera controllability and structural consistency.",
    "arxiv_url": "https://arxiv.org/abs/2512.03621v2",
    "pdf_url": "https://arxiv.org/pdf/2512.03621v2",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.03621v2",
      "pdf": "https://arxiv.org/pdf/2512.03621v2"
    },
    "bibtex": ""
  },
  {
    "title": "Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding",
    "authors": [
      "Haoran Zhou",
      "Gim Hee Lee"
    ],
    "abstract": "Recent advancements in foundation models for 2D vision have substantially improved the analysis of dynamic scenes from monocular videos. However, despite their strong generalization capabilities, these models often lack 3D consistency, a fundamental requirement for understanding scene geometry and motion, thereby causing severe spatial misalignment and temporal flickering in complex 3D environments. In this paper, we present Motion4D, a novel framework that addresses these challenges by integrating 2D priors from foundation models into a unified 4D Gaussian Splatting representation. Our method features a two-part iterative optimization framework: 1) Sequential optimization, which updates motion and semantic fields in consecutive stages to maintain local consistency, and 2) Global optimization, which jointly refines all attributes for long-term coherence. To enhance motion accuracy, we introduce a 3D confidence map that dynamically adjusts the motion priors, and an adaptive resampling process that inserts new Gaussians into under-represented regions based on per-pixel RGB and semantic errors. Furthermore, we enhance semantic coherence through an iterative refinement process that resolves semantic inconsistencies by alternately optimizing the semantic fields and updating prompts of SAM2. Extensive evaluations demonstrate that our Motion4D significantly outperforms both 2D foundation models and existing 3D-based approaches across diverse scene understanding tasks, including point-based tracking, video object segmentation, and novel view synthesis. Our code is available at https://hrzhou2.github.io/motion4d-web/.",
    "arxiv_url": "https://arxiv.org/abs/2512.03601v1",
    "pdf_url": "https://arxiv.org/pdf/2512.03601v1",
    "published_date": "2025-12-03",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "understanding",
      "dynamic",
      "tracking",
      "geometry",
      "gaussian splatting",
      "segmentation",
      "semantic",
      "ar",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.03601v1",
      "pdf": "https://arxiv.org/pdf/2512.03601v1",
      "project": "https://hrzhou2.github.io/motion4d-web"
    },
    "bibtex": ""
  },
  {
    "title": "What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models",
    "authors": [
      "Tianchen Deng",
      "Yue Pan",
      "Shenghai Yuan",
      "Dong Li",
      "Chen Wang",
      "Mingrui Li",
      "Long Chen",
      "Lihua Xie",
      "Danwei Wang",
      "Jingchuan Wang",
      "Javier Civera",
      "Hesheng Wang",
      "Weidong Chen"
    ],
    "abstract": "In this paper, we provide a comprehensive overview of existing scene representation methods for robotics, covering traditional representations such as point clouds, voxels, signed distance functions (SDF), and scene graphs, as well as more recent neural representations like Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and the emerging Foundation Models. While current SLAM and localization systems predominantly rely on sparse representations like point clouds and voxels, dense scene representations are expected to play a critical role in downstream tasks such as navigation and obstacle avoidance. Moreover, neural representations such as NeRF, 3DGS, and foundation models are well-suited for integrating high-level semantic features and language-based priors, enabling more comprehensive 3D scene understanding and embodied intelligence. In this paper, we categorized the core modules of robotics into five parts (Perception, Mapping, Localization, Navigation, Manipulation). We start by presenting the standard formulation of different scene representation methods and comparing the advantages and disadvantages of scene representation across different modules. This survey is centered around the question: What is the best 3D scene representation for robotics? We then discuss the future development trends of 3D scene representations, with a particular focus on how the 3D Foundation Model could replace current methods as the unified solution for future robotic applications. The remaining challenges in fully realizing this model are also explored. We aim to offer a valuable resource for both newcomers and experienced researchers to explore the future of 3D scene representations and their application in robotics. We have published an open-source project on GitHub and will continue to add new works and technologies to this project.",
    "arxiv_url": "https://arxiv.org/abs/2512.03422v2",
    "pdf_url": "https://arxiv.org/pdf/2512.03422v2",
    "published_date": "2025-12-03",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "understanding",
      "localization",
      "survey",
      "ar",
      "semantic",
      "nerf",
      "3d gaussian",
      "slam",
      "mapping",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.03422v2",
      "pdf": "https://arxiv.org/pdf/2512.03422v2"
    },
    "bibtex": ""
  },
  {
    "title": "Flux4D: Flow-based Unsupervised 4D Reconstruction",
    "authors": [
      "Jingkang Wang",
      "Henry Che",
      "Yun Chen",
      "Ze Yang",
      "Lily Goli",
      "Sivabalan Manivasagam",
      "Raquel Urtasun"
    ],
    "abstract": "Reconstructing large-scale dynamic scenes from visual observations is a fundamental challenge in computer vision, with critical implications for robotics and autonomous systems. While recent differentiable rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved impressive photorealistic reconstruction, they suffer from scalability limitations and require annotations to decouple actor motion. Existing self-supervised methods attempt to eliminate explicit annotations by leveraging motion cues and geometric priors, yet they remain constrained by per-scene optimization and sensitivity to hyperparameter tuning. In this paper, we introduce Flux4D, a simple and scalable framework for 4D reconstruction of large-scale dynamic scenes. Flux4D directly predicts 3D Gaussians and their motion dynamics to reconstruct sensor observations in a fully unsupervised manner. By adopting only photometric losses and enforcing an \"as static as possible\" regularization, Flux4D learns to decompose dynamic elements directly from raw data without requiring pre-trained supervised models or foundational priors simply by training across many scenes. Our approach enables efficient reconstruction of dynamic scenes within seconds, scales effectively to large datasets, and generalizes well to unseen environments, including rare and unknown objects. Experiments on outdoor driving datasets show Flux4D significantly outperforms existing methods in scalability, generalization, and reconstruction quality.",
    "arxiv_url": "https://arxiv.org/abs/2512.03210v1",
    "pdf_url": "https://arxiv.org/pdf/2512.03210v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "outdoor",
      "robotics",
      "dynamic",
      "motion",
      "ar",
      "nerf",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.03210v1",
      "pdf": "https://arxiv.org/pdf/2512.03210v1"
    },
    "bibtex": ""
  },
  {
    "title": "DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images",
    "authors": [
      "Xiaoxue Chen",
      "Ziyi Xiong",
      "Yuantao Chen",
      "Gen Li",
      "Nan Wang",
      "Hongcheng Luo",
      "Long Chen",
      "Haiyang Sun",
      "Bing Wang",
      "Guang Chen",
      "Hangjun Ye",
      "Hongyang Li",
      "Ya-Qin Zhang",
      "Hao Zhao"
    ],
    "abstract": "Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \\textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.",
    "arxiv_url": "https://arxiv.org/abs/2512.03004v1",
    "pdf_url": "https://arxiv.org/pdf/2512.03004v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "head",
      "dynamic",
      "lightweight",
      "autonomous driving",
      "ar",
      "3d gaussian",
      "fast",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.03004v1",
      "pdf": "https://arxiv.org/pdf/2512.03004v1"
    },
    "bibtex": ""
  },
  {
    "title": "A Fast Volumetric Capture and Reconstruction Pipeline for Dynamic Point Clouds and Gaussian Splats",
    "authors": [
      "Athanasios Charisoudis",
      "Simone Croci",
      "Lam Kit Yung",
      "Pascal Frossard",
      "Aljosa Smolic"
    ],
    "abstract": "We present a fast and efficient volumetric capture and reconstruction system that processes either RGB-D or RGB-only input to generate 3D representations in the form of point clouds and Gaussian splats. For Gaussian splat reconstructions, we took the GPS-Gaussian regressor and improved it, enabling high-quality reconstructions with minimal overhead. The system is designed for easy setup and deployment, supporting in-the-wild operation under uncontrolled illumination and arbitrary backgrounds, as well as flexible camera configurations, including sparse setups, arbitrary camera numbers and baselines. Captured data can be exported in standard formats such as PLY, MPEG V-PCC, and SPLAT, and visualized through a web-based viewer or Unity/Unreal plugins. A live on-location preview of both input and reconstruction is available at 5-10 FPS. We present qualitative findings focused on deployability and targeted ablations. The complete framework is open-source, facilitating reproducibility and further research.",
    "arxiv_url": "https://arxiv.org/abs/2512.15719v1",
    "pdf_url": "https://arxiv.org/pdf/2512.15719v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.GR",
      "eess.IV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "illumination",
      "dynamic",
      "ar",
      "efficient",
      "fast"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.15719v1",
      "pdf": "https://arxiv.org/pdf/2512.15719v1"
    },
    "bibtex": ""
  },
  {
    "title": "EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis",
    "authors": [
      "Yancheng Zhang",
      "Guangyu Sun",
      "Chen Chen"
    ],
    "abstract": "Novel view synthesis (NVS) is crucial in computer vision and graphics, with wide applications in AR, VR, and autonomous driving. While 3D Gaussian Splatting (3DGS) enables real-time rendering with high appearance fidelity, it suffers from multi-view inconsistencies, limiting geometric accuracy. In contrast, 2D Gaussian Splatting (2DGS) enforces multi-view consistency but compromises texture details. To address these limitations, we propose Exchangeable Gaussian Splatting (EGGS), a hybrid representation that integrates 2D and 3D Gaussians to balance appearance and geometry. To achieve this, we introduce Hybrid Gaussian Rasterization for unified rendering, Adaptive Type Exchange for dynamic adaptation between 2D and 3D Gaussians, and Frequency-Decoupled Optimization that effectively exploits the strengths of each type of Gaussian representation. Our CUDA-accelerated implementation ensures efficient training and inference. Extensive experiments demonstrate that EGGS outperforms existing methods in rendering quality, geometric accuracy, and efficiency, providing a practical solution for high-quality NVS.",
    "arxiv_url": "https://arxiv.org/abs/2512.02932v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02932v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "vr",
      "real-time rendering",
      "geometry",
      "autonomous driving",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02932v1",
      "pdf": "https://arxiv.org/pdf/2512.02932v1"
    },
    "bibtex": ""
  },
  {
    "title": "PolarGuide-GSDR: 3D Gaussian Splatting Driven by Polarization Priors and Deferred Reflection for Real-World Reflective Scenes",
    "authors": [
      "Derui Shan",
      "Qian Qiao",
      "Hao Lu",
      "Tao Du",
      "Peng Lu"
    ],
    "abstract": "Polarization-aware Neural Radiance Fields (NeRF) enable novel view synthesis of specular-reflection scenes but face challenges in slow training, inefficient rendering, and strong dependencies on material/viewpoint assumptions. However, 3D Gaussian Splatting (3DGS) enables real-time rendering yet struggles with accurate reflection reconstruction from reflection-geometry entanglement, adding a deferred reflection module introduces environment map dependence. We address these limitations by proposing PolarGuide-GSDR, a polarization-forward-guided paradigm establishing a bidirectional coupling mechanism between polarization and 3DGS: first 3DGS's geometric priors are leveraged to resolve polarization ambiguity, and then the refined polarization information cues are used to guide 3DGS's normal and spherical harmonic representation. This process achieves high-fidelity reflection separation and full-scene reconstruction without requiring environment maps or restrictive material assumptions. We demonstrate on public and self-collected datasets that PolarGuide-GSDR achieves state-of-the-art performance in specular reconstruction, normal estimation, and novel view synthesis, all while maintaining real-time rendering capabilities. To our knowledge, this is the first framework embedding polarization priors directly into 3DGS optimization, yielding superior interpretability and real-time performance for modeling complex reflective scenes.",
    "arxiv_url": "https://arxiv.org/abs/2512.02664v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02664v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "real-time rendering",
      "geometry",
      "ar",
      "efficient rendering",
      "nerf",
      "reflection",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02664v1",
      "pdf": "https://arxiv.org/pdf/2512.02664v1"
    },
    "bibtex": ""
  },
  {
    "title": "PoreTrack3D: A Benchmark for Dynamic 3D Gaussian Splatting in Pore-Scale Facial Trajectory Tracking",
    "authors": [
      "Dong Li",
      "Jiahao Xiong",
      "Yingda Huang",
      "Le Chang"
    ],
    "abstract": "We introduce PoreTrack3D, the first benchmark for dynamic 3D Gaussian splatting in pore-scale, non-rigid 3D facial trajectory tracking. It contains over 440,000 facial trajectories in total, among which more than 52,000 are longer than 10 frames, including 68 manually reviewed trajectories that span the entire 150 frames. To the best of our knowledge, PoreTrack3D is the first benchmark dataset to capture both traditional facial landmarks and pore-scale keypoints trajectory, advancing the study of fine-grained facial expressions through the analysis of subtle skin-surface motion. We systematically evaluate state-of-the-art dynamic 3D Gaussian splatting methods on PoreTrack3D, establishing the first performance baseline in this domain. Overall, the pipeline developed for this benchmark dataset's creation establishes a new framework for high-fidelity facial motion capture and dynamic 3D reconstruction. Our dataset are publicly available at: https://github.com/JHXion9/PoreTrack3D",
    "arxiv_url": "https://arxiv.org/abs/2512.02648v2",
    "pdf_url": "https://arxiv.org/pdf/2512.02648v2",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/JHXion9/PoreTrack3D",
    "keywords": [
      "dynamic",
      "high-fidelity",
      "face",
      "tracking",
      "motion",
      "3d reconstruction",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02648v2",
      "pdf": "https://arxiv.org/pdf/2512.02648v2",
      "github": "https://github.com/JHXion9/PoreTrack3D"
    },
    "bibtex": ""
  },
  {
    "title": "Content-Aware Texturing for Gaussian Splatting",
    "authors": [
      "Panagiotis Papantonakis",
      "Georgios Kopanas",
      "Fredo Durand",
      "George Drettakis"
    ],
    "abstract": "Gaussian Splatting has become the method of choice for 3D reconstruction and real-time rendering of captured real scenes. However, fine appearance details need to be represented as a large number of small Gaussian primitives, which can be wasteful when geometry and appearance exhibit different frequency characteristics.   Inspired by the long tradition of texture mapping, we propose to use texture to represent detailed appearance where possible. Our main focus is to incorporate per-primitive texture maps that adapt to the scene in a principled manner during Gaussian Splatting optimization. We do this by proposing a new appearance representation for 2D Gaussian primitives with textures where the size of a texel is bounded by the image sampling frequency and adapted to the content of the input images. We achieve this by adaptively upscaling or downscaling the texture resolution during optimization. In addition, our approach enables control of the number of primitives during optimization based on texture resolution. We show that our approach performs favorably in image quality and total number of parameters used compared to alternative solutions for textured Gaussian primitives. Project page: https://repo-sam.inria.fr/nerphys/gs-texturing/",
    "arxiv_url": "https://arxiv.org/abs/2512.02621v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02621v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "real-time rendering",
      "geometry",
      "3d reconstruction",
      "ar",
      "mapping",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02621v1",
      "pdf": "https://arxiv.org/pdf/2512.02621v1",
      "project": "https://repo-sam.inria.fr/nerphys/gs-texturing"
    },
    "bibtex": ""
  },
  {
    "title": "G-SHARP: Gaussian Surgical Hardware Accelerated Real-time Pipeline",
    "authors": [
      "Vishwesh Nath",
      "Javier G. Tejero",
      "Ruilong Li",
      "Filippo Filicori",
      "Mahdi Azizian",
      "Sean D. Huver"
    ],
    "abstract": "We propose G-SHARP, a commercially compatible, real-time surgical scene reconstruction framework designed for minimally invasive procedures that require fast and accurate 3D modeling of deformable tissue. While recent Gaussian splatting approaches have advanced real-time endoscopic reconstruction, existing implementations often depend on non-commercial derivatives, limiting deployability. G-SHARP overcomes these constraints by being the first surgical pipeline built natively on the GSplat (Apache-2.0) differentiable Gaussian rasterizer, enabling principled deformation modeling, robust occlusion handling, and high-fidelity reconstructions on the EndoNeRF pulling benchmark. Our results demonstrate state-of-the-art reconstruction quality with strong speed-accuracy trade-offs suitable for intra-operative use. Finally, we provide a Holoscan SDK application that deploys G-SHARP on NVIDIA IGX Orin and Thor edge hardware, enabling real-time surgical visualization in practical operating-room settings.",
    "arxiv_url": "https://arxiv.org/abs/2512.02482v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02482v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "deformation",
      "ar",
      "nerf",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02482v1",
      "pdf": "https://arxiv.org/pdf/2512.02482v1"
    },
    "bibtex": ""
  },
  {
    "title": "VIGS-SLAM: Visual Inertial Gaussian Splatting SLAM",
    "authors": [
      "Zihan Zhu",
      "Wei Zhang",
      "Norbert Haala",
      "Marc Pollefeys",
      "Daniel Barath"
    ],
    "abstract": "We present VIGS-SLAM, a visual-inertial 3D Gaussian Splatting SLAM system that achieves robust real-time tracking and high-fidelity reconstruction. Although recent 3DGS-based SLAM methods achieve dense and photorealistic mapping, their purely visual design degrades under motion blur, low texture, and exposure variations. Our method tightly couples visual and inertial cues within a unified optimization framework, jointly refining camera poses, depths, and IMU states. It features robust IMU initialization, time-varying bias modeling, and loop closure with consistent Gaussian updates. Experiments on four challenging datasets demonstrate our superiority over state-of-the-art methods. Project page: https://vigs-slam.github.io",
    "arxiv_url": "https://arxiv.org/abs/2512.02293v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02293v1",
    "published_date": "2025-12-02",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "tracking",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "slam",
      "mapping",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02293v1",
      "pdf": "https://arxiv.org/pdf/2512.02293v1",
      "project": "https://vigs-slam.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "SplatSuRe: Selective Super-Resolution for Multi-view Consistent 3D Gaussian Splatting",
    "authors": [
      "Pranav Asthana",
      "Alex Hanson",
      "Allen Tu",
      "Tom Goldstein",
      "Matthias Zwicker",
      "Amitabh Varshney"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enables high-quality novel view synthesis, motivating interest in generating higher-resolution renders than those available during training. A natural strategy is to apply super-resolution (SR) to low-resolution (LR) input views, but independently enhancing each image introduces multi-view inconsistencies, leading to blurry renders. Prior methods attempt to mitigate these inconsistencies through learned neural components, temporally consistent video priors, or joint optimization on LR and SR views, but all uniformly apply SR across every image. In contrast, our key insight is that close-up LR views may contain high-frequency information for regions also captured in more distant views, and that we can use the camera pose relative to scene geometry to inform where to add SR content. Building from this insight, we propose SplatSuRe, a method that selectively applies SR content only in undersampled regions lacking high-frequency supervision, yielding sharper and more consistent results. Across Tanks & Temples, Deep Blending and Mip-NeRF 360, our approach surpasses baselines in both fidelity and perceptual quality. Notably, our gains are most significant in localized foreground regions where higher detail is desired.",
    "arxiv_url": "https://arxiv.org/abs/2512.02172v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02172v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "nerf",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02172v1",
      "pdf": "https://arxiv.org/pdf/2512.02172v1"
    },
    "bibtex": ""
  },
  {
    "title": "ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation",
    "authors": [
      "Chenyang Gu",
      "Jiaming Liu",
      "Hao Chen",
      "Runzhong Huang",
      "Qingpo Wuwu",
      "Zhuoyang Liu",
      "Xiaoqi Li",
      "Ying Li",
      "Renrui Zhang",
      "Peng Jia",
      "Pheng-Ann Heng",
      "Shanghang Zhang"
    ],
    "abstract": "Vision-Language-Action (VLA) models have recently emerged, demonstrating strong generalization in robotic scene understanding and manipulation. However, when confronted with long-horizon tasks that require defined goal states, such as LEGO assembly or object rearrangement, existing VLA models still face challenges in coordinating high-level planning with precise manipulation. Therefore, we aim to endow a VLA model with the capability to infer the \"how\" process from the \"what\" outcomes, transforming goal states into executable procedures. In this paper, we introduce ManualVLA, a unified VLA framework built upon a Mixture-of-Transformers (MoT) architecture, enabling coherent collaboration between multimodal manual generation and action execution. Unlike prior VLA models that directly map sensory inputs to actions, we first equip ManualVLA with a planning expert that generates intermediate manuals consisting of images, position prompts, and textual instructions. Building upon these multimodal manuals, we design a Manual Chain-of-Thought (ManualCoT) reasoning process that feeds them into the action expert, where each manual step provides explicit control conditions, while its latent representation offers implicit guidance for accurate manipulation. To alleviate the burden of data collection, we develop a high-fidelity digital-twin toolkit based on 3D Gaussian Splatting, which automatically generates manual data for planning expert training. ManualVLA demonstrates strong real-world performance, achieving an average success rate 32% higher than the previous hierarchical SOTA baseline on LEGO assembly and object rearrangement tasks.",
    "arxiv_url": "https://arxiv.org/abs/2512.02013v1",
    "pdf_url": "https://arxiv.org/pdf/2512.02013v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.02013v1",
      "pdf": "https://arxiv.org/pdf/2512.02013v1"
    },
    "bibtex": ""
  },
  {
    "title": "TagSplat: Topology-Aware Gaussian Splatting for Dynamic Mesh Modeling and Tracking",
    "authors": [
      "Hanzhi Guo",
      "Dongdong Weng",
      "Mo Su",
      "Yixiao Chen",
      "Xiaonuo Dongye",
      "Chenyu Xu"
    ],
    "abstract": "Topology-consistent dynamic model sequences are essential for applications such as animation and model editing. However, existing 4D reconstruction methods face challenges in generating high-quality topology-consistent meshes. To address this, we propose a topology-aware dynamic reconstruction framework based on Gaussian Splatting. We introduce a Gaussian topological structure that explicitly encodes spatial connectivity. This structure enables topology-aware densification and pruning, preserving the manifold consistency of the Gaussian representation. Temporal regularization terms further ensure topological coherence over time, while differentiable mesh rasterization improves mesh quality. Experimental results demonstrate that our method reconstructs topology-consistent mesh sequences with significantly higher accuracy than existing approaches. Moreover, the resulting meshes enable precise 3D keypoint tracking. Project page: https://haza628.github.io/tagSplat/",
    "arxiv_url": "https://arxiv.org/abs/2512.01329v1",
    "pdf_url": "https://arxiv.org/pdf/2512.01329v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "face",
      "tracking",
      "animation",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.01329v1",
      "pdf": "https://arxiv.org/pdf/2512.01329v1",
      "project": "https://haza628.github.io/tagSplat"
    },
    "bibtex": ""
  },
  {
    "title": "Gaussian Swaying: Surface-Based Framework for Aerodynamic Simulation with 3D Gaussians",
    "authors": [
      "Hongru Yan",
      "Xiang Zhang",
      "Zeyuan Chen",
      "Fangyin Wei",
      "Zhuowen Tu"
    ],
    "abstract": "Branches swaying in the breeze, flags rippling in the wind, and boats rocking on the water all show how aerodynamics shape natural motion -- an effect crucial for realism in vision and graphics. In this paper, we present Gaussian Swaying, a surface-based framework for aerodynamic simulation using 3D Gaussians. Unlike mesh-based methods that require costly meshing, or particle-based approaches that rely on discrete positional data, Gaussian Swaying models surfaces continuously with 3D Gaussians, enabling efficient and fine-grained aerodynamic interaction. Our framework unifies simulation and rendering on the same representation: Gaussian patches, which support force computation for dynamics while simultaneously providing normals for lightweight shading. Comprehensive experiments on both synthetic and real-world datasets across multiple metrics demonstrate that Gaussian Swaying achieves state-of-the-art performance and efficiency, offering a scalable approach for realistic aerodynamic scene simulation.",
    "arxiv_url": "https://arxiv.org/abs/2512.01306v2",
    "pdf_url": "https://arxiv.org/pdf/2512.01306v2",
    "published_date": "2025-12-01",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "lightweight",
      "face",
      "ar",
      "3d gaussian",
      "efficient",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.01306v2",
      "pdf": "https://arxiv.org/pdf/2512.01306v2"
    },
    "bibtex": ""
  },
  {
    "title": "EGG-Fusion: Efficient 3D Reconstruction with Geometry-aware Gaussian Surfel on the Fly",
    "authors": [
      "Xiaokun Pan",
      "Zhenzhe Li",
      "Zhichao Ye",
      "Hongjia Zhai",
      "Guofeng Zhang"
    ],
    "abstract": "Real-time 3D reconstruction is a fundamental task in computer graphics. Recently, differentiable-rendering-based SLAM system has demonstrated significant potential, enabling photorealistic scene rendering through learnable scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Current differentiable rendering methods face dual challenges in real-time computation and sensor noise sensitivity, leading to degraded geometric fidelity in scene reconstruction and limited practicality. To address these challenges, we propose a novel real-time system EGG-Fusion, featuring robust sparse-to-dense camera tracking and a geometry-aware Gaussian surfel mapping module, introducing an information filter-based fusion method that explicitly accounts for sensor noise to achieve high-precision surface reconstruction. The proposed differentiable Gaussian surfel mapping effectively models multi-view consistent surfaces while enabling efficient parameter optimization. Extensive experimental results demonstrate that the proposed system achieves a surface reconstruction error of 0.6\\textit{cm} on standardized benchmark datasets including Replica and ScanNet++, representing over 20\\% improvement in accuracy compared to state-of-the-art (SOTA) GS-based methods. Notably, the system maintains real-time processing capabilities at 24 FPS, establishing it as one of the most accurate differentiable-rendering-based real-time reconstruction systems. Project Page: https://zju3dv.github.io/eggfusion/",
    "arxiv_url": "https://arxiv.org/abs/2512.01296v1",
    "pdf_url": "https://arxiv.org/pdf/2512.01296v1",
    "published_date": "2025-12-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "tracking",
      "3d reconstruction",
      "geometry",
      "ar",
      "nerf",
      "mapping",
      "3d gaussian",
      "slam",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.01296v1",
      "pdf": "https://arxiv.org/pdf/2512.01296v1",
      "project": "https://zju3dv.github.io/eggfusion"
    },
    "bibtex": ""
  },
  {
    "title": "LISA-3D: Lifting Language-Image Segmentation to 3D via Multi-View Consistency",
    "authors": [
      "Zhongbin Guo",
      "Jiahe Liu",
      "Wenyu Gao",
      "Yushan Li",
      "Chengzhi Li",
      "Ping Jian"
    ],
    "abstract": "Text-driven 3D reconstruction demands a mask generator that simultaneously understands open-vocabulary instructions and remains consistent across viewpoints. We present LISA-3D, a two-stage framework that lifts language-image segmentation into 3D by retrofitting the instruction-following model LISA with geometry-aware Low-Rank Adaptation (LoRA) layers and reusing a frozen SAM-3D reconstructor. During training we exploit off-the-shelf RGB-D sequences and their camera poses to build a differentiable reprojection loss that enforces cross-view agreement without requiring any additional 3D-text supervision. The resulting masks are concatenated with RGB images to form RGBA prompts for SAM-3D, which outputs Gaussian splats or textured meshes without retraining. Across ScanRefer and Nr3D, LISA-3D improves language-to-3D accuracy by up to +15.6 points over single-view baselines while adapting only 11.6M parameters. The system is modular, data-efficient, and supports zero-shot deployment on unseen categories, providing a practical recipe for language-guided 3D content creation. Our code will be available at https://github.com/binisalegend/LISA-3D.",
    "arxiv_url": "https://arxiv.org/abs/2512.01008v1",
    "pdf_url": "https://arxiv.org/pdf/2512.01008v1",
    "published_date": "2025-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/binisalegend/LISA-3D",
    "keywords": [
      "3d reconstruction",
      "segmentation",
      "ar",
      "geometry",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.01008v1",
      "pdf": "https://arxiv.org/pdf/2512.01008v1",
      "github": "https://github.com/binisalegend/LISA-3D"
    },
    "bibtex": ""
  },
  {
    "title": "EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head",
    "authors": [
      "Chang Liu",
      "Tianjiao Jing",
      "Chengcheng Ma",
      "Xuanqi Zhou",
      "Zhengxuan Lian",
      "Qin Jin",
      "Hongliang Yuan",
      "Shi-Sheng Huang"
    ],
    "abstract": "Recent photo-realistic 3D talking head via 3D Gaussian Splatting still has significant shortcoming in emotional expression manipulation, especially for fine-grained and expansive dynamics emotional editing using multi-modal control. This paper introduces a new editable 3D Gaussian talking head, i.e. EmoDiffTalk. Our key idea is a novel Emotion-aware Gaussian Diffusion, which includes an action unit (AU) prompt Gaussian diffusion process for fine-grained facial animator, and moreover an accurate text-to-AU emotion controller to provide accurate and expansive dynamic emotional editing using text input. Experiments on public EmoTalk3D and RenderMe-360 datasets demonstrate superior emotional subtlety, lip-sync fidelity, and controllability of our EmoDiffTalk over previous works, establishing a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis. To our best knowledge, our EmoDiffTalk is one of the first few 3D Gaussian Splatting talking-head generation framework, especially supporting continuous, multimodal emotional editing within the AU-based expression space.",
    "arxiv_url": "https://arxiv.org/abs/2512.05991v2",
    "pdf_url": "https://arxiv.org/pdf/2512.05991v2",
    "published_date": "2025-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "dynamic",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05991v2",
      "pdf": "https://arxiv.org/pdf/2512.05991v2"
    },
    "bibtex": ""
  },
  {
    "title": "Binary-Gaussian: Compact and Progressive Representation for 3D Gaussian Segmentation",
    "authors": [
      "An Yang",
      "Chenyu Liu",
      "Jun Du",
      "Jianqing Gao",
      "Jia Pan",
      "Jinshui Hu",
      "Baocai Yin",
      "Bing Yin",
      "Cong Liu"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) has emerged as an efficient 3D representation and a promising foundation for semantic tasks like segmentation. However, existing 3D-GS-based segmentation methods typically rely on high-dimensional category features, which introduce substantial memory overhead. Moreover, fine-grained segmentation remains challenging due to label space congestion and the lack of stable multi-granularity control mechanisms. To address these limitations, we propose a coarse-to-fine binary encoding scheme for per-Gaussian category representation, which compresses each feature into a single integer via the binary-to-decimal mapping, drastically reducing memory usage. We further design a progressive training strategy that decomposes panoptic segmentation into a series of independent sub-tasks, reducing inter-class conflicts and thereby enhancing fine-grained segmentation capability. Additionally, we fine-tune opacity during segmentation training to address the incompatibility between photometric rendering and semantic segmentation, which often leads to foreground-background confusion. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art segmentation performance while significantly reducing memory consumption and accelerating inference.",
    "arxiv_url": "https://arxiv.org/abs/2512.00944v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00944v1",
    "published_date": "2025-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "compact",
      "segmentation",
      "ar",
      "semantic",
      "mapping",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.00944v1",
      "pdf": "https://arxiv.org/pdf/2512.00944v1"
    },
    "bibtex": ""
  },
  {
    "title": "Feed-Forward 3D Gaussian Splatting Compression with Long-Context Modeling",
    "authors": [
      "Zhening Liu",
      "Rui Song",
      "Yushi Huang",
      "Yingdong Hu",
      "Xinjie Zhang",
      "Jiawei Shao",
      "Zehong Lin",
      "Jun Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a revolutionary 3D representation. However, its substantial data size poses a major barrier to widespread adoption. While feed-forward 3DGS compression offers a practical alternative to costly per-scene per-train compressors, existing methods struggle to model long-range spatial dependencies, due to the limited receptive field of transform coding networks and the inadequate context capacity in entropy models. In this work, we propose a novel feed-forward 3DGS compression framework that effectively models long-range correlations to enable highly compact and generalizable 3D representations. Central to our approach is a large-scale context structure that comprises thousands of Gaussians based on Morton serialization. We then design a fine-grained space-channel auto-regressive entropy model to fully leverage this expansive context. Furthermore, we develop an attention-based transform coding model to extract informative latent priors by aggregating features from a wide range of neighboring Gaussians. Our method yields a $20\\times$ compression ratio for 3DGS in a feed-forward inference and achieves state-of-the-art performance among generalizable codecs.",
    "arxiv_url": "https://arxiv.org/abs/2512.00877v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00877v1",
    "published_date": "2025-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "compression",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.00877v1",
      "pdf": "https://arxiv.org/pdf/2512.00877v1"
    },
    "bibtex": ""
  },
  {
    "title": "Smol-GS: Compact Representations for Abstract 3D Gaussian Splatting",
    "authors": [
      "Haishan Wang",
      "Mohammad Hassan Vali",
      "Arno Solin"
    ],
    "abstract": "We present Smol-GS, a novel method for learning compact representations for 3D Gaussian Splatting (3DGS). Our approach learns highly efficient encodings in 3D space that integrate both spatial and semantic information. The model captures the coordinates of the splats through a recursive voxel hierarchy, while splat-wise features store abstracted cues, including color, opacity, transformation, and material properties. This design allows the model to compress 3D scenes by orders of magnitude without loss of flexibility. Smol-GS achieves state-of-the-art compression on standard benchmarks while maintaining high rendering quality. Beyond visual fidelity, the discrete representations could potentially serve as a foundation for downstream tasks such as navigation, planning, and broader 3D scene understanding.",
    "arxiv_url": "https://arxiv.org/abs/2512.00850v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00850v1",
    "published_date": "2025-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "understanding",
      "compression",
      "ar",
      "semantic",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.00850v1",
      "pdf": "https://arxiv.org/pdf/2512.00850v1"
    },
    "bibtex": ""
  },
  {
    "title": "PolarGS: Polarimetric Cues for Ambiguity-Free Gaussian Splatting with Accurate Geometry Recovery",
    "authors": [
      "Bo Guo",
      "Sijia Wen",
      "Yifan Zhao",
      "Jia Li",
      "Zhiming Zheng"
    ],
    "abstract": "Recent advances in surface reconstruction for 3D Gaussian Splatting (3DGS) have enabled remarkable geometric accuracy. However, their performance degrades in photometrically ambiguous regions such as reflective and textureless surfaces, where unreliable cues disrupt photometric consistency and hinder accurate geometry estimation. Reflected light is often partially polarized in a manner that reveals surface orientation, making polarization an optic complement to photometric cues in resolving such ambiguities. Therefore, we propose PolarGS, an optics-aware extension of RGB-based 3DGS that leverages polarization as an optical prior to resolve photometric ambiguities and enhance reconstruction accuracy. Specifically, we introduce two complementary modules: a polarization-guided photometric correction strategy, which ensures photometric consistency by identifying reflective regions via the Degree of Linear Polarization (DoLP) and refining reflective Gaussians with Color Refinement Maps; and a polarization-enhanced Gaussian densification mechanism for textureless area geometry recovery, which integrates both Angle and Degree of Linear Polarization (A/DoLP) into a PatchMatch-based depth completion process. This enables the back-projection and fusion of new Gaussians, leading to more complete reconstruction. PolarGS is framework-agnostic and achieves superior geometric accuracy compared to state-of-the-art methods.",
    "arxiv_url": "https://arxiv.org/abs/2512.00794v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00794v1",
    "published_date": "2025-11-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.00794v1",
      "pdf": "https://arxiv.org/pdf/2512.00794v1"
    },
    "bibtex": ""
  },
  {
    "title": "Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer",
    "authors": [
      "Dong In Lee",
      "Hyungjun Doh",
      "Seunggeun Chi",
      "Runlin Duan",
      "Sangpil Kim",
      "Karthik Ramani"
    ],
    "abstract": "Recent progress in 4D representations, such as Dynamic NeRF and 4D Gaussian Splatting (4DGS), has enabled dynamic 4D scene reconstruction. However, text-driven 4D scene editing remains under-explored due to the challenge of ensuring both multi-view and temporal consistency across space and time during editing. Existing studies rely on 2D diffusion models that edit frames independently, often causing motion distortion, geometric drift, and incomplete editing. We introduce Dynamic-eDiTor, a training-free text-driven 4D editing framework leveraging Multimodal Diffusion Transformer (MM-DiT) and 4DGS. This mechanism consists of Spatio-Temporal Sub-Grid Attention (STGA) for locally consistent cross-view and temporal fusion, and Context Token Propagation (CTP) for global propagation via token inheritance and optical-flow-guided token replacement. Together, these components allow Dynamic-eDiTor to perform seamless, globally consistent multi-view video without additional training and directly optimize pre-trained source 4DGS. Extensive experiments on multi-view video dataset DyNeRF demonstrate that our method achieves superior editing fidelity and both multi-view and temporal consistency prior approaches. Project page for results and code: https://di-lee.github.io/dynamic-eDiTor/",
    "arxiv_url": "https://arxiv.org/abs/2512.00677v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00677v1",
    "published_date": "2025-11-30",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "gaussian splatting",
      "nerf",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.00677v1",
      "pdf": "https://arxiv.org/pdf/2512.00677v1",
      "project": "https://di-lee.github.io/dynamic-eDiTor"
    },
    "bibtex": ""
  },
  {
    "title": "Asset-Driven Sematic Reconstruction of Dynamic Scene with Multi-Human-Object Interactions",
    "authors": [
      "Sandika Biswas",
      "Qianyi Wu",
      "Biplab Banerjee",
      "Hamid Rezatofighi"
    ],
    "abstract": "Real-world human-built environments are highly dynamic, involving multiple humans and their complex interactions with surrounding objects. While 3D geometry modeling of such scenes is crucial for applications like AR/VR, gaming, and embodied AI, it remains underexplored due to challenges like diverse motion patterns and frequent occlusions. Beyond novel view rendering, 3D Gaussian Splatting (GS) has demonstrated remarkable progress in producing detailed, high-quality surface geometry with fast optimization of the underlying structure. However, very few GS-based methods address multihuman, multiobject scenarios, primarily due to the above-mentioned inherent challenges. In a monocular setup, these challenges are further amplified, as maintaining structural consistency under severe occlusion becomes difficult when the scene is optimized solely based on GS-based rendering loss. To tackle the challenges of such a multihuman, multiobject dynamic scene, we propose a hybrid approach that effectively combines the advantages of 1) 3D generative models for generating high-fidelity meshes of the scene elements, 2) Semantic-aware deformation, \\ie rigid transformation of the rigid objects and LBS-based deformation of the humans, and mapping of the deformed high-fidelity meshes in the dynamic scene, and 3) GS-based optimization of the individual elements for further refining their alignments in the scene. Such a hybrid approach helps maintain the object structures even under severe occlusion and can produce multiview and temporally consistent geometry. We choose HOI-M3 for evaluation, as, to the best of our knowledge, this is the only dataset featuring multihuman, multiobject interactions in a dynamic scene. Our method outperforms the state-of-the-art method in producing better surface reconstruction of such scenes.",
    "arxiv_url": "https://arxiv.org/abs/2512.00547v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00547v1",
    "published_date": "2025-11-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "vr",
      "high-fidelity",
      "face",
      "deformation",
      "motion",
      "geometry",
      "ar",
      "semantic",
      "human",
      "3d gaussian",
      "mapping",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.00547v1",
      "pdf": "https://arxiv.org/pdf/2512.00547v1"
    },
    "bibtex": ""
  },
  {
    "title": "Cross-Temporal 3D Gaussian Splatting for Sparse-View Guided Scene Update",
    "authors": [
      "Zeyuan An",
      "Yanghang Xiao",
      "Zhiying Leng",
      "Frederick W. B. Li",
      "Xiaohui Liang"
    ],
    "abstract": "Maintaining consistent 3D scene representations over time is a significant challenge in computer vision. Updating 3D scenes from sparse-view observations is crucial for various real-world applications, including urban planning, disaster assessment, and historical site preservation, where dense scans are often unavailable or impractical. In this paper, we propose Cross-Temporal 3D Gaussian Splatting (Cross-Temporal 3DGS), a novel framework for efficiently reconstructing and updating 3D scenes across different time periods, using sparse images and previously captured scene priors. Our approach comprises three stages: 1) Cross-temporal camera alignment for estimating and aligning camera poses across different timestamps; 2) Interference-based confidence initialization to identify unchanged regions between timestamps, thereby guiding updates; and 3) Progressive cross-temporal optimization, which iteratively integrates historical prior information into the 3D scene to enhance reconstruction quality. Our method supports non-continuous capture, enabling not only updates using new sparse views to refine existing scenes, but also recovering past scenes from limited data with the help of current captures. Furthermore, we demonstrate the potential of this approach to achieve temporal changes using only sparse images, which can later be reconstructed into detailed 3D representations as needed. Experimental results show significant improvements over baseline methods in reconstruction quality and data efficiency, making this approach a promising solution for scene versioning, cross-temporal digital twins, and long-term spatial documentation.",
    "arxiv_url": "https://arxiv.org/abs/2512.00534v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00534v1",
    "published_date": "2025-11-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "sparse view",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.00534v1",
      "pdf": "https://arxiv.org/pdf/2512.00534v1"
    },
    "bibtex": ""
  },
  {
    "title": "SplatFont3D: Structure-Aware Text-to-3D Artistic Font Generation with Part-Level Style Control",
    "authors": [
      "Ji Gan",
      "Lingxu Chen",
      "Jiaxu Leng",
      "Xinbo Gao"
    ],
    "abstract": "Artistic font generation (AFG) can assist human designers in creating innovative artistic fonts. However, most previous studies primarily focus on 2D artistic fonts in flat design, leaving personalized 3D-AFG largely underexplored. 3D-AFG not only enables applications in immersive 3D environments such as video games and animations, but also may enhance 2D-AFG by rendering 2D fonts of novel views. Moreover, unlike general 3D objects, 3D fonts exhibit precise semantics with strong structural constraints and also demand fine-grained part-level style control. To address these challenges, we propose SplatFont3D, a novel structure-aware text-to-3D AFG framework with 3D Gaussian splatting, which enables the creation of 3D artistic fonts from diverse style text prompts with precise part-level style control. Specifically, we first introduce a Glyph2Cloud module, which progressively enhances both the shapes and styles of 2D glyphs (or components) and produces their corresponding 3D point clouds for Gaussian initialization. The initialized 3D Gaussians are further optimized through interaction with a pretrained 2D diffusion model using score distillation sampling. To enable part-level control, we present a dynamic component assignment strategy that exploits the geometric priors of 3D Gaussians to partition components, while alleviating drift-induced entanglement during 3D Gaussian optimization. Our SplatFont3D provides more explicit and effective part-level style control than NeRF, attaining faster rendering efficiency. Experiments show that our SplatFont3D outperforms existing 3D models for 3D-AFG in style-text consistency, visual quality, and rendering efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2512.00413v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00413v1",
    "published_date": "2025-11-29",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "animation",
      "ar",
      "semantic",
      "nerf",
      "human",
      "3d gaussian",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.00413v1",
      "pdf": "https://arxiv.org/pdf/2512.00413v1"
    },
    "bibtex": ""
  },
  {
    "title": "TGSFormer: Scalable Temporal Gaussian Splatting for Embodied Semantic Scene Completion",
    "authors": [
      "Rui Qian",
      "Haozhi Cao",
      "Tianchen Deng",
      "Tianxin Hu",
      "Weixiang Guo",
      "Shenghai Yuan",
      "Lihua Xie"
    ],
    "abstract": "Embodied 3D Semantic Scene Completion (SSC) infers dense geometry and semantics from continuous egocentric observations. Most existing Gaussian-based methods rely on random initialization of many primitives within predefined spatial bounds, resulting in redundancy and poor scalability to unbounded scenes. Recent depth-guided approach alleviates this issue but remains local, suffering from latency and memory overhead as scale increases. To overcome these challenges, we propose TGSFormer, a scalable Temporal Gaussian Splatting framework for embodied SSC. It maintains a persistent Gaussian memory for temporal prediction, without relying on image coherence or frame caches. For temporal fusion, a Dual Temporal Encoder jointly processes current and historical Gaussian features through confidence-aware cross-attention. Subsequently, a Confidence-aware Voxel Fusion module merges overlapping primitives into voxel-aligned representations, regulating density and maintaining compactness. Extensive experiments demonstrate that TGSFormer achieves state-of-the-art results on both local and embodied SSC benchmarks, offering superior accuracy and scalability with significantly fewer primitives while maintaining consistent long-term scene integrity. The code will be released upon acceptance.",
    "arxiv_url": "https://arxiv.org/abs/2512.00300v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00300v1",
    "published_date": "2025-11-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "compact",
      "geometry",
      "ar",
      "semantic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.00300v1",
      "pdf": "https://arxiv.org/pdf/2512.00300v1"
    },
    "bibtex": ""
  },
  {
    "title": "Relightable Holoported Characters: Capturing and Relighting Dynamic Human Performance from Sparse Views",
    "authors": [
      "Kunwar Maheep Singh",
      "Jianchun Chen",
      "Vladislav Golyanik",
      "Stephan J. Garbin",
      "Thabo Beeler",
      "Rishabh Dabral",
      "Marc Habermann",
      "Christian Theobalt"
    ],
    "abstract": "We present Relightable Holoported Characters (RHC), a novel person-specific method for free-view rendering and relighting of full-body and highly dynamic humans solely observed from sparse-view RGB videos at inference. In contrast to classical one-light-at-a-time (OLAT)-based human relighting, our transformer-based RelightNet predicts relit appearance within a single network pass, avoiding costly OLAT-basis capture and generation. For training such a model, we introduce a new capture strategy and dataset recorded in a multi-view lightstage, where we alternate frames lit by random environment maps with uniformly lit tracking frames, simultaneously enabling accurate motion tracking and diverse illumination as well as dynamics coverage. Inspired by the rendering equation, we derive physics-informed features that encode geometry, albedo, shading, and the virtual camera view from a coarse human mesh proxy and the input views. Our RelightNet then takes these features as input and cross-attends them with a novel lighting condition, and regresses the relit appearance in the form of texel-aligned 3D Gaussian splats attached to the coarse mesh proxy. Consequently, our RelightNet implicitly learns to efficiently compute the rendering equation for novel lighting conditions within a single feed-forward pass. Experiments demonstrate our method's superior visual fidelity and lighting reproduction compared to state-of-the-art approaches. Project page: https://vcai.mpi-inf.mpg.de/projects/RHC/",
    "arxiv_url": "https://arxiv.org/abs/2512.00255v1",
    "pdf_url": "https://arxiv.org/pdf/2512.00255v1",
    "published_date": "2025-11-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "illumination",
      "sparse-view",
      "dynamic",
      "sparse view",
      "body",
      "tracking",
      "geometry",
      "ar",
      "efficient",
      "human",
      "relightable",
      "3d gaussian",
      "lighting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.00255v1",
      "pdf": "https://arxiv.org/pdf/2512.00255v1",
      "project": "https://vcai.mpi-inf.mpg.de/projects/RHC"
    },
    "bibtex": ""
  },
  {
    "title": "FACT-GS: Frequency-Aligned Complexity-Aware Texture Reparameterization for 2D Gaussian Splatting",
    "authors": [
      "Tianhao Xie",
      "Linlian Jiang",
      "Xinxin Zuo",
      "Yang Wang",
      "Tiberiu Popa"
    ],
    "abstract": "Realistic scene appearance modeling has advanced rapidly with Gaussian Splatting, which enables real-time, high-quality rendering. Recent advances introduced per-primitive textures that incorporate spatial color variations within each Gaussian, improving their expressiveness. However, texture-based Gaussians parameterize appearance with a uniform per-Gaussian sampling grid, allocating equal sampling density regardless of local visual complexity. This leads to inefficient texture space utilization, where high-frequency regions are under-sampled and smooth regions waste capacity, causing blurred appearance and loss of fine structural detail. We introduce FACT-GS, a Frequency-Aligned Complexity-aware Texture Gaussian Splatting framework that allocates texture sampling density according to local visual frequency. Grounded in adaptive sampling theory, FACT-GS reformulates texture parameterization as a differentiable sampling-density allocation problem, replacing the uniform textures with a learnable frequency-aware allocation strategy implemented via a deformation field whose Jacobian modulates local sampling density. Built on 2D Gaussian Splatting, FACT-GS performs non-uniform sampling on fixed-resolution texture grids, preserving real-time performance while recovering sharper high-frequency details under the same parameter budget.",
    "arxiv_url": "https://arxiv.org/abs/2511.23292v2",
    "pdf_url": "https://arxiv.org/pdf/2511.23292v2",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "deformation",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.23292v2",
      "pdf": "https://arxiv.org/pdf/2511.23292v2"
    },
    "bibtex": ""
  },
  {
    "title": "Robust 3DGS-based SLAM via Adaptive Kernel Smoothing",
    "authors": [
      "Shouhe Zhang",
      "Dayong Ren",
      "Sensen Song",
      "Wenjie Li",
      "Piaopiao Yu",
      "Yurong Qian"
    ],
    "abstract": "In this paper, we challenge the conventional notion in 3DGS-SLAM that rendering quality is the primary determinant of tracking accuracy. We argue that, compared to solely pursuing a perfect scene representation, it is more critical to enhance the robustness of the rasterization process against parameter errors to ensure stable camera pose tracking. To address this challenge, we propose a novel approach that leverages a smooth kernel strategy to enhance the robustness of 3DGS-based SLAM. Unlike conventional methods that focus solely on minimizing rendering error, our core insight is to make the rasterization process more resilient to imperfections in the 3DGS parameters. We hypothesize that by allowing each Gaussian to influence a smoother, wider distribution of pixels during rendering, we can mitigate the detrimental effects of parameter noise from outlier Gaussians. This approach intentionally introduces a controlled blur to the rendered image, which acts as a regularization term, stabilizing the subsequent pose optimization. While a complete redesign of the rasterization pipeline is an ideal solution, we propose a practical and effective alternative that is readily integrated into existing 3DGS frameworks. Our method, termed Corrective Blurry KNN (CB-KNN), adaptively modifies the RGB values and locations of the K-nearest neighboring Gaussians within a local region. This dynamic adjustment generates a smoother local rendering, reducing the impact of erroneous GS parameters on the overall image. Experimental results demonstrate that our approach, while maintaining the overall quality of the scene reconstruction (mapping), significantly improves the robustness and accuracy of camera pose tracking.",
    "arxiv_url": "https://arxiv.org/abs/2511.23221v1",
    "pdf_url": "https://arxiv.org/pdf/2511.23221v1",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "tracking",
      "ar",
      "slam",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.23221v1",
      "pdf": "https://arxiv.org/pdf/2511.23221v1"
    },
    "bibtex": ""
  },
  {
    "title": "Geometry-Consistent 4D Gaussian Splatting for Sparse-Input Dynamic View Synthesis",
    "authors": [
      "Yiwei Li",
      "Jiannong Cao",
      "Penghui Ruan",
      "Divya Saxena",
      "Songye Zhu",
      "Yinfeng Cao"
    ],
    "abstract": "Gaussian Splatting has been considered as a novel way for view synthesis of dynamic scenes, which shows great potential in AIoT applications such as digital twins. However, recent dynamic Gaussian Splatting methods significantly degrade when only sparse input views are available, limiting their applicability in practice. The issue arises from the incoherent learning of 4D geometry as input views decrease. This paper presents GC-4DGS, a novel framework that infuses geometric consistency into 4D Gaussian Splatting (4DGS), offering real-time and high-quality dynamic scene rendering from sparse input views. While learning-based Multi-View Stereo (MVS) and monocular depth estimators (MDEs) provide geometry priors, directly integrating these with 4DGS yields suboptimal results due to the ill-posed nature of sparse-input 4D geometric optimization. To address these problems, we introduce a dynamic consistency checking strategy to reduce estimation uncertainties of MVS across spacetime. Furthermore, we propose a global-local depth regularization approach to distill spatiotemporal-consistent geometric information from monocular depths, thereby enhancing the coherent geometry and appearance learning within the 4D volume. Extensive experiments on the popular N3DV and Technicolor datasets validate the effectiveness of GC-4DGS in rendering quality without sacrificing efficiency. Notably, our method outperforms RF-DeRF, the latest dynamic radiance field tailored for sparse-input dynamic view synthesis, and the original 4DGS by 2.62dB and 1.58dB in PSNR, respectively, with seamless deployability on resource-constrained IoT edge devices.",
    "arxiv_url": "https://arxiv.org/abs/2511.23044v1",
    "pdf_url": "https://arxiv.org/pdf/2511.23044v1",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.23044v1",
      "pdf": "https://arxiv.org/pdf/2511.23044v1"
    },
    "bibtex": ""
  },
  {
    "title": "DiskChunGS: Large-Scale 3D Gaussian SLAM Through Chunk-Based Memory Management",
    "authors": [
      "Casimir Feldmann",
      "Maximum Wilder-Smith",
      "Vaishakh Patil",
      "Michael Oechsle",
      "Michael Niemeyer",
      "Keisuke Tateno",
      "Marco Hutter"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated impressive results for novel view synthesis with real-time rendering capabilities. However, integrating 3DGS with SLAM systems faces a fundamental scalability limitation: methods are constrained by GPU memory capacity, restricting reconstruction to small-scale environments. We present DiskChunGS, a scalable 3DGS SLAM system that overcomes this bottleneck through an out-of-core approach that partitions scenes into spatial chunks and maintains only active regions in GPU memory while storing inactive areas on disk. Our architecture integrates seamlessly with existing SLAM frameworks for pose estimation and loop closure, enabling globally consistent reconstruction at scale. We validate DiskChunGS on indoor scenes (Replica, TUM-RGBD), urban driving scenarios (KITTI), and resource-constrained Nvidia Jetson platforms. Our method uniquely completes all 11 KITTI sequences without memory failures while achieving superior visual quality, demonstrating that algorithmic innovation can overcome the memory constraints that have limited previous 3DGS SLAM methods.",
    "arxiv_url": "https://arxiv.org/abs/2511.23030v1",
    "pdf_url": "https://arxiv.org/pdf/2511.23030v1",
    "published_date": "2025-11-28",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "real-time rendering",
      "ar",
      "3d gaussian",
      "slam",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.23030v1",
      "pdf": "https://arxiv.org/pdf/2511.23030v1"
    },
    "bibtex": ""
  },
  {
    "title": "MrGS: Multi-modal Radiance Fields with 3D Gaussian Splatting for RGB-Thermal Novel View Synthesis",
    "authors": [
      "Minseong Kweon",
      "Janghyun Kim",
      "Ukcheol Shin",
      "Jinsun Park"
    ],
    "abstract": "Recent advances in Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) have achieved considerable performance in RGB scene reconstruction. However, multi-modal rendering that incorporates thermal infrared imagery remains largely underexplored. Existing approaches tend to neglect distinctive thermal characteristics, such as heat conduction and the Lambertian property. In this study, we introduce MrGS, a multi-modal radiance field based on 3DGS that simultaneously reconstructs both RGB and thermal 3D scenes. Specifically, MrGS derives RGB- and thermal-related information from a single appearance feature through orthogonal feature extraction and employs view-dependent or view-independent embedding strategies depending on the degree of Lambertian reflectance exhibited by each modality. Furthermore, we leverage two physics-based principles to effectively model thermal-domain phenomena. First, we integrate Fourier's law of heat conduction prior to alpha blending to model intensity interpolation caused by thermal conduction between neighboring Gaussians. Second, we apply the Stefan-Boltzmann law and the inverse-square law to formulate a depth-aware thermal radiation map that imposes additional geometric constraints on thermal rendering. Experimental results demonstrate that the proposed MrGS achieves high-fidelity RGB-T scene reconstruction while reducing the number of Gaussians.",
    "arxiv_url": "https://arxiv.org/abs/2511.22997v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22997v1",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "nerf",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.22997v1",
      "pdf": "https://arxiv.org/pdf/2511.22997v1"
    },
    "bibtex": ""
  },
  {
    "title": "Taming the Light: Illumination-Invariant Semantic 3DGS-SLAM",
    "authors": [
      "Shouhe Zhang",
      "Dayong Ren",
      "Sensen Song",
      "Yurong Qian",
      "Zhenhong Jia"
    ],
    "abstract": "Extreme exposure degrades both the 3D map reconstruction and semantic segmentation accuracy, which is particularly detrimental to tightly-coupled systems. To achieve illumination invariance, we propose a novel semantic SLAM framework with two designs. First, the Intrinsic Appearance Normalization (IAN) module proactively disentangles the scene's intrinsic properties, such as albedo, from transient lighting. By learning a standardized, illumination-invariant appearance model, it assigns a stable and consistent color representation to each Gaussian primitive. Second, the Dynamic Radiance Balancing Loss (DRB-Loss) reactively handles frames with extreme exposure. It activates only when an image's exposure is poor, operating directly on the radiance field to guide targeted optimization. This prevents error accumulation from extreme lighting without compromising performance under normal conditions. The synergy between IAN's proactive invariance and DRB-Loss's reactive correction endows our system with unprecedented robustness. Evaluations on public datasets demonstrate state-of-the-art performance in camera tracking, map quality, and semantic and geometric accuracy.",
    "arxiv_url": "https://arxiv.org/abs/2511.22968v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22968v1",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "dynamic",
      "tracking",
      "segmentation",
      "ar",
      "semantic",
      "slam",
      "lighting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.22968v1",
      "pdf": "https://arxiv.org/pdf/2511.22968v1"
    },
    "bibtex": ""
  },
  {
    "title": "DenoiseGS: Gaussian Reconstruction Model for Burst Denoising",
    "authors": [
      "Yongsen Cheng",
      "Yuanhao Cai",
      "Yulun Zhang"
    ],
    "abstract": "Burst denoising methods are crucial for enhancing images captured on handheld devices, but they often struggle with large motion or suffer from prohibitive computational costs. In this paper, we propose DenoiseGS, the first framework to leverage the efficiency of 3D Gaussian Splatting for burst denoising. Our approach addresses two key challenges when applying feedforward Gaussian reconsturction model to noisy inputs: the degradation of Gaussian point clouds and the loss of fine details. To this end, we propose a Gaussian self-consistency (GSC) loss, which regularizes the geometry predicted from noisy inputs with high-quality Gaussian point clouds. These point clouds are generated from clean inputs by the same model that we are training, thereby alleviating potential bias or domain gaps. Additionally, we introduce a log-weighted frequency (LWF) loss to strengthen supervision within the spectral domain, effectively preserving fine-grained details. The LWF loss adaptively weights frequency discrepancies in a logarithmic manner, emphasizing challenging high-frequency details. Extensive experiments demonstrate that DenoiseGS significantly exceeds the state-of-the-art NeRF-based methods on both burst denoising and novel view synthesis under noisy conditions, while achieving 250$\\times$ faster inference speed. Code and models are released at https://github.com/yscheng04/DenoiseGS.",
    "arxiv_url": "https://arxiv.org/abs/2511.22939v2",
    "pdf_url": "https://arxiv.org/pdf/2511.22939v2",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/yscheng04/DenoiseGS",
    "keywords": [
      "motion",
      "geometry",
      "ar",
      "nerf",
      "3d gaussian",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.22939v2",
      "pdf": "https://arxiv.org/pdf/2511.22939v2",
      "github": "https://github.com/yscheng04/DenoiseGS"
    },
    "bibtex": ""
  },
  {
    "title": "VG3T: Visual Geometry Grounded Gaussian Transformer",
    "authors": [
      "Junho Kim",
      "Seongwon Lee"
    ],
    "abstract": "Generating a coherent 3D scene representation from multi-view images is a fundamental yet challenging task. Existing methods often struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance. To address this, we introduce VG3T, a novel multi-view feed-forward network that predicts a 3D semantic occupancy via a 3D Gaussian representation. Unlike prior methods that infer Gaussians from single-view images, our model directly predicts a set of semantically attributed Gaussians in a joint, multi-view fashion. This novel approach overcomes the fragmentation and inconsistency inherent in view-by-view processing, offering a unified paradigm to represent both geometry and semantics. We also introduce two key components, Grid-Based Sampling and Positional Refinement, to mitigate the distance-dependent density bias common in pixel-aligned Gaussian initialization methods. Our VG3T shows a notable 1.7%p improvement in mIoU while using 46% fewer primitives than the previous state-of-the-art on the nuScenes benchmark, highlighting its superior efficiency and performance.",
    "arxiv_url": "https://arxiv.org/abs/2512.05988v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05988v1",
    "published_date": "2025-11-28",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "semantic",
      "3d gaussian",
      "lighting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2512.05988v1",
      "pdf": "https://arxiv.org/pdf/2512.05988v1"
    },
    "bibtex": ""
  },
  {
    "title": "GSpaRC: Gaussian Splatting for Real-time Reconstruction of RF Channels",
    "authors": [
      "Bhavya Sai Nukapotula",
      "Rishabh Tripathi",
      "Seth Pregler",
      "Dileep Kalathil",
      "Srinivas Shakkottai",
      "Theodore S. Rappaport"
    ],
    "abstract": "Channel state information (CSI) is essential for adaptive beamforming and maintaining robust links in wireless communication systems. However, acquiring CSI incurs significant overhead, consuming up to 25\\% of spectrum resources in 5G networks due to frequent pilot transmissions at sub-millisecond intervals. Recent approaches aim to reduce this burden by reconstructing CSI from spatiotemporal RF measurements, such as signal strength and direction-of-arrival. While effective in offline settings, these methods often suffer from inference latencies in the 5--100~ms range, making them impractical for real-time systems. We present GSpaRC: Gaussian Splatting for Real-time Reconstruction of RF Channels, the first algorithm to break the 1 ms latency barrier while maintaining high accuracy. GSpaRC represents the RF environment using a compact set of 3D Gaussian primitives, each parameterized by a lightweight neural model augmented with physics-informed features such as distance-based attenuation. Unlike traditional vision-based splatting pipelines, GSpaRC is tailored for RF reception: it employs an equirectangular projection onto a hemispherical surface centered at the receiver to reflect omnidirectional antenna behavior. A custom CUDA pipeline enables fully parallelized directional sorting, splatting, and rendering across frequency and spatial dimensions. Evaluated on multiple RF datasets, GSpaRC achieves similar CSI reconstruction fidelity to recent state-of-the-art methods while reducing training and inference time by over an order of magnitude. By trading modest GPU computation for a substantial reduction in pilot overhead, GSpaRC enables scalable, low-latency channel estimation suitable for deployment in 5G and future wireless systems. The code is available here: \\href{https://github.com/Nbhavyasai/GSpaRC-WirelessGaussianSplatting.git}{GSpaRC}.",
    "arxiv_url": "https://arxiv.org/abs/2511.22793v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22793v1",
    "published_date": "2025-11-27",
    "categories": [
      "cs.LG"
    ],
    "github_url": "https://github.com/Nbhavyasai/GSpaRC-WirelessGaussianSplatting.git",
    "keywords": [
      "head",
      "compact",
      "lightweight",
      "face",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.22793v1",
      "pdf": "https://arxiv.org/pdf/2511.22793v1",
      "github": "https://github.com/Nbhavyasai/GSpaRC-WirelessGaussianSplatting.git"
    },
    "bibtex": ""
  },
  {
    "title": "Splat-SAP: Feed-Forward Gaussian Splatting for Human-Centered Scene with Scale-Aware Point Map Reconstruction",
    "authors": [
      "Boyao Zhou",
      "Shunyuan Zheng",
      "Zhanfeng Liao",
      "Zihan Ma",
      "Hanzhang Tu",
      "Boning Liu",
      "Yebin Liu"
    ],
    "abstract": "We present Splat-SAP, a feed-forward approach to render novel views of human-centered scenes from binocular cameras with large sparsity. Gaussian Splatting has shown its promising potential in rendering tasks, but it typically necessitates per-scene optimization with dense input views. Although some recent approaches achieve feed-forward Gaussian Splatting rendering through geometry priors obtained by multi-view stereo, such approaches still require largely overlapped input views to establish the geometry prior. To bridge this gap, we leverage pixel-wise point map reconstruction to represent geometry which is robust to large sparsity for its independent view modeling. In general, we propose a two-stage learning strategy. In stage 1, we transform the point map into real space via an iterative affinity learning process, which facilitates camera control in the following. In stage 2, we project point maps of two input views onto the target view plane and refine such geometry via stereo matching. Furthermore, we anchor Gaussian primitives on this refined plane in order to render high-quality images. As a metric representation, the scale-aware point map in stage 1 is trained in a self-supervised manner without 3D supervision and stage 2 is supervised with photo-metric loss. We collect multi-view human-centered data and demonstrate that our method improves both the stability of point map reconstruction and the visual quality of free-viewpoint rendering.",
    "arxiv_url": "https://arxiv.org/abs/2511.22704v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22704v1",
    "published_date": "2025-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "human",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.22704v1",
      "pdf": "https://arxiv.org/pdf/2511.22704v1"
    },
    "bibtex": ""
  },
  {
    "title": "Gaussians on Fire: High-Frequency Reconstruction of Flames",
    "authors": [
      "Jakob Nazarenus",
      "Dominik Michels",
      "Wojtek Palubicki",
      "Simin Kou",
      "Fang-Lue Zhang",
      "Soren Pirk",
      "Reinhard Koch"
    ],
    "abstract": "We propose a method to reconstruct dynamic fire in 3D from a limited set of camera views with a Gaussian-based spatiotemporal representation. Capturing and reconstructing fire and its dynamics is highly challenging due to its volatile nature, transparent quality, and multitude of high-frequency features. Despite these challenges, we aim to reconstruct fire from only three views, which consequently requires solving for under-constrained geometry. We solve this by separating the static background from the dynamic fire region by combining dense multi-view stereo images with monocular depth priors. The fire is initialized as a 3D flow field, obtained by fusing per-view dense optical flow projections. To capture the high frequency features of fire, each 3D Gaussian encodes a lifetime and linear velocity to match the dense optical flow. To ensure sub-frame temporal alignment across cameras we employ a custom hardware synchronization pattern -- allowing us to reconstruct fire with affordable commodity hardware. Our quantitative and qualitative validations across numerous reconstruction experiments demonstrate robust performance for diverse and challenging real fire scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2511.22459v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22459v1",
    "published_date": "2025-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "dynamic",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.22459v1",
      "pdf": "https://arxiv.org/pdf/2511.22459v1"
    },
    "bibtex": ""
  },
  {
    "title": "Can Protective Watermarking Safeguard the Copyright of 3D Gaussian Splatting?",
    "authors": [
      "Wenkai Huang",
      "Yijia Guo",
      "Gaolei Li",
      "Lei Ma",
      "Hang Zhang",
      "Liwen Hu",
      "Jiazheng Wang",
      "Jianhua Li",
      "Tiejun Huang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for 3D scenes, widely adopted due to its exceptional efficiency and high-fidelity visual quality. Given the significant value of 3DGS assets, recent works have introduced specialized watermarking schemes to ensure copyright protection and ownership verification. However, can existing 3D Gaussian watermarking approaches genuinely guarantee robust protection of the 3D assets? In this paper, for the first time, we systematically explore and validate possible vulnerabilities of 3DGS watermarking frameworks. We demonstrate that conventional watermark removal techniques designed for 2D images do not effectively generalize to the 3DGS scenario due to the specialized rendering pipeline and unique attributes of each gaussian primitives. Motivated by this insight, we propose GSPure, the first watermark purification framework specifically for 3DGS watermarking representations. By analyzing view-dependent rendering contributions and exploiting geometrically accurate feature clustering, GSPure precisely isolates and effectively removes watermark-related Gaussian primitives while preserving scene integrity. Extensive experiments demonstrate that our GSPure achieves the best watermark purification performance, reducing watermark PSNR by up to 16.34dB while minimizing degradation to original scene fidelity with less than 1dB PSNR loss. Moreover, it consistently outperforms existing methods in both effectiveness and generalization.",
    "arxiv_url": "https://arxiv.org/abs/2511.22262v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22262v1",
    "published_date": "2025-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "high-fidelity",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.22262v1",
      "pdf": "https://arxiv.org/pdf/2511.22262v1"
    },
    "bibtex": ""
  },
  {
    "title": "IE-SRGS: An Internal-External Knowledge Fusion Framework for High-Fidelity 3D Gaussian Splatting Super-Resolution",
    "authors": [
      "Xiang Feng",
      "Tieshi Zhong",
      "Shuo Chang",
      "Weiliu Wang",
      "Chengkai Wang",
      "Yifei Chen",
      "Yuhe Wang",
      "Zhenzhong Kuang",
      "Xuefei Yin",
      "Yanming Zhu"
    ],
    "abstract": "Reconstructing high-resolution (HR) 3D Gaussian Splatting (3DGS) models from low-resolution (LR) inputs remains challenging due to the lack of fine-grained textures and geometry. Existing methods typically rely on pre-trained 2D super-resolution (2DSR) models to enhance textures, but suffer from 3D Gaussian ambiguity arising from cross-view inconsistencies and domain gaps inherent in 2DSR models. We propose IE-SRGS, a novel 3DGS SR paradigm that addresses this issue by jointly leveraging the complementary strengths of external 2DSR priors and internal 3DGS features. Specifically, we use 2DSR and depth estimation models to generate HR images and depth maps as external knowledge, and employ multi-scale 3DGS models to produce cross-view consistent, domain-adaptive counterparts as internal knowledge. A mask-guided fusion strategy is introduced to integrate these two sources and synergistically exploit their complementary strengths, effectively guiding the 3D Gaussian optimization toward high-fidelity reconstruction. Extensive experiments on both synthetic and real-world benchmarks show that IE-SRGS consistently outperforms state-of-the-art methods in both quantitative accuracy and visual fidelity.",
    "arxiv_url": "https://arxiv.org/abs/2511.22233v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22233v1",
    "published_date": "2025-11-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "geometry",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.22233v1",
      "pdf": "https://arxiv.org/pdf/2511.22233v1"
    },
    "bibtex": ""
  },
  {
    "title": "3D-Consistent Multi-View Editing by Diffusion Guidance",
    "authors": [
      "Josef Bengtson",
      "David Nilsson",
      "Dong In Lee",
      "Fredrik Kahl"
    ],
    "abstract": "Recent advancements in diffusion models have greatly improved text-based image editing, yet methods that edit images independently often produce geometrically and photometrically inconsistent results across different views of the same scene. Such inconsistencies are particularly problematic for editing of 3D representations such as NeRFs or Gaussian Splat models. We propose a training-free diffusion framework that enforces multi-view consistency during the image editing process. The key assumption is that corresponding points in the unedited images should undergo similar transformations after editing. To achieve this, we introduce a consistency loss that guides the diffusion sampling toward coherent edits. The framework is flexible and can be combined with widely varying image editing methods, supporting both dense and sparse multi-view editing setups. Experimental results show that our approach significantly improves 3D consistency compared to existing multi-view editing methods. We also show that this increased consistency enables high-quality Gaussian Splat editing with sharp details and strong fidelity to user-specified text prompts. Please refer to our project page for video results: https://3d-consistent-editing.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2511.22228v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22228v1",
    "published_date": "2025-11-27",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.22228v1",
      "pdf": "https://arxiv.org/pdf/2511.22228v1",
      "project": "https://3d-consistent-editing.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "RemedyGS: Defend 3D Gaussian Splatting against Computation Cost Attacks",
    "authors": [
      "Yanping Li",
      "Zhening Liu",
      "Zijian Li",
      "Zehong Lin",
      "Jun Zhang"
    ],
    "abstract": "As a mainstream technique for 3D reconstruction, 3D Gaussian splatting (3DGS) has been applied in a wide range of applications and services. Recent studies have revealed critical vulnerabilities in this pipeline and introduced computation cost attacks that lead to malicious resource occupancies and even denial-of-service (DoS) conditions, thereby hindering the reliable deployment of 3DGS. In this paper, we propose the first effective and comprehensive black-box defense framework, named RemedyGS, against such computation cost attacks, safeguarding 3DGS reconstruction systems and services. Our pipeline comprises two key components: a detector to identify the attacked input images with poisoned textures and a purifier to recover the benign images from their attacked counterparts, mitigating the adverse effects of these attacks. Moreover, we incorporate adversarial training into the purifier to enforce distributional alignment between the recovered and original natural images, thereby enhancing the defense efficacy. Experimental results demonstrate that our framework effectively defends against white-box, black-box, and adaptive attacks in 3DGS systems, achieving state-of-the-art performance in both safety and utility.",
    "arxiv_url": "https://arxiv.org/abs/2511.22147v1",
    "pdf_url": "https://arxiv.org/pdf/2511.22147v1",
    "published_date": "2025-11-27",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.22147v1",
      "pdf": "https://arxiv.org/pdf/2511.22147v1"
    },
    "bibtex": ""
  },
  {
    "title": "EAST: Environment-Aware Stylized Transition Along the Reality-Virtuality Continuum",
    "authors": [
      "Xiaohan Zhang",
      "Kan Liu",
      "Yangle Liu",
      "Fengze Li",
      "Jieming Ma",
      "Yue Li"
    ],
    "abstract": "In the Virtual Reality (VR) gaming industry, maintaining immersion during real-world interruptions remains a challenge, particularly during transitions along the reality-virtuality continuum (RVC). Existing methods tend to rely on digital replicas or simple visual transitions, neglecting to address the aesthetic discontinuities between real and virtual environments, especially in highly stylized VR games. This paper introduces the Environment-Aware Stylized Transition (EAST) framework, which employs a novel style-transferred 3D Gaussian Splatting (3DGS) technique to transfer real-world interruptions into the virtual environment with seamless aesthetic consistency. Rather than merely transforming the real world into game-like visuals, EAST minimizes the disruptive impact of interruptions by integrating real-world elements within the framework. Qualitative user studies demonstrate significant enhancements in cognitive comfort and emotional continuity during transitions, while quantitative experiments highlight EAST's ability to maintain visual coherence across diverse VR styles.",
    "arxiv_url": "https://arxiv.org/abs/2511.22056v2",
    "pdf_url": "https://arxiv.org/pdf/2511.22056v2",
    "published_date": "2025-11-27",
    "categories": [
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.22056v2",
      "pdf": "https://arxiv.org/pdf/2511.22056v2"
    },
    "bibtex": ""
  },
  {
    "title": "Resolution Where It Counts: Hash-based GPU-Accelerated 3D Reconstruction via Variance-Adaptive Voxel Grids",
    "authors": [
      "Lorenzo De Rebotti",
      "Emanuele Giacomini",
      "Giorgio Grisetti",
      "Luca Di Giammarino"
    ],
    "abstract": "Efficient and scalable 3D surface reconstruction from range data remains a core challenge in computer graphics and vision, particularly in real-time and resource-constrained scenarios. Traditional volumetric methods based on fixed-resolution voxel grids or hierarchical structures like octrees often suffer from memory inefficiency, computational overhead, and a lack of GPU support. We propose a novel variance-adaptive, multi-resolution voxel grid that dynamically adjusts voxel size based on the local variance of signed distance field (SDF) observations. Unlike prior multi-resolution approaches that rely on recursive octree structures, our method leverages a flat spatial hash table to store all voxel blocks, supporting constant-time access and full GPU parallelism. This design enables high memory efficiency and real-time scalability. We further demonstrate how our representation supports GPU-accelerated rendering through a parallel quad-tree structure for Gaussian Splatting, enabling effective control over splat density. Our open-source CUDA/C++ implementation achieves up to 13x speedup and 4x lower memory usage compared to fixed-resolution baselines, while maintaining on par results in terms of reconstruction accuracy, offering a practical and extensible solution for high-performance 3D reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2511.21459v1",
    "pdf_url": "https://arxiv.org/pdf/2511.21459v1",
    "published_date": "2025-11-26",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "dynamic",
      "face",
      "3d reconstruction",
      "ar",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.21459v1",
      "pdf": "https://arxiv.org/pdf/2511.21459v1"
    },
    "bibtex": ""
  },
  {
    "title": "Endo-G$^{2}$T: Geometry-Guided & Temporally Aware Time-Embedded 4DGS For Endoscopic Scenes",
    "authors": [
      "Yangle Liu",
      "Fengze Li",
      "Kan Liu",
      "Jieming Ma"
    ],
    "abstract": "Endoscopic (endo) video exhibits strong view-dependent effects such as specularities, wet reflections, and occlusions. Pure photometric supervision misaligns with geometry and triggers early geometric drift, where erroneous shapes are reinforced during densification and become hard to correct. We ask how to anchor geometry early for 4D Gaussian splatting (4DGS) while maintaining temporal consistency and efficiency in dynamic endoscopic scenes. Thus, we present Endo-G$^{2}$T, a geometry-guided and temporally aware training scheme for time-embedded 4DGS. First, geo-guided prior distillation converts confidence-gated monocular depth into supervision with scale-invariant depth and depth-gradient losses, using a warm-up-to-cap schedule to inject priors softly and avoid early overfitting. Second, a time-embedded Gaussian field represents dynamics in XYZT with a rotor-like rotation parameterization, yielding temporally coherent geometry with lightweight regularization that favors smooth motion and crisp opacity boundaries. Third, keyframe-constrained streaming improves efficiency and long-horizon stability through keyframe-focused optimization under a max-points budget, while non-keyframes advance with lightweight updates. Across EndoNeRF and StereoMIS-P1 datasets, Endo-G$^{2}$T achieves state-of-the-art results among monocular reconstruction baselines.",
    "arxiv_url": "https://arxiv.org/abs/2511.21367v1",
    "pdf_url": "https://arxiv.org/pdf/2511.21367v1",
    "published_date": "2025-11-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "lightweight",
      "motion",
      "geometry",
      "ar",
      "nerf",
      "reflection",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.21367v1",
      "pdf": "https://arxiv.org/pdf/2511.21367v1"
    },
    "bibtex": ""
  },
  {
    "title": "Unlocking Zero-shot Potential of Semi-dense Image Matching via Gaussian Splatting",
    "authors": [
      "Juncheng Chen",
      "Chao Xu",
      "Yanjun Cao"
    ],
    "abstract": "Learning-based image matching critically depends on large-scale, diverse, and geometrically accurate training data. 3D Gaussian Splatting (3DGS) enables photorealistic novel-view synthesis and thus is attractive for data generation. However, its geometric inaccuracies and biased depth rendering currently prevent robust correspondence labeling. To address this, we introduce MatchGS, the first framework designed to systematically correct and leverage 3DGS for robust, zero-shot image matching. Our approach is twofold: (1) a geometrically-faithful data generation pipeline that refines 3DGS geometry to produce highly precise correspondence labels, enabling the synthesis of a vast and diverse range of viewpoints without compromising rendering fidelity; and (2) a 2D-3D representation alignment strategy that infuses 3DGS' explicit 3D knowledge into the 2D matcher, guiding 2D semi-dense matchers to learn viewpoint-invariant 3D representations. Our generated ground-truth correspondences reduce the epipolar error by up to 40 times compared to existing datasets, enable supervision under extreme viewpoint changes, and provide self-supervisory signals through Gaussian attributes. Consequently, state-of-the-art matchers trained solely on our data achieve significant zero-shot performance gains on public benchmarks, with improvements of up to 17.7%. Our work demonstrates that with proper geometric refinement, 3DGS can serve as a scalable, high-fidelity, and structurally-rich data source, paving the way for a new generation of robust zero-shot image matchers.",
    "arxiv_url": "https://arxiv.org/abs/2511.21265v1",
    "pdf_url": "https://arxiv.org/pdf/2511.21265v1",
    "published_date": "2025-11-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "geometry",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.21265v1",
      "pdf": "https://arxiv.org/pdf/2511.21265v1"
    },
    "bibtex": ""
  },
  {
    "title": "FaithFusion: Harmonizing Reconstruction and Generation via Pixel-wise Information Gain",
    "authors": [
      "YuAn Wang",
      "Xiaofan Li",
      "Chi Huang",
      "Wenhao Zhang",
      "Hao Li",
      "Bosheng Wang",
      "Xun Sun",
      "Jun Wang"
    ],
    "abstract": "In controllable driving-scene reconstruction and 3D scene generation, maintaining geometric fidelity while synthesizing visually plausible appearance under large viewpoint shifts is crucial. However, effective fusion of geometry-based 3DGS and appearance-driven diffusion models faces inherent challenges, as the absence of pixel-wise, 3D-consistent editing criteria often leads to over-restoration and geometric drift. To address these issues, we introduce \\textbf{FaithFusion}, a 3DGS-diffusion fusion framework driven by pixel-wise Expected Information Gain (EIG). EIG acts as a unified policy for coherent spatio-temporal synthesis: it guides diffusion as a spatial prior to refine high-uncertainty regions, while its pixel-level weighting distills the edits back into 3DGS. The resulting plug-and-play system is free from extra prior conditions and structural modifications.Extensive experiments on the Waymo dataset demonstrate that our approach attains SOTA performance across NTA-IoU, NTL-IoU, and FID, maintaining an FID of 107.47 even at 6 meters lane shift. Our code is available at https://github.com/wangyuanbiubiubiu/FaithFusion.",
    "arxiv_url": "https://arxiv.org/abs/2511.21113v1",
    "pdf_url": "https://arxiv.org/pdf/2511.21113v1",
    "published_date": "2025-11-26",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/wangyuanbiubiubiu/FaithFusion",
    "keywords": [
      "geometry",
      "face",
      "ar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.21113v1",
      "pdf": "https://arxiv.org/pdf/2511.21113v1",
      "github": "https://github.com/wangyuanbiubiubiu/FaithFusion"
    },
    "bibtex": ""
  },
  {
    "title": "Wanderland: Geometrically Grounded Simulation for Open-World Embodied AI",
    "authors": [
      "Xinhao Liu",
      "Jiaqi Li",
      "Youming Deng",
      "Ruxin Chen",
      "Yingjia Zhang",
      "Yifei Ma",
      "Li Guo",
      "Yiming Li",
      "Jing Zhang",
      "Chen Feng"
    ],
    "abstract": "Reproducible closed-loop evaluation remains a major bottleneck in Embodied AI such as visual navigation. A promising path forward is high-fidelity simulation that combines photorealistic sensor rendering with geometrically grounded interaction in complex, open-world urban environments. Although recent video-3DGS methods ease open-world scene capturing, they are still unsuitable for benchmarking due to large visual and geometric sim-to-real gaps. To address these challenges, we introduce Wanderland, a real-to-sim framework that features multi-sensor capture, reliable reconstruction, accurate geometry, and robust view synthesis. Using this pipeline, we curate a diverse dataset of indoor-outdoor urban scenes and systematically demonstrate how image-only pipelines scale poorly, how geometry quality impacts novel view synthesis, and how all of these adversely affect navigation policy learning and evaluation reliability. Beyond serving as a trusted testbed for embodied navigation, Wanderland's rich raw sensor data further allows benchmarking of 3D reconstruction and novel view synthesis models. Our work establishes a new foundation for reproducible research in open-world embodied AI. Project website is at https://ai4ce.github.io/wanderland/.",
    "arxiv_url": "https://arxiv.org/abs/2511.20620v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20620v1",
    "published_date": "2025-11-25",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "urban scene",
      "high-fidelity",
      "3d reconstruction",
      "geometry",
      "ar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.20620v1",
      "pdf": "https://arxiv.org/pdf/2511.20620v1",
      "project": "https://ai4ce.github.io/wanderland"
    },
    "bibtex": ""
  },
  {
    "title": "GS-Checker: Tampering Localization for 3D Gaussian Splatting",
    "authors": [
      "Haoliang Han",
      "Ziyuan Luo",
      "Jun Qi",
      "Anderson Rocha",
      "Renjie Wan"
    ],
    "abstract": "Recent advances in editing technologies for 3D Gaussian Splatting (3DGS) have made it simple to manipulate 3D scenes. However, these technologies raise concerns about potential malicious manipulation of 3D content. To avoid such malicious applications, localizing tampered regions becomes crucial. In this paper, we propose GS-Checker, a novel method for locating tampered areas in 3DGS models. Our approach integrates a 3D tampering attribute into the 3D Gaussian parameters to indicate whether the Gaussian has been tampered. Additionally, we design a 3D contrastive mechanism by comparing the similarity of key attributes between 3D Gaussians to seek tampering cues at 3D level. Furthermore, we introduce a cyclic optimization strategy to refine the 3D tampering attribute, enabling more accurate tampering localization. Notably, our approach does not require expensive 3D labels for supervision. Extensive experimental results demonstrate the effectiveness of our proposed method to locate the tampered 3DGS area.",
    "arxiv_url": "https://arxiv.org/abs/2511.20354v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20354v1",
    "published_date": "2025-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "localization",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.20354v1",
      "pdf": "https://arxiv.org/pdf/2511.20354v1"
    },
    "bibtex": ""
  },
  {
    "title": "Material-informed Gaussian Splatting for 3D World Reconstruction in a Digital Twin",
    "authors": [
      "Andy Huynh",
      "João Malheiro Silva",
      "Holger Caesar",
      "Tong Duy Son"
    ],
    "abstract": "3D reconstruction for Digital Twins often relies on LiDAR-based methods, which provide accurate geometry but lack the semantics and textures naturally captured by cameras. Traditional LiDAR-camera fusion approaches require complex calibration and still struggle with certain materials like glass, which are visible in images but poorly represented in point clouds. We propose a camera-only pipeline that reconstructs scenes using 3D Gaussian Splatting from multi-view images, extracts semantic material masks via vision models, converts Gaussian representations to mesh surfaces with projected material labels, and assigns physics-based material properties for accurate sensor simulation in modern graphics engines and simulators. This approach combines photorealistic reconstruction with physics-based material assignment, providing sensor simulation fidelity comparable to LiDAR-camera fusion while eliminating hardware complexity and calibration requirements. We validate our camera-only method using an internal dataset from an instrumented test vehicle, leveraging LiDAR as ground truth for reflectivity validation alongside image similarity metrics.",
    "arxiv_url": "https://arxiv.org/abs/2511.20348v3",
    "pdf_url": "https://arxiv.org/pdf/2511.20348v3",
    "published_date": "2025-11-25",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "3d reconstruction",
      "geometry",
      "ar",
      "semantic",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.20348v3",
      "pdf": "https://arxiv.org/pdf/2511.20348v3"
    },
    "bibtex": ""
  },
  {
    "title": "GigaWorld-0: World Models as Data Engine to Empower Embodied AI",
    "authors": [
      "GigaWorld Team",
      "Angen Ye",
      "Boyuan Wang",
      "Chaojun Ni",
      "Guan Huang",
      "Guosheng Zhao",
      "Haoyun Li",
      "Jiagang Zhu",
      "Kerui Li",
      "Mengyuan Xu",
      "Qiuping Deng",
      "Siting Wang",
      "Wenkang Qin",
      "Xinze Chen",
      "Xiaofeng Wang",
      "Yankai Wang",
      "Yu Cao",
      "Yifan Chang",
      "Yuan Xu",
      "Yun Ye",
      "Yang Wang",
      "Yukun Zhou",
      "Zhengyuan Zhang",
      "Zhehao Dong",
      "Zheng Zhu"
    ],
    "abstract": "World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training.",
    "arxiv_url": "https://arxiv.org/abs/2511.19861v2",
    "pdf_url": "https://arxiv.org/pdf/2511.19861v2",
    "published_date": "2025-11-25",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "semantic",
      "3d gaussian",
      "efficient",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.19861v2",
      "pdf": "https://arxiv.org/pdf/2511.19861v2"
    },
    "bibtex": ""
  },
  {
    "title": "STAvatar: Soft Binding and Temporal Density Control for Monocular 3D Head Avatars Reconstruction",
    "authors": [
      "Jiankuo Zhao",
      "Xiangyu Zhu",
      "Zidu Wang",
      "Zhen Lei"
    ],
    "abstract": "Reconstructing high-fidelity and animatable 3D head avatars from monocular videos remains a challenging yet essential task. Existing methods based on 3D Gaussian Splatting typically bind Gaussians to mesh triangles and model deformations solely via Linear Blend Skinning, which results in rigid motion and limited expressiveness. Moreover, they lack specialized strategies to handle frequently occluded regions (e.g., mouth interiors, eyelids). To address these limitations, we propose STAvatar, which consists of two key components: (1) a UV-Adaptive Soft Binding framework that leverages both image-based and geometric priors to learn per-Gaussian feature offsets within the UV space. This UV representation supports dynamic resampling, ensuring full compatibility with Adaptive Density Control (ADC) and enhanced adaptability to shape and textural variations. (2) a Temporal ADC strategy, which first clusters structurally similar frames to facilitate more targeted computation of the densification criterion. It further introduces a novel fused perceptual error as clone criterion to jointly capture geometric and textural discrepancies, encouraging densification in regions requiring finer details. Extensive experiments on four benchmark datasets demonstrate that STAvatar achieves state-of-the-art reconstruction performance, especially in capturing fine-grained details and reconstructing frequently occluded regions. The code will be publicly available.",
    "arxiv_url": "https://arxiv.org/abs/2511.19854v2",
    "pdf_url": "https://arxiv.org/pdf/2511.19854v2",
    "published_date": "2025-11-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "dynamic",
      "high-fidelity",
      "deformation",
      "avatar",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.19854v2",
      "pdf": "https://arxiv.org/pdf/2511.19854v2"
    },
    "bibtex": ""
  },
  {
    "title": "DensifyBeforehand: LiDAR-assisted Content-aware Densification for Efficient and Quality 3D Gaussian Splatting",
    "authors": [
      "Phurtivilai Patt",
      "Leyang Huang",
      "Yinqiang Zhang",
      "Yang Lei"
    ],
    "abstract": "This paper addresses the limitations of existing 3D Gaussian Splatting (3DGS) methods, particularly their reliance on adaptive density control, which can lead to floating artifacts and inefficient resource usage. We propose a novel densify beforehand approach that enhances the initialization of 3D scenes by combining sparse LiDAR data with monocular depth estimation from corresponding RGB images. Our ROI-aware sampling scheme prioritizes semantically and geometrically important regions, yielding a dense point cloud that improves visual fidelity and computational efficiency. This densify beforehand approach bypasses the adaptive density control that may introduce redundant Gaussians in the original pipeline, allowing the optimization to focus on the other attributes of 3D Gaussian primitives, reducing overlap while enhancing visual quality. Our method achieves comparable results to state-of-the-art techniques while significantly lowering resource consumption and training time. We validate our approach through extensive comparisons and ablation studies on four newly collected datasets, showcasing its effectiveness in preserving regions of interest in complex scenes.",
    "arxiv_url": "https://arxiv.org/abs/2511.19294v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19294v1",
    "published_date": "2025-11-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "semantic",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.19294v1",
      "pdf": "https://arxiv.org/pdf/2511.19294v1"
    },
    "bibtex": ""
  },
  {
    "title": "IDSplat: Instance-Decomposed 3D Gaussian Splatting for Driving Scenes",
    "authors": [
      "Carl Lindström",
      "Mahan Rafidashti",
      "Maryam Fatemi",
      "Lars Hammarstrand",
      "Martin R. Oswald",
      "Lennart Svensson"
    ],
    "abstract": "Reconstructing dynamic driving scenes is essential for developing autonomous systems through sensor-realistic simulation. Although recent methods achieve high-fidelity reconstructions, they either rely on costly human annotations for object trajectories or use time-varying representations without explicit object-level decomposition, leading to intertwined static and dynamic elements that hinder scene separation. We present IDSplat, a self-supervised 3D Gaussian Splatting framework that reconstructs dynamic scenes with explicit instance decomposition and learnable motion trajectories, without requiring human annotations. Our key insight is to model dynamic objects as coherent instances undergoing rigid transformations, rather than unstructured time-varying primitives. For instance decomposition, we employ zero-shot, language-grounded video tracking anchored to 3D using lidar, and estimate consistent poses via feature correspondences. We introduce a coordinated-turn smoothing scheme to obtain temporally and physically consistent motion trajectories, mitigating pose misalignments and tracking failures, followed by joint optimization of object poses and Gaussian parameters. Experiments on the Waymo Open Dataset demonstrate that our method achieves competitive reconstruction quality while maintaining instance-level decomposition and generalizes across diverse sequences and view densities without retraining, making it practical for large-scale autonomous driving applications. Code will be released.",
    "arxiv_url": "https://arxiv.org/abs/2511.19235v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19235v1",
    "published_date": "2025-11-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "high-fidelity",
      "tracking",
      "gaussian splatting",
      "autonomous driving",
      "ar",
      "human",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.19235v1",
      "pdf": "https://arxiv.org/pdf/2511.19235v1"
    },
    "bibtex": ""
  },
  {
    "title": "NVGS: Neural Visibility for Occlusion Culling in 3D Gaussian Splatting",
    "authors": [
      "Brent Zoomers",
      "Florian Hahlbohm",
      "Joni Vanherck",
      "Lode Jorissen",
      "Marcus Magnor",
      "Nick Michiels"
    ],
    "abstract": "3D Gaussian Splatting can exploit frustum culling and level-of-detail strategies to accelerate rendering of scenes containing a large number of primitives. However, the semi-transparent nature of Gaussians prevents the application of another highly effective technique: occlusion culling. We address this limitation by proposing a novel method to learn the viewpoint-dependent visibility function of all Gaussians in a trained model using a small, shared MLP across instances of an asset in a scene. By querying it for Gaussians within the viewing frustum prior to rasterization, our method can discard occluded primitives during rendering. Leveraging Tensor Cores for efficient computation, we integrate these neural queries directly into a novel instanced software rasterizer. Our approach outperforms the current state of the art for composed scenes in terms of VRAM usage and image quality, utilizing a combination of our instanced rasterizer and occlusion culling MLP, and exhibits complementary properties to existing LoD techniques.",
    "arxiv_url": "https://arxiv.org/abs/2511.19202v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19202v1",
    "published_date": "2025-11-24",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.19202v1",
      "pdf": "https://arxiv.org/pdf/2511.19202v1"
    },
    "bibtex": ""
  },
  {
    "title": "AvatarBrush: Monocular Reconstruction of Gaussian Avatars with Intuitive Local Editing",
    "authors": [
      "Mengtian Li",
      "Shengxiang Yao",
      "Yichen Pan",
      "Haiyao Xiao",
      "Zhongmei Li",
      "Zhifeng Xie",
      "Keyu Chen"
    ],
    "abstract": "The efficient reconstruction of high-quality and intuitively editable human avatars presents a pressing challenge in the field of computer vision. Recent advancements, such as 3DGS, have demonstrated impressive reconstruction efficiency and rapid rendering speeds. However, intuitive local editing of these representations remains a significant challenge. In this work, we propose AvatarBrush, a framework that reconstructs fully animatable and locally editable avatars using only a monocular video input. We propose a three-layer model to represent the avatar and, inspired by mesh morphing techniques, design a framework to generate the Gaussian model from local information of the parametric body model. Compared to previous methods that require scanned meshes or multi-view captures as input, our approach reduces costs and enhances editing capabilities such as body shape adjustment, local texture modification, and geometry transfer. Our experimental results demonstrate superior quality across two datasets and emphasize the enhanced, user-friendly, and localized editing capabilities of our method.",
    "arxiv_url": "https://arxiv.org/abs/2511.19189v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19189v1",
    "published_date": "2025-11-24",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "body",
      "avatar",
      "geometry",
      "ar",
      "human",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.19189v1",
      "pdf": "https://arxiv.org/pdf/2511.19189v1"
    },
    "bibtex": ""
  },
  {
    "title": "MetroGS: Efficient and Stable Reconstruction of Geometrically Accurate High-Fidelity Large-Scale Scenes",
    "authors": [
      "Kehua Chen",
      "Tianlu Mao",
      "Zhuxin Ma",
      "Hao Jiang",
      "Zehao Li",
      "Zihan Liu",
      "Shuqi Gao",
      "Honglong Zhao",
      "Feng Dai",
      "Yucheng Zhang",
      "Zhaoqi Wang"
    ],
    "abstract": "Recently, 3D Gaussian Splatting and its derivatives have achieved significant breakthroughs in large-scale scene reconstruction. However, how to efficiently and stably achieve high-quality geometric fidelity remains a core challenge. To address this issue, we introduce MetroGS, a novel Gaussian Splatting framework for efficient and robust reconstruction in complex urban environments. Our method is built upon a distributed 2D Gaussian Splatting representation as the core foundation, serving as a unified backbone for subsequent modules. To handle potential sparse regions in complex scenes, we propose a structured dense enhancement scheme that utilizes SfM priors and a pointmap model to achieve a denser initialization, while incorporating a sparsity compensation mechanism to improve reconstruction completeness. Furthermore, we design a progressive hybrid geometric optimization strategy that organically integrates monocular and multi-view optimization to achieve efficient and accurate geometric refinement. Finally, to address the appearance inconsistency commonly observed in large-scale scenes, we introduce a depth-guided appearance modeling approach that learns spatial features with 3D consistency, facilitating effective decoupling between geometry and appearance and further enhancing reconstruction stability. Experiments on large-scale urban datasets demonstrate that MetroGS achieves superior geometric accuracy, rendering quality, offering a unified solution for high-fidelity large-scale scene reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2511.19172v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19172v1",
    "published_date": "2025-11-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "geometry",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.19172v1",
      "pdf": "https://arxiv.org/pdf/2511.19172v1"
    },
    "bibtex": ""
  },
  {
    "title": "Proxy-Free Gaussian Splats Deformation with Splat-Based Surface Estimation",
    "authors": [
      "Jaeyeong Kim",
      "Seungwoo Yoo",
      "Minhyuk Sung"
    ],
    "abstract": "We introduce SpLap, a proxy-free deformation method for Gaussian splats (GS) based on a Laplacian operator computed from our novel surface-aware splat graph. Existing approaches to GS deformation typically rely on deformation proxies such as cages or meshes, but they suffer from dependency on proxy quality and additional computational overhead. An alternative is to directly apply Laplacian-based deformation techniques by treating splats as point clouds. However, this often fail to properly capture surface information due to lack of explicit structure. To address this, we propose a novel method that constructs a surface-aware splat graph, enabling the Laplacian operator derived from it to support more plausible deformations that preserve details and topology. Our key idea is to leverage the spatial arrangement encoded in splats, defining neighboring splats not merely by the distance between their centers, but by their intersections. Furthermore, we introduce a Gaussian kernel adaptation technique that preserves surface structure under deformation, thereby improving rendering quality after deformation. In our experiments, we demonstrate the superior performance of our method compared to both proxy-based and proxy-free baselines, evaluated on 50 challenging objects from the ShapeNet, Objaverse, and Sketchfab datasets, as well as the NeRF-Synthetic dataset. Code is available at https://github.com/kjae0/SpLap.",
    "arxiv_url": "https://arxiv.org/abs/2511.19542v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19542v1",
    "published_date": "2025-11-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/kjae0/SpLap",
    "keywords": [
      "head",
      "face",
      "deformation",
      "ar",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.19542v1",
      "pdf": "https://arxiv.org/pdf/2511.19542v1",
      "github": "https://github.com/kjae0/SpLap"
    },
    "bibtex": ""
  },
  {
    "title": "Neural Texture Splatting: Expressive 3D Gaussian Splatting for View Synthesis, Geometry, and Dynamic Reconstruction",
    "authors": [
      "Yiming Wang",
      "Shaofei Wang",
      "Marko Mihajlovic",
      "Siyu Tang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a leading approach for high-quality novel view synthesis, with numerous variants extending its applicability to a broad spectrum of 3D and 4D scene reconstruction tasks. Despite its success, the representational capacity of 3DGS remains limited by the use of 3D Gaussian kernels to model local variations. Recent works have proposed to augment 3DGS with additional per-primitive capacity, such as per-splat textures, to enhance its expressiveness. However, these per-splat texture approaches primarily target dense novel view synthesis with a reduced number of Gaussian primitives, and their effectiveness tends to diminish when applied to more general reconstruction scenarios. In this paper, we aim to achieve concrete performance improvement over state-of-the-art 3DGS variants across a wide range of reconstruction tasks, including novel view synthesis, geometry and dynamic reconstruction, under both sparse and dense input settings. To this end, we introduce Neural Texture Splatting (NTS). At the core of our approach is a global neural field (represented as a hybrid of a tri-plane and a neural decoder) that predicts local appearance and geometric fields for each primitive. By leveraging this shared global representation that models local texture fields across primitives, we significantly reduce model size and facilitate efficient global information exchange, demonstrating strong generalization across tasks. Furthermore, our neural modeling of local texture fields introduces expressive view- and time-dependent effects, a critical aspect that existing methods fail to account for. Extensive experiments show that Neural Texture Splatting consistently improves models and achieves state-of-the-art results across multiple benchmarks.",
    "arxiv_url": "https://arxiv.org/abs/2511.18873v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18873v1",
    "published_date": "2025-11-24",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "geometry",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.18873v1",
      "pdf": "https://arxiv.org/pdf/2511.18873v1"
    },
    "bibtex": ""
  },
  {
    "title": "Splatonic: Architecture Support for 3D Gaussian Splatting SLAM via Sparse Processing",
    "authors": [
      "Xiaotong Huang",
      "He Zhu",
      "Tianrui Ma",
      "Yuxiang Xiong",
      "Fangxin Liu",
      "Zhezhi He",
      "Yiming Gan",
      "Zihan Liu",
      "Jingwen Leng",
      "Yu Feng",
      "Minyi Guo"
    ],
    "abstract": "3D Gaussian splatting (3DGS) has emerged as a promising direction for SLAM due to its high-fidelity reconstruction and rapid convergence. However, 3DGS-SLAM algorithms remain impractical for mobile platforms due to their high computational cost, especially for their tracking process.   This work introduces Splatonic, a sparse and efficient real-time 3DGS-SLAM algorithm-hardware co-design for resource-constrained devices. Inspired by classical SLAMs, we propose an adaptive sparse pixel sampling algorithm that reduces the number of rendered pixels by up to 256$\\times$ while retaining accuracy. To unlock this performance potential on mobile GPUs, we design a novel pixel-based rendering pipeline that improves hardware utilization via Gaussian-parallel rendering and preemptive $α$-checking. Together, these optimizations yield up to 121.7$\\times$ speedup on the bottleneck stages and 14.6$\\times$ end-to-end speedup on off-the-shelf GPUs. To further address new bottlenecks introduced by our rendering pipeline, we propose a pipelined architecture that simplifies the overall design while addressing newly emerged bottlenecks in projection and aggregation. Evaluated across four 3DGS-SLAM algorithms, Splatonic achieves up to 274.9$\\times$ speedup and 4738.5$\\times$ energy savings over mobile GPUs and up to 25.2$\\times$ speedup and 241.1$\\times$ energy savings over state-of-the-art accelerators, all with comparable accuracy.",
    "arxiv_url": "https://arxiv.org/abs/2511.18755v2",
    "pdf_url": "https://arxiv.org/pdf/2511.18755v2",
    "published_date": "2025-11-24",
    "categories": [
      "cs.AR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "tracking",
      "ar",
      "3d gaussian",
      "slam",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.18755v2",
      "pdf": "https://arxiv.org/pdf/2511.18755v2"
    },
    "bibtex": ""
  },
  {
    "title": "NeAR: Coupled Neural Asset-Renderer Stack",
    "authors": [
      "Hong Li",
      "Chongjie Ye",
      "Houyuan Chen",
      "Weiqing Xiao",
      "Ziyang Yan",
      "Lixing Xiao",
      "Zhaoxi Chen",
      "Jianfeng Xiang",
      "Shaocong Xu",
      "Xuhui Liu",
      "Yikai Wang",
      "Baochang Zhang",
      "Xiaoguang Han",
      "Jiaolong Yang",
      "Hao Zhao"
    ],
    "abstract": "Neural asset authoring and neural rendering have traditionally evolved as disjoint paradigms: one generates digital assets for fixed graphics pipelines, while the other maps conventional assets to images. However, treating them as independent entities limits the potential for end-to-end optimization in fidelity and consistency. In this paper, we bridge this gap with NeAR, a Coupled Neural Asset--Renderer Stack. We argue that co-designing the asset representation and the renderer creates a robust \"contract\" for superior generation. On the asset side, we introduce the Lighting-Homogenized SLAT (LH-SLAT). Leveraging a rectified-flow model, NeAR lifts casually lit single images into a canonical, illumination-invariant latent space, effectively suppressing baked-in shadows and highlights. On the renderer side, we design a lighting-aware neural decoder tailored to interpret these homogenized latents. Conditioned on HDR environment maps and camera views, it synthesizes relightable 3D Gaussian splats in real-time without per-object optimization. We validate NeAR on four tasks: (1) G-buffer-based forward rendering, (2) random-lit reconstruction, (3) unknown-lit relighting, and (4) novel-view relighting. Extensive experiments demonstrate that our coupled stack outperforms state-of-the-art baselines in both quantitative metrics and perceptual quality. We hope this coupled asset-renderer perspective inspires future graphics stacks that view neural assets and renderers as co-designed components instead of independent entities.",
    "arxiv_url": "https://arxiv.org/abs/2511.18600v2",
    "pdf_url": "https://arxiv.org/pdf/2511.18600v2",
    "published_date": "2025-11-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "illumination",
      "shadow",
      "neural rendering",
      "ar",
      "relightable",
      "3d gaussian",
      "lighting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.18600v2",
      "pdf": "https://arxiv.org/pdf/2511.18600v2"
    },
    "bibtex": ""
  },
  {
    "title": "PhysGS: Bayesian-Inferred Gaussian Splatting for Physical Property Estimation",
    "authors": [
      "Samarth Chopra",
      "Jing Liang",
      "Gershom Seneviratne",
      "Dinesh Manocha"
    ],
    "abstract": "Understanding physical properties such as friction, stiffness, hardness, and material composition is essential for enabling robots to interact safely and effectively with their surroundings. However, existing 3D reconstruction methods focus on geometry and appearance and cannot infer these underlying physical properties. We present PhysGS, a Bayesian-inferred extension of 3D Gaussian Splatting that estimates dense, per-point physical properties from visual cues and vision--language priors. We formulate property estimation as Bayesian inference over Gaussian splats, where material and property beliefs are iteratively refined as new observations arrive. PhysGS also models aleatoric and epistemic uncertainties, enabling uncertainty-aware object and scene interpretation. Across object-scale (ABO-500), indoor, and outdoor real-world datasets, PhysGS improves accuracy of the mass estimation by up to 22.8%, reduces Shore hardness error by up to 61.2%, and lowers kinetic friction error by up to 18.1% compared to deterministic baselines. Our results demonstrate that PhysGS unifies 3D reconstruction, uncertainty modeling, and physical reasoning in a single, spatially continuous framework for dense physical property estimation. Additional results are available at https://samchopra2003.github.io/physgs.",
    "arxiv_url": "https://arxiv.org/abs/2511.18570v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18570v1",
    "published_date": "2025-11-23",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "understanding",
      "3d reconstruction",
      "geometry",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.18570v1",
      "pdf": "https://arxiv.org/pdf/2511.18570v1",
      "project": "https://samchopra2003.github.io/physgs"
    },
    "bibtex": ""
  },
  {
    "title": "Splatblox: Traversability-Aware Gaussian Splatting for Outdoor Robot Navigation",
    "authors": [
      "Samarth Chopra",
      "Jing Liang",
      "Gershom Seneviratne",
      "Yonghan Lee",
      "Jaehoon Choi",
      "Jianyu An",
      "Stephen Cheng",
      "Dinesh Manocha"
    ],
    "abstract": "We present Splatblox, a real-time system for autonomous navigation in outdoor environments with dense vegetation, irregular obstacles, and complex terrain. Our method fuses segmented RGB images and LiDAR point clouds using Gaussian Splatting to construct a traversability-aware Euclidean Signed Distance Field (ESDF) that jointly encodes geometry and semantics. Updated online, this field enables semantic reasoning to distinguish traversable vegetation (e.g., tall grass) from rigid obstacles (e.g., trees), while LiDAR ensures 360-degree geometric coverage for extended planning horizons. We validate Splatblox on a quadruped robot and demonstrate transfer to a wheeled platform. In field trials across vegetation-rich scenarios, it outperforms state-of-the-art methods with over 50% higher success rate, 40% fewer freezing incidents, 5% shorter paths, and up to 13% faster time to goal, while supporting long-range missions up to 100 meters. Experiment videos and more details can be found on our project page: https://splatblox.github.io",
    "arxiv_url": "https://arxiv.org/abs/2511.18525v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18525v1",
    "published_date": "2025-11-23",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "geometry",
      "ar",
      "semantic",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.18525v1",
      "pdf": "https://arxiv.org/pdf/2511.18525v1",
      "project": "https://splatblox.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "ReCoGS: Real-time ReColoring for Gaussian Splatting scenes",
    "authors": [
      "Lorenzo Rutayisire",
      "Nicola Capodieci",
      "Fabio Pellacini"
    ],
    "abstract": "Gaussian Splatting has emerged as a leading method for novel view synthesis, offering superior training efficiency and real-time inference compared to NeRF approaches, while still delivering high-quality reconstructions. Beyond view synthesis, this 3D representation has also been explored for editing tasks. Many existing methods leverage 2D diffusion models to generate multi-view datasets for training, but they often suffer from limitations such as view inconsistencies, lack of fine-grained control, and high computational demand. In this work, we focus specifically on the editing task of recoloring. We introduce a user-friendly pipeline that enables precise selection and recoloring of regions within a pre-trained Gaussian Splatting scene. To demonstrate the real-time performance of our method, we also present an interactive tool that allows users to experiment with the pipeline in practice. Code is available at https://github.com/loryruta/recogs.",
    "arxiv_url": "https://arxiv.org/abs/2511.18441v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18441v1",
    "published_date": "2025-11-23",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "https://github.com/loryruta/recogs",
    "keywords": [
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.18441v1",
      "pdf": "https://arxiv.org/pdf/2511.18441v1",
      "github": "https://github.com/loryruta/recogs"
    },
    "bibtex": ""
  },
  {
    "title": "SegSplat: Feed-forward Gaussian Splatting and Open-Set Semantic Segmentation",
    "authors": [
      "Peter Siegel",
      "Federico Tombari",
      "Marc Pollefeys",
      "Daniel Barath"
    ],
    "abstract": "We have introduced SegSplat, a novel framework designed to bridge the gap between rapid, feed-forward 3D reconstruction and rich, open-vocabulary semantic understanding. By constructing a compact semantic memory bank from multi-view 2D foundation model features and predicting discrete semantic indices alongside geometric and appearance attributes for each 3D Gaussian in a single pass, SegSplat efficiently imbues scenes with queryable semantics. Our experiments demonstrate that SegSplat achieves geometric fidelity comparable to state-of-the-art feed-forward 3D Gaussian Splatting methods while simultaneously enabling robust open-set semantic segmentation, crucially \\textit{without} requiring any per-scene optimization for semantic feature integration. This work represents a significant step towards practical, on-the-fly generation of semantically aware 3D environments, vital for advancing robotic interaction, augmented reality, and other intelligent systems.",
    "arxiv_url": "https://arxiv.org/abs/2511.18386v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18386v1",
    "published_date": "2025-11-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "understanding",
      "3d reconstruction",
      "segmentation",
      "ar",
      "semantic",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.18386v1",
      "pdf": "https://arxiv.org/pdf/2511.18386v1"
    },
    "bibtex": ""
  },
  {
    "title": "Alias-free 4D Gaussian Splatting",
    "authors": [
      "Zilong Chen",
      "Huan-ang Gao",
      "Delin Qu",
      "Haohan Chi",
      "Hao Tang",
      "Kai Zhang",
      "Hao Zhao"
    ],
    "abstract": "Existing dynamic scene reconstruction methods based on Gaussian Splatting enable real-time rendering and generate realistic images. However, adjusting the camera's focal length or the distance between Gaussian primitives and the camera to modify rendering resolution often introduces strong artifacts, stemming from the frequency constraints of 4D Gaussians and Gaussian scale mismatch induced by the 2D dilated filter. To address this, we derive a maximum sampling frequency formulation for 4D Gaussian Splatting and introduce a 4D scale-adaptive filter and scale loss, which flexibly regulates the sampling frequency of 4D Gaussian Splatting. Our approach eliminates high-frequency artifacts under increased rendering frequencies while effectively reducing redundant Gaussians in multi-view video reconstruction. We validate the proposed method through monocular and multi-view video reconstruction experiments.Ours project page: https://4d-alias-free.github.io/4D-Alias-free/",
    "arxiv_url": "https://arxiv.org/abs/2511.18367v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18367v1",
    "published_date": "2025-11-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "real-time rendering",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.18367v1",
      "pdf": "https://arxiv.org/pdf/2511.18367v1",
      "project": "https://4d-alias-free.github.io/4D-Alias-free"
    },
    "bibtex": ""
  },
  {
    "title": "Observer Actor: Active Vision Imitation Learning with Sparse View Gaussian Splatting",
    "authors": [
      "Yilong Wang",
      "Cheng Qian",
      "Ruomeng Fan",
      "Edward Johns"
    ],
    "abstract": "We propose Observer Actor (ObAct), a novel framework for active vision imitation learning in which the observer moves to optimal visual observations for the actor. We study ObAct on a dual-arm robotic system equipped with wrist-mounted cameras. At test time, ObAct dynamically assigns observer and actor roles: the observer arm constructs a 3D Gaussian Splatting (3DGS) representation from three images, virtually explores this to find an optimal camera pose, then moves to this pose; the actor arm then executes a policy using the observer's observations. This formulation enhances the clarity and visibility of both the object and the gripper in the policy's observations. As a result, we enable the training of ambidextrous policies on observations that remain closer to the occlusion-free training distribution, leading to more robust policies. We study this formulation with two existing imitation learning methods -- trajectory transfer and behavior cloning -- and experiments show that ObAct significantly outperforms static-camera setups: trajectory transfer improves by 145% without occlusion and 233% with occlusion, while behavior cloning improves by 75% and 143%, respectively. Videos are available at https://obact.github.io.",
    "arxiv_url": "https://arxiv.org/abs/2511.18140v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18140v1",
    "published_date": "2025-11-22",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "sparse view",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.18140v1",
      "pdf": "https://arxiv.org/pdf/2511.18140v1",
      "project": "https://obact.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "RoboArmGS: High-Quality Robotic Arm Splatting via Bézier Curve Refinement",
    "authors": [
      "Hao Wang",
      "Xiaobao Wei",
      "Ying Li",
      "Qingpo Wuwu",
      "Dongli Wu",
      "Jiajun Cao",
      "Ming Lu",
      "Wenzhao Zheng",
      "Shanghang Zhang"
    ],
    "abstract": "Constructing photorealistic and controllable robotic arm digital assets from real observations is fundamental to robotic applications. Current approaches naively bind static 3D Gaussians according to URDF links, forcing them to follow an URDF-rigged motion passively. However, the idealized URDF-rigged motion cannot accurately model the actual motion captured in real-world observations, leading to severe rendering artifacts in 3D Gaussians. To address these challenges, we propose RoboArmGS, a novel hybrid representation that refines the URDF-rigged motion with learnable Bézier curves, enabling more accurate real-world motion modeling. To be more specific, we present a learnable Bézier Curve motion refiner that corrects per-joint residuals to address mismatches between real-world motion and URDF-rigged motion. RoboArmGS enables the learning of more accurate real-world motion while achieving a coherent binding of 3D Gaussians across arm parts. To support future research, we contribute a carefully collected dataset named RoboArm4D, which comprises several widely used robotic arms for evaluating the quality of building high-quality digital assets. We evaluate our approach on RoboArm4D, and RoboArmGS achieves state-of-the-art performance in real-world motion modeling and rendering quality. The code and dataset will be released.",
    "arxiv_url": "https://arxiv.org/abs/2511.17961v2",
    "pdf_url": "https://arxiv.org/pdf/2511.17961v2",
    "published_date": "2025-11-22",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "3d gaussian",
      "ar",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.17961v2",
      "pdf": "https://arxiv.org/pdf/2511.17961v2"
    },
    "bibtex": ""
  },
  {
    "title": "Novel View Synthesis from A Few Glimpses via Test-Time Natural Video Completion",
    "authors": [
      "Yan Xu",
      "Yixing Wang",
      "Stella X. Yu"
    ],
    "abstract": "Given just a few glimpses of a scene, can you imagine the movie playing out as the camera glides through it? That's the lens we take on \\emph{sparse-input novel view synthesis}, not only as filling spatial gaps between widely spaced views, but also as \\emph{completing a natural video} unfolding through space.   We recast the task as \\emph{test-time natural video completion}, using powerful priors from \\emph{pretrained video diffusion models} to hallucinate plausible in-between views. Our \\emph{zero-shot, generation-guided} framework produces pseudo views at novel camera poses, modulated by an \\emph{uncertainty-aware mechanism} for spatial coherence. These synthesized frames densify supervision for \\emph{3D Gaussian Splatting} (3D-GS) for scene reconstruction, especially in under-observed regions. An iterative feedback loop lets 3D geometry and 2D view synthesis inform each other, improving both the scene reconstruction and the generated views.   The result is coherent, high-fidelity renderings from sparse inputs \\emph{without any scene-specific training or fine-tuning}. On LLFF, DTU, DL3DV, and MipNeRF-360, our method significantly outperforms strong 3D-GS baselines under extreme sparsity.",
    "arxiv_url": "https://arxiv.org/abs/2511.17932v1",
    "pdf_url": "https://arxiv.org/pdf/2511.17932v1",
    "published_date": "2025-11-22",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "geometry",
      "ar",
      "nerf",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.17932v1",
      "pdf": "https://arxiv.org/pdf/2511.17932v1"
    },
    "bibtex": ""
  },
  {
    "title": "Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization",
    "authors": [
      "Youngsik Yun",
      "Dongjun Gu",
      "Youngjung Uh"
    ],
    "abstract": "Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR.",
    "arxiv_url": "https://arxiv.org/abs/2511.17918v1",
    "pdf_url": "https://arxiv.org/pdf/2511.17918v1",
    "published_date": "2025-11-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "few-shot",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.17918v1",
      "pdf": "https://arxiv.org/pdf/2511.17918v1",
      "project": "https://bbangsik13.github.io/FASR"
    },
    "bibtex": ""
  },
  {
    "title": "CUS-GS: A Compact Unified Structured Gaussian Splatting Framework for Multimodal Scene Representation",
    "authors": [
      "Yuhang Ming",
      "Chenxin Fang",
      "Xingyuan Yu",
      "Fan Zhang",
      "Weichen Dai",
      "Wanzeng Kong",
      "Guofeng Zhang"
    ],
    "abstract": "Recent advances in Gaussian Splatting based 3D scene representation have shown two major trends: semantics-oriented approaches that focus on high-level understanding but lack explicit 3D geometry modeling, and structure-oriented approaches that capture spatial structures yet provide limited semantic abstraction. To bridge this gap, we present CUS-GS, a compact unified structured Gaussian Splatting representation, which connects multimodal semantic features with structured 3D geometry. Specifically, we design a voxelized anchor structure that constructs a spatial scaffold, while extracting multimodal semantic features from a set of foundation models (e.g., CLIP, DINOv2, SEEM). Moreover, we introduce a multimodal latent feature allocation mechanism to unify appearance, geometry, and semantics across heterogeneous feature spaces, ensuring a consistent representation across multiple foundation models. Finally, we propose a feature-aware significance evaluation strategy to dynamically guide anchor growing and pruning, effectively removing redundant or invalid anchors while maintaining semantic integrity. Extensive experiments show that CUS-GS achieves competitive performance compared to state-of-the-art methods using as few as 6M parameters - an order of magnitude smaller than the closest rival at 35M - highlighting the excellent trade off between performance and model efficiency of the proposed framework.",
    "arxiv_url": "https://arxiv.org/abs/2511.17904v1",
    "pdf_url": "https://arxiv.org/pdf/2511.17904v1",
    "published_date": "2025-11-22",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "understanding",
      "dynamic",
      "geometry",
      "ar",
      "semantic",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.17904v1",
      "pdf": "https://arxiv.org/pdf/2511.17904v1"
    },
    "bibtex": ""
  },
  {
    "title": "AEGIS: Preserving privacy of 3D Facial Avatars with Adversarial Perturbations",
    "authors": [
      "Dawid Wolkiewicz",
      "Anastasiya Pechko",
      "Przemysław Spurek",
      "Piotr Syga"
    ],
    "abstract": "The growing adoption of photorealistic 3D facial avatars, particularly those utilizing efficient 3D Gaussian Splatting representations, introduces new risks of online identity theft, especially in systems that rely on biometric authentication. While effective adversarial masking methods have been developed for 2D images, a significant gap remains in achieving robust, viewpoint-consistent identity protection for dynamic 3D avatars. To address this, we present AEGIS, the first privacy-preserving identity masking framework for 3D Gaussian Avatars that maintains the subject's perceived characteristics. Our method aims to conceal identity-related facial features while preserving the avatar's perceptual realism and functional integrity. AEGIS applies adversarial perturbations to the Gaussian color coefficients, guided by a pre-trained face verification network, ensuring consistent protection across multiple viewpoints without retraining or modifying the avatar's geometry. AEGIS achieves complete de-identification, reducing face retrieval and verification accuracy to 0%, while maintaining high perceptual quality (SSIM = 0.9555, PSNR = 35.52 dB). It also preserves key facial attributes such as age, race, gender, and emotion, demonstrating strong privacy protection with minimal visual distortion.",
    "arxiv_url": "https://arxiv.org/abs/2511.17747v1",
    "pdf_url": "https://arxiv.org/pdf/2511.17747v1",
    "published_date": "2025-11-21",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "face",
      "avatar",
      "geometry",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "efficient",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.17747v1",
      "pdf": "https://arxiv.org/pdf/2511.17747v1"
    },
    "bibtex": ""
  },
  {
    "title": "SVRecon: Sparse Voxel Rasterization for Surface Reconstruction",
    "authors": [
      "Seunghun Oh",
      "Jaesung Choe",
      "Dongjae Lee",
      "Daeun Lee",
      "Seunghoon Jeong",
      "Yu-Chiang Frank Wang",
      "Jaesik Park"
    ],
    "abstract": "We extend the recently proposed sparse voxel rasterization paradigm to the task of high-fidelity surface reconstruction by integrating Signed Distance Function (SDF), named SVRecon. Unlike 3D Gaussians, sparse voxels are spatially disentangled from their neighbors and have sharp boundaries, which makes them prone to local minima during optimization. Although SDF values provide a naturally smooth and continuous geometric field, preserving this smoothness across independently parameterized sparse voxels is nontrivial. To address this challenge, we promote coherent and smooth voxel-wise structure through (1) robust geometric initialization using a visual geometry model and (2) a spatial smoothness loss that enforces coherent relationships across parent-child and sibling voxel groups. Extensive experiments across various benchmarks show that our method achieves strong reconstruction accuracy while having consistently speedy convergence. The code will be made public.",
    "arxiv_url": "https://arxiv.org/abs/2511.17364v1",
    "pdf_url": "https://arxiv.org/pdf/2511.17364v1",
    "published_date": "2025-11-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "high-fidelity",
      "face",
      "geometry",
      "ar",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.17364v1",
      "pdf": "https://arxiv.org/pdf/2511.17364v1"
    },
    "bibtex": ""
  },
  {
    "title": "FisheyeGaussianLift: BEV Feature Lifting for Surround-View Fisheye Camera Perception",
    "authors": [
      "Shubham Sonarghare",
      "Prasad Deshpande",
      "Ciaran Hogan",
      "Deepika-Rani Kaliappan-Mahalingam",
      "Ganesh Sistu"
    ],
    "abstract": "Accurate BEV semantic segmentation from fisheye imagery remains challenging due to extreme non-linear distortion, occlusion, and depth ambiguity inherent to wide-angle projections. We present a distortion-aware BEV segmentation framework that directly processes multi-camera high-resolution fisheye images,utilizing calibrated geometric unprojection and per-pixel depth distribution estimation. Each image pixel is lifted into 3D space via Gaussian parameterization, predicting spatial means and anisotropic covariances to explicitly model geometric uncertainty. The projected 3D Gaussians are fused into a BEV representation via differentiable splatting, producing continuous, uncertainty-aware semantic maps without requiring undistortion or perspective rectification. Extensive experiments demonstrate strong segmentation performance on complex parking and urban driving scenarios, achieving IoU scores of 87.75% for drivable regions and 57.26% for vehicles under severe fisheye distortion and diverse environmental conditions.",
    "arxiv_url": "https://arxiv.org/abs/2511.17210v1",
    "pdf_url": "https://arxiv.org/pdf/2511.17210v1",
    "published_date": "2025-11-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "segmentation",
      "ar",
      "semantic"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.17210v1",
      "pdf": "https://arxiv.org/pdf/2511.17210v1"
    },
    "bibtex": ""
  },
  {
    "title": "PEGS: Physics-Event Enhanced Large Spatiotemporal Motion Reconstruction via 3D Gaussian Splatting",
    "authors": [
      "Yijun Xu",
      "Jingrui Zhang",
      "Hongyi Liu",
      "Yuhan Chen",
      "Yuanyang Wang",
      "Qingyao Guo",
      "Dingwen Wang",
      "Lei Yu",
      "Chu He"
    ],
    "abstract": "Reconstruction of rigid motion over large spatiotemporal scales remains a challenging task due to limitations in modeling paradigms, severe motion blur, and insufficient physical consistency. In this work, we propose PEGS, a framework that integrates Physical priors with Event stream enhancement within a 3D Gaussian Splatting pipeline to perform deblurred target-focused modeling and motion recovery. We introduce a cohesive triple-level supervision scheme that enforces physical plausibility via an acceleration constraint, leverages event streams for high-temporal resolution guidance, and employs a Kalman regularizer to fuse multi-source observations. Furthermore, we design a motion-aware simulated annealing strategy that adaptively schedules the training process based on real-time kinematic states. We also contribute the first RGB-Event paired dataset targeting natural, fast rigid motion across diverse scenarios. Experiments show PEGS's superior performance in reconstructing motion over large spatiotemporal scales compared to mainstream dynamic methods.",
    "arxiv_url": "https://arxiv.org/abs/2511.17116v1",
    "pdf_url": "https://arxiv.org/pdf/2511.17116v1",
    "published_date": "2025-11-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "acceleration",
      "motion",
      "ar",
      "3d gaussian",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.17116v1",
      "pdf": "https://arxiv.org/pdf/2511.17116v1"
    },
    "bibtex": ""
  },
  {
    "title": "Towards Generative Design Using Optimal Transport for Shape Exploration and Solution Field Interpolation",
    "authors": [
      "Sergio Torregrosa",
      "David Munoz",
      "Hector Navarro",
      "Charbel Farhat",
      "Francisco Chinesta"
    ],
    "abstract": "Generative Design (GD) combines artificial intelligence (AI), physics-based modeling, and multi-objective optimization to autonomously explore and refine engineering designs. Despite its promise in aerospace, automotive, and other high-performance applications, current GD methods face critical challenges: AI approaches require large datasets and often struggle to generalize; topology optimization is computationally intensive and difficult to extend to multiphysics problems; and model order reduction for evolving geometries remains underdeveloped. To address these challenges, we introduce a unified, structure-preserving framework for GD based on optimal transport (OT), enabling simultaneous interpolation of complex geometries and their associated physical solution fields across evolving design spaces, even with non-matching meshes and substantial shape changes. This capability leverages Gaussian splatting to provide a continuous, mesh-independent representation of the solution and Wasserstein barycenters to enable smooth, mathematically ''mass''-preserving blending of geometries, offering a major advance over surrogate models tied to static meshes. Our framework efficiently interpolates positive scalar fields across arbitrarily shaped, evolving geometries without requiring identical mesh topology or dimensionality. OT also naturally preserves localized physical features -- such as stress concentrations or sharp gradients -- by conserving the spatial distribution of quantities, interpreted as ''mass'' in a mathematical sense, rather than averaging them, avoiding artificial smoothing. Preliminary extensions to signed and vector fields are presented. Representative test cases demonstrate enhanced efficiency, adaptability, and physical fidelity, establishing a foundation for future foundation-model-powered generative design workflows.",
    "arxiv_url": "https://arxiv.org/abs/2511.17111v1",
    "pdf_url": "https://arxiv.org/pdf/2511.17111v1",
    "published_date": "2025-11-21",
    "categories": [
      "cs.CE"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "efficient",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.17111v1",
      "pdf": "https://arxiv.org/pdf/2511.17111v1"
    },
    "bibtex": ""
  },
  {
    "title": "SPAGS: Sparse-View Articulated Object Reconstruction from Single State via Planar Gaussian Splatting",
    "authors": [
      "Di Wu",
      "Liu Liu",
      "Xueyu Yuan",
      "Qiaojun Yu",
      "Wenxiao Chen",
      "Ruilong Yan",
      "Yiming Tang",
      "Liangtu Song"
    ],
    "abstract": "Articulated objects are ubiquitous in daily environments, and their 3D reconstruction holds great significance across various fields. However, existing articulated object reconstruction methods typically require costly inputs such as multi-stage and multi-view observations. To address the limitations, we propose a category-agnostic articulated object reconstruction framework via planar Gaussian Splatting, which only uses sparse-view RGB images from a single state. Specifically, we first introduce a Gaussian information field to perceive the optimal sparse viewpoints from candidate camera poses. Then we compress 3D Gaussians into planar Gaussians to facilitate accurate estimation of normal and depth. The planar Gaussians are optimized in a coarse-to-fine manner through depth smooth regularization and few-shot diffusion. Moreover, we introduce a part segmentation probability for each Gaussian primitive and update them by back-projecting part segmentation masks of renderings. Extensive experimental results demonstrate that our method achieves higher-fidelity part-level surface reconstruction on both synthetic and real-world data than existing methods. Codes will be made publicly available.",
    "arxiv_url": "https://arxiv.org/abs/2511.17092v2",
    "pdf_url": "https://arxiv.org/pdf/2511.17092v2",
    "published_date": "2025-11-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "sparse view",
      "few-shot",
      "face",
      "3d reconstruction",
      "segmentation",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.17092v2",
      "pdf": "https://arxiv.org/pdf/2511.17092v2"
    },
    "bibtex": ""
  },
  {
    "title": "REArtGS++: Generalizable Articulation Reconstruction with Temporal Geometry Constraint via Planar Gaussian Splatting",
    "authors": [
      "Di Wu",
      "Liu Liu",
      "Anran Huang",
      "Yuyan Liu",
      "Qiaojun Yu",
      "Shaofan Liu",
      "Liangtu Song",
      "Cewu Lu"
    ],
    "abstract": "Articulated objects are pervasive in daily environments, such as drawers and refrigerators. Towards their part-level surface reconstruction and joint parameter estimation, REArtGS introduces a category-agnostic approach using multi-view RGB images at two different states. However, we observe that REArtGS still struggles with screw-joint or multi-part objects and lacks geometric constraints for unseen states. In this paper, we propose REArtGS++, a novel method towards generalizable articulated object reconstruction with temporal geometry constraint and planar Gaussian splatting. We first model a decoupled screw motion for each joint without type prior, and jointly optimize part-aware Gaussians with joint parameters through part motion blending. To introduce time-continuous geometric constraint for articulated modeling, we encourage Gaussians to be planar and propose a temporally consistent regularization between planar normal and depth through Taylor first-order expansion. Extensive experiments on both synthetic and real-world articulated objects demonstrate our superiority in generalizable part-level surface reconstruction and joint parameter estimation, compared to existing approaches. Project Site: https://sites.google.com/view/reartgs2/home.",
    "arxiv_url": "https://arxiv.org/abs/2511.17059v3",
    "pdf_url": "https://arxiv.org/pdf/2511.17059v3",
    "published_date": "2025-11-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "geometry",
      "gaussian splatting",
      "ar",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.17059v3",
      "pdf": "https://arxiv.org/pdf/2511.17059v3",
      "project": "https://sites.google.com/view/reartgs2/home"
    },
    "bibtex": ""
  },
  {
    "title": "RoomPlanner: Explicit Layout Planner for Easier LLM-Driven 3D Room Generation",
    "authors": [
      "Wenzhuo Sun",
      "Mingjian Liang",
      "Wenxuan Song",
      "Xuelian Cheng",
      "Zongyuan Ge"
    ],
    "abstract": "In this paper, we propose RoomPlanner, the first fully automatic 3D room generation framework for painlessly creating realistic indoor scenes with only short text as input. Without any manual layout design or panoramic image guidance, our framework can generate explicit layout criteria for rational spatial placement. We begin by introducing a hierarchical structure of language-driven agent planners that can automatically parse short and ambiguous prompts into detailed scene descriptions. These descriptions include raw spatial and semantic attributes for each object and the background, which are then used to initialize 3D point clouds. To position objects within bounded environments, we implement two arrangement constraints that iteratively optimize spatial arrangements, ensuring a collision-free and accessible layout solution. In the final rendering stage, we propose a novel AnyReach Sampling strategy for camera trajectory, along with the Interval Timestep Flow Sampling (ITFS) strategy, to efficiently optimize the coarse 3D Gaussian scene representation. These approaches help reduce the total generation time to under 30 minutes. Extensive experiments demonstrate that our method can produce geometrically rational 3D indoor scenes, surpassing prior approaches in both rendering speed and visual quality while preserving editability. The code will be available soon.",
    "arxiv_url": "https://arxiv.org/abs/2511.17048v1",
    "pdf_url": "https://arxiv.org/pdf/2511.17048v1",
    "published_date": "2025-11-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "efficient",
      "semantic",
      "ar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.17048v1",
      "pdf": "https://arxiv.org/pdf/2511.17048v1"
    },
    "bibtex": ""
  },
  {
    "title": "PhysMorph-GS: Differentiable Shape Morphing via Joint Optimization of Physics and Rendering Objectives",
    "authors": [
      "Chang-Yong Song",
      "David Hyde"
    ],
    "abstract": "Shape morphing with physics-based simulation naturally supports large deformations and topology changes, but existing methods suffer from a \"rendering gap\": nondifferentiable surface extraction prevents image losses from directly guiding physics optimization. We introduce PhysMorph-GS, which couples a differentiable material point method (MPM) with 3D Gaussian splatting through a deformation-aware upsampling bridge that maps sparse particle states (x, F) to dense Gaussians (mu, Sigma). Multi-modal rendering losses on silhouette and depth backpropagate along two paths, from covariances to deformation gradients via a stretch-based mapping and from Gaussian means to particle positions. Through the MPM adjoint, these gradients update deformation controls while mass is conserved at a compact set of anchor particles. A multi-pass interleaved optimization scheme repeatedly injects rendering gradients into successive physics steps, avoiding collapse to purely physics-driven solutions. On challenging morphing sequences, PhysMorph-GS improves boundary fidelity and temporal stability over a differentiable MPM baseline and better reconstructs thin structures such as ears and tails. Quantitatively, our depth-supervised variant reduces Chamfer distance by about 2.5 percent relative to the physics-only baseline. By providing a differentiable particle-to-Gaussian bridge, PhysMorph-GS closes a key gap in physics-aware rendering pipelines and enables inverse design directly from image-space supervision.",
    "arxiv_url": "https://arxiv.org/abs/2511.16988v1",
    "pdf_url": "https://arxiv.org/pdf/2511.16988v1",
    "published_date": "2025-11-21",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "face",
      "deformation",
      "ar",
      "3d gaussian",
      "mapping",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.16988v1",
      "pdf": "https://arxiv.org/pdf/2511.16988v1"
    },
    "bibtex": ""
  },
  {
    "title": "Gradient-Driven Natural Selection for Compact 3D Gaussian Splatting",
    "authors": [
      "Xiaobin Deng",
      "Qiuli Yu",
      "Changyu Diao",
      "Min Li",
      "Duanqing Xu"
    ],
    "abstract": "3DGS employs a large number of Gaussian primitives to fit scenes, resulting in substantial storage and computational overhead. Existing pruning methods rely on manually designed criteria or introduce additional learnable parameters, yielding suboptimal results. To address this, we propose an natural selection inspired pruning framework that models survival pressure as a regularization gradient field applied to opacity, allowing the optimization gradients--driven by the goal of maximizing rendering quality--to autonomously determine which Gaussians to retain or prune. This process is fully learnable and requires no human intervention. We further introduce an opacity decay technique with a finite opacity prior, which accelerates the selection process without compromising pruning effectiveness. Compared to 3DGS, our method achieves over 0.6 dB PSNR gain under 15\\% budgets, establishing state-of-the-art performance for compact 3DGS. Project page https://xiaobin2001.github.io/GNS-web.",
    "arxiv_url": "https://arxiv.org/abs/2511.16980v1",
    "pdf_url": "https://arxiv.org/pdf/2511.16980v1",
    "published_date": "2025-11-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "compact",
      "ar",
      "human",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.16980v1",
      "pdf": "https://arxiv.org/pdf/2511.16980v1",
      "project": "https://xiaobin2001.github.io/GNS-web"
    },
    "bibtex": ""
  },
  {
    "title": "One Walk is All You Need: Data-Efficient 3D RF Scene Reconstruction with Human Movements",
    "authors": [
      "Yiheng Bian",
      "Zechen Li",
      "Lanqing Yang",
      "Hao Pan",
      "Yezhou Wang",
      "Longyuan Ge",
      "Jeffery Wu",
      "Ruiheng Liu",
      "Yongjian Fu",
      "Yichao chen",
      "Guangtao xue"
    ],
    "abstract": "Reconstructing 3D Radiance Field (RF) scenes through opaque obstacles is a long-standing goal, yet it is fundamentally constrained by a laborious data acquisition process requiring thousands of static measurements, which treats human motion as noise to be filtered. This work introduces a new paradigm with a core objective: to perform fast, data-efficient, and high-fidelity RF reconstruction of occluded 3D static scenes, using only a single, brief human walk. We argue that this unstructured motion is not noise, but is in fact an information-rich signal available for reconstruction. To achieve this, we design a factorization framework based on composite 3D Gaussian Splatting (3DGS) that learns to model the dynamic effects of human motion from the persistent static scene geometry within a raw RF stream. Trained on just a single 60-second casual walk, our model reconstructs the full static scene with a Structural Similarity Index (SSIM) of 0.96, remarkably outperforming heavily-sampled state-of-the-art (SOTA) by 12%. By transforming the human movements into its valuable signals, our method eliminates the data acquisition bottleneck and paves the way for on-the-fly 3D RF mapping of unseen environments.",
    "arxiv_url": "https://arxiv.org/abs/2511.16966v1",
    "pdf_url": "https://arxiv.org/pdf/2511.16966v1",
    "published_date": "2025-11-21",
    "categories": [
      "cs.NI"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "high-fidelity",
      "motion",
      "geometry",
      "ar",
      "human",
      "mapping",
      "3d gaussian",
      "efficient",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.16966v1",
      "pdf": "https://arxiv.org/pdf/2511.16966v1"
    },
    "bibtex": ""
  },
  {
    "title": "Vorion: A RISC-V GPU with Hardware-Accelerated 3D Gaussian Rendering and Training",
    "authors": [
      "Yipeng Wang",
      "Mengtian Yang",
      "Chieh-pu Lo",
      "Jaydeep P. Kulkarni"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a foundational technique for real-time neural rendering, 3D scene generation, volumetric video (4D) capture. However, its rendering and training impose massive computation, making real-time rendering on edge devices and real-time 4D reconstruction on workstations currently infeasible. Given its fixed-function nature and similarity with traditional rasterization, 3DGS presents a strong case for dedicated hardware in the graphics pipeline of next-generation GPUs. This work, Vorion, presents the first GPGPU prototype with hardware-accelerated 3DGS rendering and training. Vorion features scalable architecture, minimal hardware change to traditional rasterizers, z-tiling to increase parallelism, and Gaussian/pixel-centric hybrid dataflow. We prototype the minimal system (8 SIMT cores, 2 Gaussian rasterizer) using TSMC 16nm FinFET technology, which achieves 19 FPS for rendering. The scaled design with 16 rasterizers achieves 38.6 iterations/s for training.",
    "arxiv_url": "https://arxiv.org/abs/2511.16831v1",
    "pdf_url": "https://arxiv.org/pdf/2511.16831v1",
    "published_date": "2025-11-20",
    "categories": [
      "cs.AR",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "real-time rendering",
      "neural rendering",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.16831v1",
      "pdf": "https://arxiv.org/pdf/2511.16831v1"
    },
    "bibtex": ""
  },
  {
    "title": "TRIM: Scalable 3D Gaussian Diffusion Inference with Temporal and Spatial Trimming",
    "authors": [
      "Zeyuan Yin",
      "Xiaoming Liu"
    ],
    "abstract": "Recent advances in 3D Gaussian diffusion models suffer from time-intensive denoising and post-denoising processing due to the massive number of Gaussian primitives, resulting in slow generation and limited scalability along sampling trajectories. To improve the efficiency of 3D diffusion models, we propose $\\textbf{TRIM}$ ($\\textbf{T}$rajectory $\\textbf{R}$eduction and $\\textbf{I}$nstance $\\textbf{M}$ask denoising), a post-training approach that incorporates both temporal and spatial trimming strategies, to accelerate inference without compromising output quality while supporting the inference-time scaling for Gaussian diffusion models. Instead of scaling denoising trajectories in a costly end-to-end manner, we develop a lightweight selector model to evaluate latent Gaussian primitives derived from multiple sampled noises, enabling early trajectory reduction by selecting candidates with high-quality potential. Furthermore, we introduce instance mask denoising to prune learnable Gaussian primitives by filtering out redundant background regions, reducing inference computation at each denoising step. Extensive experiments and analysis demonstrate that TRIM significantly improves both the efficiency and quality of 3D generation. Source code is available at $\\href{https://github.com/zeyuanyin/TRIM}{link}$.",
    "arxiv_url": "https://arxiv.org/abs/2511.16642v1",
    "pdf_url": "https://arxiv.org/pdf/2511.16642v1",
    "published_date": "2025-11-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/zeyuanyin/TRIM",
    "keywords": [
      "3d gaussian",
      "lightweight",
      "ar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.16642v1",
      "pdf": "https://arxiv.org/pdf/2511.16642v1",
      "github": "https://github.com/zeyuanyin/TRIM"
    },
    "bibtex": ""
  },
  {
    "title": "EOGS++: Earth Observation Gaussian Splatting with Internal Camera Refinement and Direct Panchromatic Rendering",
    "authors": [
      "Pierrick Bournez",
      "Luca Savant Aira",
      "Thibaud Ehret",
      "Gabriele Facciolo"
    ],
    "abstract": "Recently, 3D Gaussian Splatting has been introduced as a compelling alternative to NeRF for Earth observation, offering com- petitive reconstruction quality with significantly reduced training times. In this work, we extend the Earth Observation Gaussian Splatting (EOGS) framework to propose EOGS++, a novel method tailored for satellite imagery that directly operates on raw high-resolution panchromatic data without requiring external preprocessing. Furthermore, leveraging optical flow techniques we embed bundle adjustment directly within the training process, avoiding reliance on external optimization tools while improving camera pose estimation. We also introduce several improvements to the original implementation, including early stopping and TSDF post-processing, all contributing to sharper reconstructions and better geometric accuracy. Experiments on the IARPA 2016 and DFC2019 datasets demonstrate that EOGS++ achieves state-of-the-art performance in terms of reconstruction quality and effi- ciency, outperforming the original EOGS method and other NeRF-based methods while maintaining the computational advantages of Gaussian Splatting. Our model demonstrates an improvement from 1.33 to 1.19 mean MAE errors on buildings compared to the original EOGS models",
    "arxiv_url": "https://arxiv.org/abs/2511.16542v1",
    "pdf_url": "https://arxiv.org/pdf/2511.16542v1",
    "published_date": "2025-11-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.16542v1",
      "pdf": "https://arxiv.org/pdf/2511.16542v1"
    },
    "bibtex": ""
  },
  {
    "title": "CRISTAL: Real-time Camera Registration in Static LiDAR Scans using Neural Rendering",
    "authors": [
      "Joni Vanherck",
      "Steven Moonen",
      "Brent Zoomers",
      "Kobe Werner",
      "Jeroen Put",
      "Lode Jorissen",
      "Nick Michiels"
    ],
    "abstract": "Accurate camera localization is crucial for robotics and Extended Reality (XR), enabling reliable navigation and alignment of virtual and real content. Existing visual methods often suffer from drift, scale ambiguity, and depend on fiducials or loop closure. This work introduces a real-time method for localizing a camera within a pre-captured, highly accurate colored LiDAR point cloud. By rendering synthetic views from this cloud, 2D-3D correspondences are established between live frames and the point cloud. A neural rendering technique narrows the domain gap between synthetic and real images, reducing occlusion and background artifacts to improve feature matching. The result is drift-free camera tracking with correct metric scale in the global LiDAR coordinate system. Two real-time variants are presented: Online Render and Match, and Prebuild and Localize. We demonstrate improved results on the ScanNet++ dataset and outperform existing SLAM pipelines.",
    "arxiv_url": "https://arxiv.org/abs/2511.16349v1",
    "pdf_url": "https://arxiv.org/pdf/2511.16349v1",
    "published_date": "2025-11-20",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "localization",
      "tracking",
      "neural rendering",
      "ar",
      "slam"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.16349v1",
      "pdf": "https://arxiv.org/pdf/2511.16349v1"
    },
    "bibtex": ""
  },
  {
    "title": "Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling",
    "authors": [
      "Minseok Seo",
      "Mark Hamilton",
      "Changick Kim"
    ],
    "abstract": "We present \\textbf{Upsample Anything}, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling. The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only $\\approx0.419 \\text{s}$ per 224x224 image and achieves state-of-the-art performance on semantic segmentation, depth estimation, and both depth and probability map upsampling. \\textbf{Project page:} \\href{https://seominseok0429.github.io/Upsample-Anything/}{https://seominseok0429.github.io/Upsample-Anything/}",
    "arxiv_url": "https://arxiv.org/abs/2511.16301v2",
    "pdf_url": "https://arxiv.org/pdf/2511.16301v2",
    "published_date": "2025-11-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "segmentation",
      "ar",
      "semantic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.16301v2",
      "pdf": "https://arxiv.org/pdf/2511.16301v2",
      "project": "https://seominseok0429.github.io/Upsample-Anything"
    },
    "bibtex": ""
  },
  {
    "title": "Optimizing 3D Gaussian Splattering for Mobile GPUs",
    "authors": [
      "Md Musfiqur Rahman Sanim",
      "Zhihao Shu",
      "Bahram Afsharmanesh",
      "AmirAli Mirian",
      "Jiexiong Guan",
      "Wei Niu",
      "Bin Ren",
      "Gagan Agrawal"
    ],
    "abstract": "Image-based 3D scene reconstruction, which transforms multi-view images into a structured 3D representation of the surrounding environment, is a common task across many modern applications. 3D Gaussian Splatting (3DGS) is a new paradigm to address this problem and offers considerable efficiency as compared to the previous methods. Motivated by this, and considering various benefits of mobile device deployment (data privacy, operating without internet connectivity, and potentially faster responses), this paper develops Texture3dgs, an optimized mapping of 3DGS for a mobile GPU. A critical challenge in this area turns out to be optimizing for the two-dimensional (2D) texture cache, which needs to be exploited for faster executions on mobile GPUs. As a sorting method dominates the computations in 3DGS on mobile platforms, the core of Texture3dgs is a novel sorting algorithm where the processing, data movement, and placement are highly optimized for 2D memory. The properties of this algorithm are analyzed in view of a cost model for the texture cache. In addition, we accelerate other steps of the 3DGS algorithm through improved variable layout design and other optimizations. End-to-end evaluation shows that Texture3dgs delivers up to 4.1$\\times$ and 1.7$\\times$ speedup for the sorting and overall 3D scene reconstruction, respectively -- while also reducing memory usage by up to 1.6$\\times$ -- demonstrating the effectiveness of our design for efficient mobile 3D scene reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2511.16298v1",
    "pdf_url": "https://arxiv.org/pdf/2511.16298v1",
    "published_date": "2025-11-20",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "efficient",
      "3d gaussian",
      "mapping",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.16298v1",
      "pdf": "https://arxiv.org/pdf/2511.16298v1"
    },
    "bibtex": ""
  },
  {
    "title": "LEGO-SLAM: Language-Embedded Gaussian Optimization SLAM",
    "authors": [
      "Sibaek Lee",
      "Seongbo Ha",
      "Kyeongsu Kang",
      "Joonyeol Choi",
      "Seungjun Tak",
      "Hyeonwoo Yu"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled Simultaneous Localization and Mapping (SLAM) systems to build photorealistic maps. However, these maps lack the open-vocabulary semantic understanding required for advanced robotic interaction. Integrating language features into SLAM remains a significant challenge, as storing high-dimensional features demands excessive memory and rendering overhead, while existing methods with static models lack adaptability for novel environments. To address these limitations, we propose LEGO-SLAM (Language-Embedded Gaussian Optimization SLAM), the first framework to achieve real-time, open-vocabulary mapping within a 3DGS-based SLAM system. At the core of our method is a scene-adaptive encoder-decoder that distills high-dimensional language embeddings into a compact 16-dimensional feature space. This design reduces the memory per Gaussian and accelerates rendering, enabling real-time performance. Unlike static approaches, our encoder adapts online to unseen scenes. These compact features also enable a language-guided pruning strategy that identifies semantic redundancy, reducing the map's Gaussian count by over 60\\% while maintaining rendering quality. Furthermore, we introduce a language-based loop detection approach that reuses these mapping features, eliminating the need for a separate detection model. Extensive experiments demonstrate that LEGO-SLAM achieves competitive mapping quality and tracking accuracy, all while providing open-vocabulary capabilities at 15 FPS.",
    "arxiv_url": "https://arxiv.org/abs/2511.16144v1",
    "pdf_url": "https://arxiv.org/pdf/2511.16144v1",
    "published_date": "2025-11-20",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "compact",
      "understanding",
      "localization",
      "tracking",
      "ar",
      "semantic",
      "3d gaussian",
      "slam",
      "mapping",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.16144v1",
      "pdf": "https://arxiv.org/pdf/2511.16144v1"
    },
    "bibtex": ""
  },
  {
    "title": "Clustered Error Correction with Grouped 4D Gaussian Splatting",
    "authors": [
      "Taeho Kang",
      "Jaeyeon Park",
      "Kyungjin Lee",
      "Youngki Lee"
    ],
    "abstract": "Existing 4D Gaussian Splatting (4DGS) methods struggle to accurately reconstruct dynamic scenes, often failing to resolve ambiguous pixel correspondences and inadequate densification in dynamic regions. We address these issues by introducing a novel method composed of two key components: (1) Elliptical Error Clustering and Error Correcting Splat Addition that pinpoints dynamic areas to improve and initialize fitting splats, and (2) Grouped 4D Gaussian Splatting that improves consistency of mapping between splats and represented dynamic objects. Specifically, we classify rendering errors into missing-color and occlusion types, then apply targeted corrections via backprojection or foreground splitting guided by cross-view color consistency. Evaluations on Neural 3D Video and Technicolor datasets demonstrate that our approach significantly improves temporal consistency and achieves state-of-the-art perceptual rendering quality, improving 0.39dB of PSNR on the Technicolor Light Field dataset. Our visualization shows improved alignment between splats and dynamic objects, and the error correction method's capability to identify errors and properly initialize new splats. Our implementation details and source code are available at https://github.com/tho-kn/cem-4dgs.",
    "arxiv_url": "https://arxiv.org/abs/2511.16112v1",
    "pdf_url": "https://arxiv.org/pdf/2511.16112v1",
    "published_date": "2025-11-20",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "https://github.com/tho-kn/cem-4dgs",
    "keywords": [
      "4d",
      "dynamic",
      "ar",
      "mapping",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.16112v1",
      "pdf": "https://arxiv.org/pdf/2511.16112v1",
      "github": "https://github.com/tho-kn/cem-4dgs"
    },
    "bibtex": ""
  },
  {
    "title": "Rad-GS: Radar-Vision Integration for 3D Gaussian Splatting SLAM in Outdoor Environments",
    "authors": [
      "Renxiang Xiao",
      "Wei Liu",
      "Yuanfan Zhang",
      "Yushuai Chen",
      "Jinming Chen",
      "Zilu Wang",
      "Liang Hu"
    ],
    "abstract": "We present Rad-GS, a 4D radar-camera SLAM system designed for kilometer-scale outdoor environments, utilizing 3D Gaussian as a differentiable spatial representation. Rad-GS combines the advantages of raw radar point cloud with Doppler information and geometrically enhanced point cloud to guide dynamic object masking in synchronized images, thereby alleviating rendering artifacts and improving localization accuracy. Additionally, unsynchronized image frames are leveraged to globally refine the 3D Gaussian representation, enhancing texture consistency and novel view synthesis fidelity. Furthermore, the global octree structure coupled with a targeted Gaussian primitive management strategy further suppresses noise and significantly reduces memory consumption in large-scale environments. Extensive experiments and ablation studies demonstrate that Rad-GS achieves performance comparable to traditional 3D Gaussian methods based on camera or LiDAR inputs, highlighting the feasibility of robust outdoor mapping using 4D mmWave radar. Real-world reconstruction at kilometer scale validates the potential of Rad-GS for large-scale scene reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2511.16091v1",
    "pdf_url": "https://arxiv.org/pdf/2511.16091v1",
    "published_date": "2025-11-20",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "outdoor",
      "localization",
      "dynamic",
      "ar",
      "mapping",
      "3d gaussian",
      "slam",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.16091v1",
      "pdf": "https://arxiv.org/pdf/2511.16091v1"
    },
    "bibtex": ""
  },
  {
    "title": "CuriGS: Curriculum-Guided Gaussian Splatting for Sparse View Synthesis",
    "authors": [
      "Zijian Wu",
      "Mingfeng Jiang",
      "Zidian Lin",
      "Ying Song",
      "Hanjie Ma",
      "Qun Wu",
      "Dongping Zhang",
      "Guiyang Pu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as an efficient, high-fidelity representation for real-time scene reconstruction and rendering. However, extending 3DGS to sparse-view settings remains challenging because of supervision scarcity and overfitting caused by limited viewpoint coverage. In this paper, we present CuriGS, a curriculum-guided framework for sparse-view 3D reconstruction using 3DGS. CuriGS addresses the core challenge of sparse-view synthesis by introducing student views: pseudo-views sampled around ground-truth poses (teacher). For each teacher, we generate multiple groups of student views with different perturbation levels. During training, we follow a curriculum schedule that gradually unlocks higher perturbation level, randomly sampling candidate students from the active level to assist training. Each sampled student is regularized via depth-correlation and co-regularization, and evaluated using a multi-signal metric that combines SSIM, LPIPS, and an image-quality measure. For every teacher and perturbation level, we periodically retain the best-performing students and promote those that satisfy a predefined quality threshold to the training set, resulting in a stable augmentation of sparse training views. Experimental results show that CuriGS outperforms state-of-the-art baselines in both rendering fidelity and geometric consistency across various synthetic and real sparse-view scenes. Project page: https://zijian1026.github.io/CuriGS/",
    "arxiv_url": "https://arxiv.org/abs/2511.16030v1",
    "pdf_url": "https://arxiv.org/pdf/2511.16030v1",
    "published_date": "2025-11-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "sparse view",
      "high-fidelity",
      "3d reconstruction",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.16030v1",
      "pdf": "https://arxiv.org/pdf/2511.16030v1",
      "project": "https://zijian1026.github.io/CuriGS"
    },
    "bibtex": ""
  },
  {
    "title": "One algebra for all : Geometric Algebra methods for neurosymbolic XR scene authoring, animation and neural rendering",
    "authors": [
      "Manos Kamarianakis",
      "Antonis Protopsaltis",
      "George Papagiannakis"
    ],
    "abstract": "This position paper delves into the transformative role of Geometric Algebra (GA) in advancing specific areas of Computer Graphics (CG) and Extended Reality (XR), particularly in character animation, rendering, rigging, neural rendering, and generative AI-driven scene editing. Common CG algorithms require handling rotations, translations, and dilations (uniform scalings) in operations such as object rendering, rigged model animation, soft-body deformation, and XR simulations. Traditional representation forms - such as matrices, quaternions, and vectors - often introduce limitations in precision and performance. Recent breakthroughs in the use of GA suggest it can significantly enhance these processes by encapsulating geometric forms and transformations into uniform algebraic expressions, which maintain critical geometric properties throughout multi-step transformations. Furthermore, we explore how GA can serve as a unifying mathematical substrate for neurosymbolic XR scene authoring, bridging learned neural representations and explicit geometric reasoning. This paper outlines how GA-based approaches can improve the fidelity of rigged character animations, enhance soft-body simulations, streamline real-time rendering, and optimize neural and generative AI scene editing. GA offers a coherent and efficient framework for these processes, resulting in superior visual outcomes and computational efficiency, particularly in XR environments.",
    "arxiv_url": "https://arxiv.org/abs/2511.15398v1",
    "pdf_url": "https://arxiv.org/pdf/2511.15398v1",
    "published_date": "2025-11-19",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "body",
      "deformation",
      "animation",
      "real-time rendering",
      "neural rendering",
      "ar",
      "efficient"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.15398v1",
      "pdf": "https://arxiv.org/pdf/2511.15398v1"
    },
    "bibtex": ""
  },
  {
    "title": "Gaussian Blending: Rethinking Alpha Blending in 3D Gaussian Splatting",
    "authors": [
      "Junseo Koo",
      "Jinseo Jeong",
      "Gunhee Kim"
    ],
    "abstract": "The recent introduction of 3D Gaussian Splatting (3DGS) has significantly advanced novel view synthesis. Several studies have further improved the rendering quality of 3DGS, yet they still exhibit noticeable visual discrepancies when synthesizing views at sampling rates unseen during training. Specifically, they suffer from (i) erosion-induced blurring artifacts when zooming in and (ii) dilation-induced staircase artifacts when zooming out. We speculate that these artifacts arise from the fundamental limitation of the alpha blending adopted in 3DGS methods. Instead of the conventional alpha blending that computes alpha and transmittance as scalar quantities over a pixel, we propose to replace it with our novel Gaussian Blending that treats alpha and transmittance as spatially varying distributions. Thus, transmittances can be updated considering the spatial distribution of alpha values across the pixel area, allowing nearby background splats to contribute to the final rendering. Our Gaussian Blending maintains real-time rendering speed and requires no additional memory cost, while being easily integrated as a drop-in replacement into existing 3DGS-based or other NVS frameworks. Extensive experiments demonstrate that Gaussian Blending effectively captures fine details at various sampling rates unseen during training, consistently outperforming existing novel view synthesis models across both unseen and seen sampling rates.",
    "arxiv_url": "https://arxiv.org/abs/2511.15102v1",
    "pdf_url": "https://arxiv.org/pdf/2511.15102v1",
    "published_date": "2025-11-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "real-time rendering",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.15102v1",
      "pdf": "https://arxiv.org/pdf/2511.15102v1"
    },
    "bibtex": ""
  },
  {
    "title": "Gaussian See, Gaussian Do: Semantic 3D Motion Transfer from Multiview Video",
    "authors": [
      "Yarin Bekor",
      "Gal Michael Harari",
      "Or Perel",
      "Or Litany"
    ],
    "abstract": "We present Gaussian See, Gaussian Do, a novel approach for semantic 3D motion transfer from multiview video. Our method enables rig-free, cross-category motion transfer between objects with semantically meaningful correspondence. Building on implicit motion transfer techniques, we extract motion embeddings from source videos via condition inversion, apply them to rendered frames of static target shapes, and use the resulting videos to supervise dynamic 3D Gaussian Splatting reconstruction. Our approach introduces an anchor-based view-aware motion embedding mechanism, ensuring cross-view consistency and accelerating convergence, along with a robust 4D reconstruction pipeline that consolidates noisy supervision videos. We establish the first benchmark for semantic 3D motion transfer and demonstrate superior motion fidelity and structural consistency compared to adapted baselines. Code and data for this paper available at https://gsgd-motiontransfer.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2511.14848v1",
    "pdf_url": "https://arxiv.org/pdf/2511.14848v1",
    "published_date": "2025-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "gaussian splatting",
      "ar",
      "semantic",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.14848v1",
      "pdf": "https://arxiv.org/pdf/2511.14848v1",
      "project": "https://gsgd-motiontransfer.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction",
    "authors": [
      "Meiying Gu",
      "Jiawei Zhang",
      "Jiahe Li",
      "Xiaohan Yu",
      "Haonan Luo",
      "Jin Zheng",
      "Xiao Bai"
    ],
    "abstract": "Recent advances in optimizing Gaussian Splatting for scene geometry have enabled efficient reconstruction of detailed surfaces from images. However, when input views are sparse, such optimization is prone to overfitting, leading to suboptimal reconstruction quality. Existing approaches address this challenge by employing flattened Gaussian primitives to better fit surface geometry, combined with depth regularization to alleviate geometric ambiguities under limited viewpoints. Nevertheless, the increased anisotropy inherent in flattened Gaussians exacerbates overfitting in sparse-view scenarios, hindering accurate surface fitting and degrading novel view synthesis performance. In this paper, we propose \\net{}, a method that reconstructs more accurate and detailed surfaces while preserving high-quality novel view rendering. Our key insight is to introduce Stereo Geometry-Texture Alignment, which bridges rendering quality and geometry estimation, thereby jointly enhancing both surface reconstruction and view synthesis. In addition, we present a Pseudo-Feature Enhanced Geometry Consistency that enforces multi-view geometric consistency by incorporating both training and unseen views, effectively mitigating overfitting caused by sparse supervision. Extensive experiments on the DTU, BlendedMVS, and Mip-NeRF360 datasets demonstrate that our method achieves the state-of-the-art performance.",
    "arxiv_url": "https://arxiv.org/abs/2511.14633v1",
    "pdf_url": "https://arxiv.org/pdf/2511.14633v1",
    "published_date": "2025-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "face",
      "geometry",
      "ar",
      "nerf",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.14633v1",
      "pdf": "https://arxiv.org/pdf/2511.14633v1"
    },
    "bibtex": ""
  },
  {
    "title": "Interaction-Aware 4D Gaussian Splatting for Dynamic Hand-Object Interaction Reconstruction",
    "authors": [
      "Hao Tian",
      "Chenyangguang Zhang",
      "Rui Liu",
      "Wen Shen",
      "Xiaolin Qin"
    ],
    "abstract": "This paper focuses on a challenging setting of simultaneously modeling geometry and appearance of hand-object interaction scenes without any object priors. We follow the trend of dynamic 3D Gaussian Splatting based methods, and address several significant challenges. To model complex hand-object interaction with mutual occlusion and edge blur, we present interaction-aware hand-object Gaussians with newly introduced optimizable parameters aiming to adopt piecewise linear hypothesis for clearer structural representation. Moreover, considering the complementarity and tightness of hand shape and object shape during interaction dynamics, we incorporate hand information into object deformation field, constructing interaction-aware dynamic fields to model flexible motions. To further address difficulties in the optimization process, we propose a progressive strategy that handles dynamic regions and static background step by step. Correspondingly, explicit regularizations are designed to stabilize the hand-object representations for smooth motion transition, physical interaction reality, and coherent lighting. Experiments show that our approach surpasses existing dynamic 3D-GS-based methods and achieves state-of-the-art performance in reconstructing dynamic hand-object interaction.",
    "arxiv_url": "https://arxiv.org/abs/2511.14540v1",
    "pdf_url": "https://arxiv.org/pdf/2511.14540v1",
    "published_date": "2025-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "deformation",
      "motion",
      "geometry",
      "ar",
      "3d gaussian",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.14540v1",
      "pdf": "https://arxiv.org/pdf/2511.14540v1"
    },
    "bibtex": ""
  },
  {
    "title": "2D Gaussians Spatial Transport for Point-supervised Density Regression",
    "authors": [
      "Miao Shang",
      "Xiaopeng Hong"
    ],
    "abstract": "This paper introduces Gaussian Spatial Transport (GST), a novel framework that leverages Gaussian splatting to facilitate transport from the probability measure in the image coordinate space to the annotation map. We propose a Gaussian splatting-based method to estimate pixel-annotation correspondence, which is then used to compute a transport plan derived from Bayesian probability. To integrate the resulting transport plan into standard network optimization in typical computer vision tasks, we derive a loss function that measures discrepancy after transport. Extensive experiments on representative computer vision tasks, including crowd counting and landmark detection, validate the effectiveness of our approach. Compared to conventional optimal transport schemes, GST eliminates iterative transport plan computation during training, significantly improving efficiency. Code is available at https://github.com/infinite0522/GST.",
    "arxiv_url": "https://arxiv.org/abs/2511.14477v2",
    "pdf_url": "https://arxiv.org/pdf/2511.14477v2",
    "published_date": "2025-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/infinite0522/GST",
    "keywords": [
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.14477v2",
      "pdf": "https://arxiv.org/pdf/2511.14477v2",
      "github": "https://github.com/infinite0522/GST"
    },
    "bibtex": ""
  },
  {
    "title": "IBGS: Image-Based Gaussian Splatting",
    "authors": [
      "Hoang Chuong Nguyen",
      "Wei Mao",
      "Jose M. Alvarez",
      "Miaomiao Liu"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a fast, high-quality method for novel view synthesis (NVS). However, its use of low-degree spherical harmonics limits its ability to capture spatially varying color and view-dependent effects such as specular highlights. Existing works augment Gaussians with either a global texture map, which struggles with complex scenes, or per-Gaussian texture maps, which introduces high storage overhead. We propose Image-Based Gaussian Splatting, an efficient alternative that leverages high-resolution source images for fine details and view-specific color modeling. Specifically, we model each pixel color as a combination of a base color from standard 3DGS rendering and a learned residual inferred from neighboring training images. This promotes accurate surface alignment and enables rendering images of high-frequency details and accurate view-dependent effects. Experiments on standard NVS benchmarks show that our method significantly outperforms prior Gaussian Splatting approaches in rendering quality, without increasing the storage footprint.",
    "arxiv_url": "https://arxiv.org/abs/2511.14357v1",
    "pdf_url": "https://arxiv.org/pdf/2511.14357v1",
    "published_date": "2025-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "face",
      "ar",
      "3d gaussian",
      "efficient",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.14357v1",
      "pdf": "https://arxiv.org/pdf/2511.14357v1"
    },
    "bibtex": ""
  },
  {
    "title": "Silhouette-to-Contour Registration: Aligning Intraoral Scan Models with Cephalometric Radiographs",
    "authors": [
      "Yiyi Miao",
      "Taoyu Wu",
      "Ji Jiang",
      "Tong Chen",
      "Zhe Tang",
      "Zhengyong Jiang",
      "Angelos Stefanidis",
      "Limin Yu",
      "Jionglong Su"
    ],
    "abstract": "Reliable 3D-2D alignment between intraoral scan (IOS) models and lateral cephalometric radiographs is critical for orthodontic diagnosis, yet conventional intensity-driven registration methods struggle under real clinical conditions, where cephalograms exhibit projective magnification, geometric distortion, low-contrast dental crowns, and acquisition-dependent variation. These factors hinder the stability of appearance-based similarity metrics and often lead to convergence failures or anatomically implausible alignments. To address these limitations, we propose DentalSCR, a pose-stable, contour-guided framework for accurate and interpretable silhouette-to-contour registration. Our method first constructs a U-Midline Dental Axis (UMDA) to establish a unified cross-arch anatomical coordinate system, thereby stabilizing initialization and standardizing projection geometry across cases. Using this reference frame, we generate radiograph-like projections via a surface-based DRR formulation with coronal-axis perspective and Gaussian splatting, which preserves clinical source-object-detector magnification and emphasizes external silhouettes. Registration is then formulated as a 2D similarity transform optimized with a symmetric bidirectional Chamfer distance under a hierarchical coarse-to-fine schedule, enabling both large capture range and subpixel-level contour agreement. We evaluate DentalSCR on 34 expert-annotated clinical cases. Experimental results demonstrate substantial reductions in landmark error-particularly at posterior teeth-tighter dispersion on the lower jaw, and low Chamfer and controlled Hausdorff distances at the curve level. These findings indicate that DentalSCR robustly handles real-world cephalograms and delivers high-fidelity, clinically inspectable 3D--2D alignment, outperforming conventional baselines.",
    "arxiv_url": "https://arxiv.org/abs/2511.14343v1",
    "pdf_url": "https://arxiv.org/pdf/2511.14343v1",
    "published_date": "2025-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "geometry",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.14343v1",
      "pdf": "https://arxiv.org/pdf/2511.14343v1"
    },
    "bibtex": ""
  },
  {
    "title": "Dental3R: Geometry-Aware Pairing for Intraoral 3D Reconstruction from Sparse-View Photographs",
    "authors": [
      "Yiyi Miao",
      "Taoyu Wu",
      "Tong Chen",
      "Ji Jiang",
      "Zhe Tang",
      "Zhengyong Jiang",
      "Angelos Stefanidis",
      "Limin Yu",
      "Jionglong Su"
    ],
    "abstract": "Intraoral 3D reconstruction is fundamental to digital orthodontics, yet conventional methods like intraoral scanning are inaccessible for remote tele-orthodontics, which typically relies on sparse smartphone imagery. While 3D Gaussian Splatting (3DGS) shows promise for novel view synthesis, its application to the standard clinical triad of unposed anterior and bilateral buccal photographs is challenging. The large view baselines, inconsistent illumination, and specular surfaces common in intraoral settings can destabilize simultaneous pose and geometry estimation. Furthermore, sparse-view photometric supervision often induces a frequency bias, leading to over-smoothed reconstructions that lose critical diagnostic details. To address these limitations, we propose \\textbf{Dental3R}, a pose-free, graph-guided pipeline for robust, high-fidelity reconstruction from sparse intraoral photographs. Our method first constructs a Geometry-Aware Pairing Strategy (GAPS) to intelligently select a compact subgraph of high-value image pairs. The GAPS focuses on correspondence matching, thereby improving the stability of the geometry initialization and reducing memory usage. Building on the recovered poses and point cloud, we train the 3DGS model with a wavelet-regularized objective. By enforcing band-limited fidelity using a discrete wavelet transform, our approach preserves fine enamel boundaries and interproximal edges while suppressing high-frequency artifacts. We validate our approach on a large-scale dataset of 950 clinical cases and an additional video-based test set of 195 cases. Experimental results demonstrate that Dental3R effectively handles sparse, unposed inputs and achieves superior novel view synthesis quality for dental occlusion visualization, outperforming state-of-the-art methods.",
    "arxiv_url": "https://arxiv.org/abs/2511.14315v1",
    "pdf_url": "https://arxiv.org/pdf/2511.14315v1",
    "published_date": "2025-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "compact",
      "sparse-view",
      "high-fidelity",
      "face",
      "3d reconstruction",
      "geometry",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.14315v1",
      "pdf": "https://arxiv.org/pdf/2511.14315v1"
    },
    "bibtex": ""
  },
  {
    "title": "GEN3D: Generating Domain-Free 3D Scenes from a Single Image",
    "authors": [
      "Yuxin Zhang",
      "Ziyu Lu",
      "Hongbo Duan",
      "Keyu Fan",
      "Pengting Luo",
      "Peiyu Zhuang",
      "Mengyu Yang",
      "Houde Liu"
    ],
    "abstract": "Despite recent advancements in neural 3D reconstruction, the dependence on dense multi-view captures restricts their broader applicability. Additionally, 3D scene generation is vital for advancing embodied AI and world models, which depend on diverse, high-quality scenes for learning and evaluation. In this work, we propose Gen3d, a novel method for generation of high-quality, wide-scope, and generic 3D scenes from a single image. After the initial point cloud is created by lifting the RGBD image, Gen3d maintains and expands its world model. The 3D scene is finalized through optimizing a Gaussian splatting representation. Extensive experiments on diverse datasets demonstrate the strong generalization capability and superior performance of our method in generating a world model and Synthesizing high-fidelity and consistent novel views.",
    "arxiv_url": "https://arxiv.org/abs/2511.14291v1",
    "pdf_url": "https://arxiv.org/pdf/2511.14291v1",
    "published_date": "2025-11-18",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "high-fidelity",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.14291v1",
      "pdf": "https://arxiv.org/pdf/2511.14291v1"
    },
    "bibtex": ""
  },
  {
    "title": "Gaussian Splatting-based Low-Rank Tensor Representation for Multi-Dimensional Image Recovery",
    "authors": [
      "Yiming Zeng",
      "Xi-Le Zhao",
      "Wei-Hao Wu",
      "Teng-Yu Ji",
      "Chao Wang"
    ],
    "abstract": "Tensor singular value decomposition (t-SVD) is a promising tool for multi-dimensional image representation, which decomposes a multi-dimensional image into a latent tensor and an accompanying transform matrix. However, two critical limitations of t-SVD methods persist: (1) the approximation of the latent tensor (e.g., tensor factorizations) is coarse and fails to accurately capture spatial local high-frequency information; (2) The transform matrix is composed of fixed basis atoms (e.g., complex exponential atoms in DFT and cosine atoms in DCT) and cannot precisely capture local high-frequency information along the mode-3 fibers. To address these two limitations, we propose a Gaussian Splatting-based Low-rank tensor Representation (GSLR) framework, which compactly and continuously represents multi-dimensional images. Specifically, we leverage tailored 2D Gaussian splatting and 1D Gaussian splatting to generate the latent tensor and transform matrix, respectively. The 2D and 1D Gaussian splatting are indispensable and complementary under this representation framework, which enjoys a powerful representation capability, especially for local high-frequency information. To evaluate the representation ability of the proposed GSLR, we develop an unsupervised GSLR-based multi-dimensional image recovery model. Extensive experiments on multi-dimensional image recovery demonstrate that GSLR consistently outperforms state-of-the-art methods, particularly in capturing local high-frequency information.",
    "arxiv_url": "https://arxiv.org/abs/2511.14270v2",
    "pdf_url": "https://arxiv.org/pdf/2511.14270v2",
    "published_date": "2025-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.14270v2",
      "pdf": "https://arxiv.org/pdf/2511.14270v2"
    },
    "bibtex": ""
  },
  {
    "title": "RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action",
    "authors": [
      "Xiaoquan Sun",
      "Ruijian Zhang",
      "Kang Pang",
      "Bingchen Miao",
      "Yuxiang Tan",
      "Zhen Yang",
      "Ming Li",
      "Jiayu Chen"
    ],
    "abstract": "Household tidying is an important application area, yet current benchmarks neither model user preferences nor support mobility, and they generalize poorly, making it hard to comprehensively assess integrated language-to-action capabilities. To address this, we propose RoboTidy, a unified benchmark for language-guided household tidying that supports Vision-Language-Action (VLA) and Vision-Language-Navigation (VLN) training and evaluation. RoboTidy provides 500 photorealistic 3D Gaussian Splatting (3DGS) household scenes (covering 500 objects and containers) with collisions, formulates tidying as an \"Action (Object, Container)\" list, and supplies 6.4k high-quality manipulation demonstration trajectories and 1.5k naviagtion trajectories to support both few-shot and large-scale training. We also deploy RoboTidy in the real world for object tidying, establishing an end-to-end benchmark for household tidying. RoboTidy offers a scalable platform and bridges a key gap in embodied AI by enabling holistic and realistic evaluation of language-guided robots.",
    "arxiv_url": "https://arxiv.org/abs/2511.14161v2",
    "pdf_url": "https://arxiv.org/pdf/2511.14161v2",
    "published_date": "2025-11-18",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "few-shot",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.14161v2",
      "pdf": "https://arxiv.org/pdf/2511.14161v2"
    },
    "bibtex": ""
  },
  {
    "title": "iGaussian: Real-Time Camera Pose Estimation via Feed-Forward 3D Gaussian Splatting Inversion",
    "authors": [
      "Hao Wang",
      "Linqing Zhao",
      "Xiuwei Xu",
      "Jiwen Lu",
      "Haibin Yan"
    ],
    "abstract": "Recent trends in SLAM and visual navigation have embraced 3D Gaussians as the preferred scene representation, highlighting the importance of estimating camera poses from a single image using a pre-built Gaussian model. However, existing approaches typically rely on an iterative \\textit{render-compare-refine} loop, where candidate views are first rendered using NeRF or Gaussian Splatting, then compared against the target image, and finally, discrepancies are used to update the pose. This multi-round process incurs significant computational overhead, hindering real-time performance in robotics. In this paper, we propose iGaussian, a two-stage feed-forward framework that achieves real-time camera pose estimation through direct 3D Gaussian inversion. Our method first regresses a coarse 6DoF pose using a Gaussian Scene Prior-based Pose Regression Network with spatial uniform sampling and guided attention mechanisms, then refines it through feature matching and multi-model fusion. The key contribution lies in our cross-correlation module that aligns image embeddings with 3D Gaussian attributes without differentiable rendering, coupled with a Weighted Multiview Predictor that fuses features from Multiple strategically sampled viewpoints. Experimental results on the NeRF Synthetic, Mip-NeRF 360, and T\\&T+DB datasets demonstrate a significant performance improvement over previous methods, reducing median rotation errors to 0.2° while achieving 2.87 FPS tracking on mobile robots, which is an impressive 10 times speedup compared to optimization-based approaches. Code: https://github.com/pythongod-exe/iGaussian",
    "arxiv_url": "https://arxiv.org/abs/2511.14149v1",
    "pdf_url": "https://arxiv.org/pdf/2511.14149v1",
    "published_date": "2025-11-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/pythongod-exe/iGaussian",
    "keywords": [
      "head",
      "robotics",
      "tracking",
      "ar",
      "nerf",
      "3d gaussian",
      "slam",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.14149v1",
      "pdf": "https://arxiv.org/pdf/2511.14149v1",
      "github": "https://github.com/pythongod-exe/iGaussian"
    },
    "bibtex": ""
  },
  {
    "title": "Splat Regression Models",
    "authors": [
      "Mara Daniels",
      "Philippe Rigollet"
    ],
    "abstract": "We introduce a highly expressive class of function approximators called Splat Regression Models. Model outputs are mixtures of heterogeneous and anisotropic bump functions, termed splats, each weighted by an output vector. The power of splat modeling lies in its ability to locally adjust the scale and direction of each splat, achieving both high interpretability and accuracy. Fitting splat models reduces to optimization over the space of mixing measures, which can be implemented using Wasserstein-Fisher-Rao gradient flows. As a byproduct, we recover the popular Gaussian Splatting methodology as a special case, providing a unified theoretical framework for this state-of-the-art technique that clearly disambiguates the inverse problem, the model, and the optimization algorithm. Through numerical experiments, we demonstrate that the resulting models and algorithms constitute a flexible and promising approach for solving diverse approximation, estimation, and inverse problems involving low-dimensional data.",
    "arxiv_url": "https://arxiv.org/abs/2511.14042v1",
    "pdf_url": "https://arxiv.org/pdf/2511.14042v1",
    "published_date": "2025-11-18",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.14042v1",
      "pdf": "https://arxiv.org/pdf/2511.14042v1"
    },
    "bibtex": ""
  },
  {
    "title": "Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting",
    "authors": [
      "Jiangnan Ye",
      "Jiedong Zhuang",
      "Lianrui Mu",
      "Wenjie Zheng",
      "Jiaqi Hu",
      "Xingze Zou",
      "Jing Wang",
      "Haoji Hu"
    ],
    "abstract": "We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.",
    "arxiv_url": "https://arxiv.org/abs/2511.13684v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13684v1",
    "published_date": "2025-11-17",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "illumination",
      "outdoor",
      "high-fidelity",
      "face",
      "geometry",
      "segmentation",
      "ar",
      "efficient",
      "semantic",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.13684v1",
      "pdf": "https://arxiv.org/pdf/2511.13684v1"
    },
    "bibtex": ""
  },
  {
    "title": "Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation",
    "authors": [
      "Ziyang Huang",
      "Jiagang Chen",
      "Jin Liu",
      "Shunping Ji"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a leading framework for novel view synthesis, yet its core optimization challenges remain underexplored. We identify two key issues in 3DGS optimization: entrapment in suboptimal local optima and insufficient convergence quality. To address these, we propose Opt3DGS, a robust framework that enhances 3DGS through a two-stage optimization process of adaptive exploration and curvature-guided exploitation. In the exploration phase, an Adaptive Weighted Stochastic Gradient Langevin Dynamics (SGLD) method enhances global search to escape local optima. In the exploitation phase, a Local Quasi-Newton Direction-guided Adam optimizer leverages curvature information for precise and efficient convergence. Extensive experiments on diverse benchmark datasets demonstrate that Opt3DGS achieves state-of-the-art rendering quality by refining the 3DGS optimization process without modifying its underlying representation.",
    "arxiv_url": "https://arxiv.org/abs/2511.13571v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13571v1",
    "published_date": "2025-11-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.13571v1",
      "pdf": "https://arxiv.org/pdf/2511.13571v1"
    },
    "bibtex": ""
  },
  {
    "title": "SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting",
    "authors": [
      "Zihan Li",
      "Tengfei Wang",
      "Wentian Gan",
      "Hao Zhan",
      "Xin Wang",
      "Zongqian Zhan"
    ],
    "abstract": "Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:https://lzh282140127-cell.github.io/SF-Recon-project/",
    "arxiv_url": "https://arxiv.org/abs/2511.13278v2",
    "pdf_url": "https://arxiv.org/pdf/2511.13278v2",
    "published_date": "2025-11-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "face",
      "geometry",
      "ar",
      "3d gaussian",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.13278v2",
      "pdf": "https://arxiv.org/pdf/2511.13278v2",
      "project": "https://lzh282140127-cell.github.io/SF-Recon-project"
    },
    "bibtex": ""
  },
  {
    "title": "SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression",
    "authors": [
      "Keshav Gupta",
      "Akshat Sanghvi",
      "Shreyas Reddy Palley",
      "Astitva Srivastava",
      "Charu Sharma",
      "Avinash Sharma"
    ],
    "abstract": "3D Gaussian Splatting has emerged as a transformative technique in novel view synthesis, primarily due to its high rendering speed and photorealistic fidelity. However, its memory footprint scales rapidly with scene complexity, often reaching several gigabytes. Existing methods address this issue by introducing compression strategies that exploit primitive-level redundancy through similarity detection and quantization. We aim to surpass the compression limits of such methods by incorporating symmetry-aware techniques, specifically targeting mirror symmetries to eliminate redundant primitives. We propose a novel compression framework, SymGS, introducing learnable mirrors into the scene, thereby eliminating local and global reflective redundancies for compression. Our framework functions as a plug-and-play enhancement to state-of-the-art compression methods, (e.g. HAC) to achieve further compression. Compared to HAC, we achieve $1.66 \\times$ compression across benchmark datasets (upto $3\\times$ on large-scale scenes). On an average, SymGS enables $\\bf{108\\times}$ compression of a 3DGS scene, while preserving rendering quality. The project page and supplementary can be found at symgs.github.io",
    "arxiv_url": "https://arxiv.org/abs/2511.13264v2",
    "pdf_url": "https://arxiv.org/pdf/2511.13264v2",
    "published_date": "2025-11-17",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "compression",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.13264v2",
      "pdf": "https://arxiv.org/pdf/2511.13264v2"
    },
    "bibtex": ""
  },
  {
    "title": "Monocular 3D Lane Detection via Structure Uncertainty-Aware Network with Curve-Point Queries",
    "authors": [
      "Ruixin Liu",
      "Zejian Yuan"
    ],
    "abstract": "Monocular 3D lane detection is challenged by aleatoric uncertainty arising from inherent observation noise. Existing methods rely on simplified geometric assumptions, such as independent point predictions or global planar modeling, failing to capture structural variations and aleatoric uncertainty in real-world scenarios. In this paper, we propose MonoUnc, a bird's-eye view (BEV)-free 3D lane detector that explicitly models aleatoric uncertainty informed by local lane structures. Specifically, 3D lanes are projected onto the front-view (FV) space and approximated by parametric curves. Guided by curve predictions, curve-point query embeddings are dynamically generated for lane point predictions in 3D space. Each segment formed by two adjacent points is modeled as a 3D Gaussian, parameterized by the local structure and uncertainty estimations. Accordingly, a novel 3D Gaussian matching loss is designed to constrain these parameters jointly. Experiments on the ONCE-3DLanes and OpenLane datasets demonstrate that MonoUnc outperforms previous state-of-the-art (SoTA) methods across all benchmarks under stricter evaluation criteria. Additionally, we propose two comprehensive evaluation metrics for ONCE-3DLanes, calculating the average and maximum bidirectional Chamfer distances to quantify global and local errors. Codes are released at https://github.com/lrx02/MonoUnc.",
    "arxiv_url": "https://arxiv.org/abs/2511.13055v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13055v1",
    "published_date": "2025-11-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/lrx02/MonoUnc",
    "keywords": [
      "3d gaussian",
      "ar",
      "dynamic"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.13055v1",
      "pdf": "https://arxiv.org/pdf/2511.13055v1",
      "github": "https://github.com/lrx02/MonoUnc"
    },
    "bibtex": ""
  },
  {
    "title": "Beyond Darkness: Thermal-Supervised 3D Gaussian Splatting for Low-Light Novel View Synthesis",
    "authors": [
      "Qingsen Ma",
      "Chen Zou",
      "Dianyun Wang",
      "Jia Wang",
      "Liuyu Xiang",
      "Zhaofeng He"
    ],
    "abstract": "Under extremely low-light conditions, novel view synthesis (NVS) faces severe degradation in terms of geometry, color consistency, and radiometric stability. Standard 3D Gaussian Splatting (3DGS) pipelines fail when applied directly to underexposed inputs, as independent enhancement across views causes illumination inconsistencies and geometric distortion. To address this, we present DTGS, a unified framework that tightly couples Retinex-inspired illumination decomposition with thermal-guided 3D Gaussian Splatting for illumination-invariant reconstruction. Unlike prior approaches that treat enhancement as a pre-processing step, DTGS performs joint optimization across enhancement, geometry, and thermal supervision through a cyclic enhancement-reconstruction mechanism. A thermal supervisory branch stabilizes both color restoration and geometry learning by dynamically balancing enhancement, structural, and thermal losses. Moreover, a Retinex-based decomposition module embedded within the 3DGS loop provides physically interpretable reflectance-illumination separation, ensuring consistent color and texture across viewpoints. To evaluate our method, we construct RGBT-LOW, a new multi-view low-light thermal dataset capturing severe illumination degradation. Extensive experiments show that DTGS significantly outperforms existing low-light enhancement and 3D reconstruction baselines, achieving superior radiometric consistency, geometric fidelity, and color stability under extreme illumination.",
    "arxiv_url": "https://arxiv.org/abs/2511.13011v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13011v1",
    "published_date": "2025-11-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "dynamic",
      "face",
      "3d reconstruction",
      "geometry",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.13011v1",
      "pdf": "https://arxiv.org/pdf/2511.13011v1"
    },
    "bibtex": ""
  },
  {
    "title": "TR-Gaussians: High-fidelity Real-time Rendering of Planar Transmission and Reflection with 3D Gaussian Splatting",
    "authors": [
      "Yong Liu",
      "Keyang Ye",
      "Tianjia Shao",
      "Kun Zhou"
    ],
    "abstract": "We propose Transmission-Reflection Gaussians (TR-Gaussians), a novel 3D-Gaussian-based representation for high-fidelity rendering of planar transmission and reflection, which are ubiquitous in indoor scenes. Our method combines 3D Gaussians with learnable reflection planes that explicitly model the glass planes with view-dependent reflectance strengths. Real scenes and transmission components are modeled by 3D Gaussians and the reflection components are modeled by the mirrored Gaussians with respect to the reflection plane. The transmission and reflection components are blended according to a Fresnel-based, view-dependent weighting scheme, allowing for faithful synthesis of complex appearance effects under varying viewpoints. To effectively optimize TR-Gaussians, we develop a multi-stage optimization framework incorporating color and geometry constraints and an opacity perturbation mechanism. Experiments on different datasets demonstrate that TR-Gaussians achieve real-time, high-fidelity novel view synthesis in scenes with planar transmission and reflection, and outperform state-of-the-art approaches both quantitatively and qualitatively.",
    "arxiv_url": "https://arxiv.org/abs/2511.13009v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13009v1",
    "published_date": "2025-11-17",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "real-time rendering",
      "geometry",
      "ar",
      "reflection",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.13009v1",
      "pdf": "https://arxiv.org/pdf/2511.13009v1"
    },
    "bibtex": ""
  },
  {
    "title": "SplatSearch: Instance Image Goal Navigation for Mobile Robots using 3D Gaussian Splatting and Diffusion Models",
    "authors": [
      "Siddarth Narasimhan",
      "Matthew Lisondra",
      "Haitong Wang",
      "Goldie Nejat"
    ],
    "abstract": "The Instance Image Goal Navigation (IIN) problem requires mobile robots deployed in unknown environments to search for specific objects or people of interest using only a single reference goal image of the target. This problem can be especially challenging when: 1) the reference image is captured from an arbitrary viewpoint, and 2) the robot must operate with sparse-view scene reconstructions. In this paper, we address the IIN problem, by introducing SplatSearch, a novel architecture that leverages sparse-view 3D Gaussian Splatting (3DGS) reconstructions. SplatSearch renders multiple viewpoints around candidate objects using a sparse online 3DGS map, and uses a multi-view diffusion model to complete missing regions of the rendered images, enabling robust feature matching against the goal image. A novel frontier exploration policy is introduced which uses visual context from the synthesized viewpoints with semantic context from the goal image to evaluate frontier locations, allowing the robot to prioritize frontiers that are semantically and visually relevant to the goal image. Extensive experiments in photorealistic home and real-world environments validate the higher performance of SplatSearch against current state-of-the-art methods in terms of Success Rate and Success Path Length. An ablation study confirms the design choices of SplatSearch.",
    "arxiv_url": "https://arxiv.org/abs/2511.12972v1",
    "pdf_url": "https://arxiv.org/pdf/2511.12972v1",
    "published_date": "2025-11-17",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "sparse-view",
      "ar",
      "semantic",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.12972v1",
      "pdf": "https://arxiv.org/pdf/2511.12972v1"
    },
    "bibtex": ""
  },
  {
    "title": "GUIDE: Gaussian Unified Instance Detection for Enhanced Obstacle Perception in Autonomous Driving",
    "authors": [
      "Chunyong Hu",
      "Qi Luo",
      "Jianyun Xu",
      "Song Wang",
      "Qiang Li",
      "Sheng Yang"
    ],
    "abstract": "In the realm of autonomous driving, accurately detecting surrounding obstacles is crucial for effective decision-making. Traditional methods primarily rely on 3D bounding boxes to represent these obstacles, which often fail to capture the complexity of irregularly shaped, real-world objects. To overcome these limitations, we present GUIDE, a novel framework that utilizes 3D Gaussians for instance detection and occupancy prediction. Unlike conventional occupancy prediction methods, GUIDE also offers robust tracking capabilities. Our framework employs a sparse representation strategy, using Gaussian-to-Voxel Splatting to provide fine-grained, instance-level occupancy data without the computational demands associated with dense voxel grids. Experimental validation on the nuScenes dataset demonstrates GUIDE's performance, with an instance occupancy mAP of 21.61, marking a 50\\% improvement over existing methods, alongside competitive tracking capabilities. GUIDE establishes a new benchmark in autonomous perception systems, effectively combining precision with computational efficiency to better address the complexities of real-world driving environments.",
    "arxiv_url": "https://arxiv.org/abs/2511.12941v1",
    "pdf_url": "https://arxiv.org/pdf/2511.12941v1",
    "published_date": "2025-11-17",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "autonomous driving",
      "ar",
      "tracking"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.12941v1",
      "pdf": "https://arxiv.org/pdf/2511.12941v1"
    },
    "bibtex": ""
  },
  {
    "title": "Neo: Real-Time On-Device 3D Gaussian Splatting with Reuse-and-Update Sorting Acceleration",
    "authors": [
      "Changhun Oh",
      "Seongryong Oh",
      "Jinwoo Hwang",
      "Yoonsung Kim",
      "Hardik Sharma",
      "Jongse Park"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) rendering in real-time on resource-constrained devices is essential for delivering immersive augmented and virtual reality (AR/VR) experiences. However, existing solutions struggle to achieve high frame rates, especially for high-resolution rendering. Our analysis identifies the sorting stage in the 3DGS rendering pipeline as the major bottleneck due to its high memory bandwidth demand. This paper presents Neo, which introduces a reuse-and-update sorting algorithm that exploits temporal redundancy in Gaussian ordering across consecutive frames, and devises a hardware accelerator optimized for this algorithm. By efficiently tracking and updating Gaussian depth ordering instead of re-sorting from scratch, Neo significantly reduces redundant computations and memory bandwidth pressure. Experimental results show that Neo achieves up to 10.0x and 5.6x higher throughput than state-of-the-art edge GPU and ASIC solution, respectively, while reducing DRAM traffic by 94.5% and 81.3%. These improvements make high-quality and low-latency on-device 3D rendering more practical.",
    "arxiv_url": "https://arxiv.org/abs/2511.12930v1",
    "pdf_url": "https://arxiv.org/pdf/2511.12930v1",
    "published_date": "2025-11-17",
    "categories": [
      "cs.AR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "acceleration",
      "tracking",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.12930v1",
      "pdf": "https://arxiv.org/pdf/2511.12930v1"
    },
    "bibtex": ""
  },
  {
    "title": "Reconstructing 3D Scenes in Native High Dynamic Range",
    "authors": [
      "Kaixuan Zhang",
      "Minxian Li",
      "Mingwu Ren",
      "Jiankang Deng",
      "Xiatian Zhu"
    ],
    "abstract": "High Dynamic Range (HDR) imaging is essential for professional digital media creation, e.g., filmmaking, virtual production, and photorealistic rendering. However, 3D scene reconstruction has primarily focused on Low Dynamic Range (LDR) data, limiting its applicability to professional workflows. Existing approaches that reconstruct HDR scenes from LDR observations rely on multi-exposure fusion or inverse tone-mapping, which increase capture complexity and depend on synthetic supervision. With the recent emergence of cameras that directly capture native HDR data in a single exposure, we present the first method for 3D scene reconstruction that directly models native HDR observations. We propose {\\bf Native High dynamic range 3D Gaussian Splatting (NH-3DGS)}, which preserves the full dynamic range throughout the reconstruction pipeline. Our key technical contribution is a novel luminance-chromaticity decomposition of the color representation that enables direct optimization from native HDR camera data. We demonstrate on both synthetic and real multi-view HDR datasets that NH-3DGS significantly outperforms existing methods in reconstruction quality and dynamic range preservation, enabling professional-grade 3D reconstruction directly from native HDR captures. Code and datasets will be made available.",
    "arxiv_url": "https://arxiv.org/abs/2511.12895v1",
    "pdf_url": "https://arxiv.org/pdf/2511.12895v1",
    "published_date": "2025-11-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "3d reconstruction",
      "ar",
      "3d gaussian",
      "mapping",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.12895v1",
      "pdf": "https://arxiv.org/pdf/2511.12895v1"
    },
    "bibtex": ""
  },
  {
    "title": "Changes in Real Time: Online Scene Change Detection with Multi-View Fusion",
    "authors": [
      "Chamuditha Jayanga Galappaththige",
      "Jason Lai",
      "Lloyd Windrim",
      "Donald Dansereau",
      "Niko Sünderhauf",
      "Dimity Miller"
    ],
    "abstract": "Online Scene Change Detection (SCD) is an extremely challenging problem that requires an agent to detect relevant changes on the fly while observing the scene from unconstrained viewpoints. Existing online SCD methods are significantly less accurate than offline approaches. We present the first online SCD approach that is pose-agnostic, label-free, and ensures multi-view consistency, while operating at over 10 FPS and achieving new state-of-the-art performance, surpassing even the best offline approaches. Our method introduces a new self-supervised fusion loss to infer scene changes from multiple cues and observations, PnP-based fast pose estimation against the reference scene, and a fast change-guided update strategy for the 3D Gaussian Splatting scene representation. Extensive experiments on complex real-world datasets demonstrate that our approach outperforms both online and offline baselines.",
    "arxiv_url": "https://arxiv.org/abs/2511.12370v2",
    "pdf_url": "https://arxiv.org/pdf/2511.12370v2",
    "published_date": "2025-11-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.12370v2",
      "pdf": "https://arxiv.org/pdf/2511.12370v2"
    },
    "bibtex": ""
  },
  {
    "title": "LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors",
    "authors": [
      "Qifeng Chen",
      "Jiarun Liu",
      "Rengan Xie",
      "Tao Tang",
      "Sicong Du",
      "Yiru Zhao",
      "Yuchi Huo",
      "Sheng Yang"
    ],
    "abstract": "Recent GS-based rendering has made significant progress for LiDAR, surpassing Neural Radiance Fields (NeRF) in both quality and speed. However, these methods exhibit artifacts in extrapolated novel view synthesis due to the incomplete reconstruction from single traversal scans. To address this limitation, we present LiDAR-GS++, a LiDAR Gaussian Splatting reconstruction method enhanced by diffusion priors for real-time and high-fidelity re-simulation on public urban roads. Specifically, we introduce a controllable LiDAR generation model conditioned on coarsely extrapolated rendering to produce extra geometry-consistent scans and employ an effective distillation mechanism for expansive reconstruction. By extending reconstruction to under-fitted regions, our approach ensures global geometric consistency for extrapolative novel views while preserving detailed scene surfaces captured by sensors. Experiments on multiple public datasets demonstrate that LiDAR-GS++ achieves state-of-the-art performance for both interpolated and extrapolated viewpoints, surpassing existing GS and NeRF-based methods.",
    "arxiv_url": "https://arxiv.org/abs/2511.12304v1",
    "pdf_url": "https://arxiv.org/pdf/2511.12304v1",
    "published_date": "2025-11-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "geometry",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.12304v1",
      "pdf": "https://arxiv.org/pdf/2511.12304v1"
    },
    "bibtex": ""
  },
  {
    "title": "SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images",
    "authors": [
      "Xinyuan Hu",
      "Changyue Shi",
      "Chuxiao Yang",
      "Minghao Chen",
      "Jiajun Ding",
      "Tao Wei",
      "Chen Wei",
      "Zhou Yu",
      "Min Tan"
    ],
    "abstract": "Feed-forward 3D reconstruction from sparse, low-resolution (LR) images is a crucial capability for real-world applications, such as autonomous driving and embodied AI. However, existing methods often fail to recover fine texture details. This limitation stems from the inherent lack of high-frequency information in LR inputs. To address this, we propose \\textbf{SRSplat}, a feed-forward framework that reconstructs high-resolution 3D scenes from only a few LR views. Our main insight is to compensate for the deficiency of texture information by jointly leveraging external high-quality reference images and internal texture cues. We first construct a scene-specific reference gallery, generated for each scene using Multimodal Large Language Models (MLLMs) and diffusion models. To integrate this external information, we introduce the \\textit{Reference-Guided Feature Enhancement (RGFE)} module, which aligns and fuses features from the LR input images and their reference twin image. Subsequently, we train a decoder to predict the Gaussian primitives using the multi-view fused feature obtained from \\textit{RGFE}. To further refine predicted Gaussian primitives, we introduce \\textit{Texture-Aware Density Control (TADC)}, which adaptively adjusts Gaussian density based on the internal texture richness of the LR inputs. Extensive experiments demonstrate that our SRSplat outperforms existing methods on various datasets, including RealEstate10K, ACID, and DTU, and exhibits strong cross-dataset and cross-resolution generalization capabilities.",
    "arxiv_url": "https://arxiv.org/abs/2511.12040v1",
    "pdf_url": "https://arxiv.org/pdf/2511.12040v1",
    "published_date": "2025-11-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "autonomous driving",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.12040v1",
      "pdf": "https://arxiv.org/pdf/2511.12040v1"
    },
    "bibtex": ""
  },
  {
    "title": "3D Gaussian and Diffusion-Based Gaze Redirection",
    "authors": [
      "Abiram Panchalingam",
      "Indu Bodala",
      "Stuart Middleton"
    ],
    "abstract": "High-fidelity gaze redirection is critical for generating augmented data to improve the generalization of gaze estimators. 3D Gaussian Splatting (3DGS) models like GazeGaussian represent the state-of-the-art but can struggle with rendering subtle, continuous gaze shifts. In this paper, we propose DiT-Gaze, a framework that enhances 3D gaze redirection models using a novel combination of Diffusion Transformer (DiT), weak supervision across gaze angles, and an orthogonality constraint loss. DiT allows higher-fidelity image synthesis, while our weak supervision strategy using synthetically generated intermediate gaze angles provides a smooth manifold of gaze directions during training. The orthogonality constraint loss mathematically enforces the disentanglement of internal representations for gaze, head pose, and expression. Comprehensive experiments show that DiT-Gaze sets a new state-of-the-art in both perceptual quality and redirection accuracy, reducing the state-of-the-art gaze error by 4.1% to 6.353 degrees, providing a superior method for creating synthetic training data. Our code and models will be made available for the research community to benchmark against.",
    "arxiv_url": "https://arxiv.org/abs/2511.11231v1",
    "pdf_url": "https://arxiv.org/pdf/2511.11231v1",
    "published_date": "2025-11-14",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.11231v1",
      "pdf": "https://arxiv.org/pdf/2511.11231v1"
    },
    "bibtex": ""
  },
  {
    "title": "RealisticDreamer: Guidance Score Distillation for Few-shot Gaussian Splatting",
    "authors": [
      "Ruocheng Wu",
      "Haolan He",
      "Yufei Wang",
      "Zhihao Li",
      "Bihan Wen"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently gained great attention in the 3D scene representation for its high-quality real-time rendering capabilities. However, when the input comprises sparse training views, 3DGS is prone to overfitting, primarily due to the lack of intermediate-view supervision. Inspired by the recent success of Video Diffusion Models (VDM), we propose a framework called Guidance Score Distillation (GSD) to extract the rich multi-view consistency priors from pretrained VDMs. Building on the insights from Score Distillation Sampling (SDS), GSD supervises rendered images from multiple neighboring views, guiding the Gaussian splatting representation towards the generative direction of VDM. However, the generative direction often involves object motion and random camera trajectories, making it challenging for direct supervision in the optimization process. To address this problem, we introduce an unified guidance form to correct the noise prediction result of VDM. Specifically, we incorporate both a depth warp guidance based on real depth maps and a guidance based on semantic image features, ensuring that the score update direction from VDM aligns with the correct camera pose and accurate geometry. Experimental results show that our method outperforms existing approaches across multiple datasets.",
    "arxiv_url": "https://arxiv.org/abs/2511.11213v1",
    "pdf_url": "https://arxiv.org/pdf/2511.11213v1",
    "published_date": "2025-11-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "few-shot",
      "real-time rendering",
      "geometry",
      "gaussian splatting",
      "ar",
      "semantic",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.11213v1",
      "pdf": "https://arxiv.org/pdf/2511.11213v1"
    },
    "bibtex": ""
  },
  {
    "title": "Dynamic Gaussian Scene Reconstruction from Unsynchronized Videos",
    "authors": [
      "Zhixin Xu",
      "Hengyu Zhou",
      "Yuan Liu",
      "Wenhan Xue",
      "Hao Pan",
      "Wenping Wang",
      "Bin Wang"
    ],
    "abstract": "Multi-view video reconstruction plays a vital role in computer vision, enabling applications in film production, virtual reality, and motion analysis. While recent advances such as 4D Gaussian Splatting (4DGS) have demonstrated impressive capabilities in dynamic scene reconstruction, they typically rely on the assumption that input video streams are temporally synchronized. However, in real-world scenarios, this assumption often fails due to factors like camera trigger delays or independent recording setups, leading to temporal misalignment across views and reduced reconstruction quality. To address this challenge, a novel temporal alignment strategy is proposed for high-quality 4DGS reconstruction from unsynchronized multi-view videos. Our method features a coarse-to-fine alignment module that estimates and compensates for each camera's time shift. The method first determines a coarse, frame-level offset and then refines it to achieve sub-frame accuracy. This strategy can be integrated as a readily integrable module into existing 4DGS frameworks, enhancing their robustness when handling asynchronous data. Experiments show that our approach effectively processes temporally misaligned videos and significantly enhances baseline methods.",
    "arxiv_url": "https://arxiv.org/abs/2511.11175v1",
    "pdf_url": "https://arxiv.org/pdf/2511.11175v1",
    "published_date": "2025-11-14",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "gaussian splatting",
      "ar",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.11175v1",
      "pdf": "https://arxiv.org/pdf/2511.11175v1"
    },
    "bibtex": ""
  },
  {
    "title": "PINGS-X: Physics-Informed Normalized Gaussian Splatting with Axes Alignment for Efficient Super-Resolution of 4D Flow MRI",
    "authors": [
      "Sun Jo",
      "Seok Young Hong",
      "JinHyun Kim",
      "Seungmin Kang",
      "Ahjin Choi",
      "Don-Gwan An",
      "Simon Song",
      "Je Hyeong Hong"
    ],
    "abstract": "4D flow magnetic resonance imaging (MRI) is a reliable, non-invasive approach for estimating blood flow velocities, vital for cardiovascular diagnostics. Unlike conventional MRI focused on anatomical structures, 4D flow MRI requires high spatiotemporal resolution for early detection of critical conditions such as stenosis or aneurysms. However, achieving such resolution typically results in prolonged scan times, creating a trade-off between acquisition speed and prediction accuracy. Recent studies have leveraged physics-informed neural networks (PINNs) for super-resolution of MRI data, but their practical applicability is limited as the prohibitively slow training process must be performed for each patient. To overcome this limitation, we propose PINGS-X, a novel framework modeling high-resolution flow velocities using axes-aligned spatiotemporal Gaussian representations. Inspired by the effectiveness of 3D Gaussian splatting (3DGS) in novel view synthesis, PINGS-X extends this concept through several non-trivial novel innovations: (i) normalized Gaussian splatting with a formal convergence guarantee, (ii) axes-aligned Gaussians that simplify training for high-dimensional data while preserving accuracy and the convergence guarantee, and (iii) a Gaussian merging procedure to prevent degenerate solutions and boost computational efficiency. Experimental results on computational fluid dynamics (CFD) and real 4D flow MRI datasets demonstrate that PINGS-X substantially reduces training time while achieving superior super-resolution accuracy. Our code and datasets are available at https://github.com/SpatialAILab/PINGS-X.",
    "arxiv_url": "https://arxiv.org/abs/2511.11048v2",
    "pdf_url": "https://arxiv.org/pdf/2511.11048v2",
    "published_date": "2025-11-14",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "https://github.com/SpatialAILab/PINGS-X",
    "keywords": [
      "4d",
      "dynamic",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.11048v2",
      "pdf": "https://arxiv.org/pdf/2511.11048v2",
      "github": "https://github.com/SpatialAILab/PINGS-X"
    },
    "bibtex": ""
  },
  {
    "title": "Depth-Consistent 3D Gaussian Splatting via Physical Defocus Modeling and Multi-View Geometric Supervision",
    "authors": [
      "Yu Deng",
      "Baozhu Zhao",
      "Junyan Su",
      "Xiaohan Zhang",
      "Qi Liu"
    ],
    "abstract": "Three-dimensional reconstruction in scenes with extreme depth variations remains challenging due to inconsistent supervisory signals between near-field and far-field regions. Existing methods fail to simultaneously address inaccurate depth estimation in distant areas and structural degradation in close-range regions. This paper proposes a novel computational framework that integrates depth-of-field supervision and multi-view consistency supervision to advance 3D Gaussian Splatting. Our approach comprises two core components: (1) Depth-of-field Supervision employs a scale-recovered monocular depth estimator (e.g., Metric3D) to generate depth priors, leverages defocus convolution to synthesize physically accurate defocused images, and enforces geometric consistency through a novel depth-of-field loss, thereby enhancing depth fidelity in both far-field and near-field regions; (2) Multi-View Consistency Supervision employing LoFTR-based semi-dense feature matching to minimize cross-view geometric errors and enforce depth consistency via least squares optimization of reliable matched points. By unifying defocus physics with multi-view geometric constraints, our method achieves superior depth fidelity, demonstrating a 0.8 dB PSNR improvement over the state-of-the-art method on the Waymo Open Dataset. This framework bridges physical imaging principles and learning-based depth regularization, offering a scalable solution for complex depth stratification in urban environments.",
    "arxiv_url": "https://arxiv.org/abs/2511.10316v1",
    "pdf_url": "https://arxiv.org/pdf/2511.10316v1",
    "published_date": "2025-11-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.10316v1",
      "pdf": "https://arxiv.org/pdf/2511.10316v1"
    },
    "bibtex": ""
  },
  {
    "title": "Multivariate Gaussian Representation Learning for Medical Action Evaluation",
    "authors": [
      "Luming Yang",
      "Haoxian Liu",
      "Siqing Li",
      "Alper Yilmaz"
    ],
    "abstract": "Fine-grained action evaluation in medical vision faces unique challenges due to the unavailability of comprehensive datasets, stringent precision requirements, and insufficient spatiotemporal dynamic modeling of very rapid actions. To support development and evaluation, we introduce CPREval-6k, a multi-view, multi-label medical action benchmark containing 6,372 expert-annotated videos with 22 clinical labels. Using this dataset, we present GaussMedAct, a multivariate Gaussian encoding framework, to advance medical motion analysis through adaptive spatiotemporal representation learning. Multivariate Gaussian Representation projects the joint motions to a temporally scaled multi-dimensional space, and decomposes actions into adaptive 3D Gaussians that serve as tokens. These tokens preserve motion semantics through anisotropic covariance modeling while maintaining robustness to spatiotemporal noise. Hybrid Spatial Encoding, employing a Cartesian and Vector dual-stream strategy, effectively utilizes skeletal information in the form of joint and bone features. The proposed method achieves 92.1% Top-1 accuracy with real-time inference on the benchmark, outperforming baseline by +5.9% accuracy with only 10% FLOPs. Cross-dataset experiments confirm the superiority of our method in robustness.",
    "arxiv_url": "https://arxiv.org/abs/2511.10060v2",
    "pdf_url": "https://arxiv.org/pdf/2511.10060v2",
    "published_date": "2025-11-13",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "face",
      "medical",
      "ar",
      "semantic",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.10060v2",
      "pdf": "https://arxiv.org/pdf/2511.10060v2"
    },
    "bibtex": ""
  },
  {
    "title": "TSPE-GS: Probabilistic Depth Extraction for Semi-Transparent Surface Reconstruction via 3D Gaussian Splatting",
    "authors": [
      "Zhiyuan Xu",
      "Nan Min",
      "Yuhang Guo",
      "Tong Wei"
    ],
    "abstract": "3D Gaussian Splatting offers a strong speed-quality trade-off but struggles to reconstruct semi-transparent surfaces because most methods assume a single depth per pixel, which fails when multiple surfaces are visible. We propose TSPE-GS (Transparent Surface Probabilistic Extraction for Gaussian Splatting), which uniformly samples transmittance to model a pixel-wise multi-modal distribution of opacity and depth, replacing the prior single-peak assumption and resolving cross-surface depth ambiguity. By progressively fusing truncated signed distance functions, TSPE-GS reconstructs external and internal surfaces separately within a unified framework. The method generalizes to other Gaussian-based reconstruction pipelines without extra training overhead. Extensive experiments on public and self-collected semi-transparent and opaque datasets show TSPE-GS significantly improves semi-transparent geometry reconstruction while maintaining performance on opaque scenes.",
    "arxiv_url": "https://arxiv.org/abs/2511.09944v1",
    "pdf_url": "https://arxiv.org/pdf/2511.09944v1",
    "published_date": "2025-11-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "face",
      "geometry",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.09944v1",
      "pdf": "https://arxiv.org/pdf/2511.09944v1"
    },
    "bibtex": ""
  },
  {
    "title": "AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting",
    "authors": [
      "Aymen Mir",
      "Jian Wang",
      "Riza Alp Guler",
      "Chuan Guo",
      "Gerard Pons-Moll",
      "Bing Zhou"
    ],
    "abstract": "We present a novel framework for animating humans in 3D scenes using 3D Gaussian Splatting (3DGS), a neural scene representation that has recently achieved state-of-the-art photorealistic results for novel-view synthesis but remains under-explored for human-scene animation and interaction. Unlike existing animation pipelines that use meshes or point clouds as the underlying 3D representation, our approach introduces the use of 3DGS as the 3D representation for animating humans in scenes. By representing humans and scenes as Gaussians, our approach allows geometry-consistent free-viewpoint rendering of humans interacting with 3D scenes. Our key insight is that rendering can be decoupled from motion synthesis, and each sub-problem can be addressed independently without the need for paired human-scene data. Central to our method is a Gaussian-aligned motion module that synthesizes motion without explicit scene geometry, using opacity-based cues and projected Gaussian structures to guide human placement and pose alignment. To ensure natural interactions, we further propose a human-scene Gaussian refinement optimization that enforces realistic contact and navigation. We evaluate our approach on scenes from Scannet++ and the SuperSplat library, and on avatars reconstructed from sparse and dense multi-view human capture. Finally, we demonstrate that our framework enables novel applications such as geometry-consistent free-viewpoint rendering of edited monocular RGB videos with newly animated humans, showcasing the unique advantages of 3DGS for monocular video-based human animation. To assess the full quality of our results, we encourage readers to view the supplementary material available at https://miraymen.github.io/aha/ .",
    "arxiv_url": "https://arxiv.org/abs/2511.09827v2",
    "pdf_url": "https://arxiv.org/pdf/2511.09827v2",
    "published_date": "2025-11-13",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "animation",
      "avatar",
      "geometry",
      "gaussian splatting",
      "ar",
      "human",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.09827v2",
      "pdf": "https://arxiv.org/pdf/2511.09827v2",
      "project": "https://miraymen.github.io/aha"
    },
    "bibtex": ""
  },
  {
    "title": "Lumos3D: A Single-Forward Framework for Low-Light 3D Scene Restoration",
    "authors": [
      "Hanzhou Liu",
      "Peng Jiang",
      "Jia Huang",
      "Mi Lu"
    ],
    "abstract": "Restoring 3D scenes captured under low-light con- ditions remains a fundamental yet challenging problem. Most existing approaches depend on precomputed camera poses and scene-specific optimization, which greatly restricts their scala- bility to dynamic real-world environments. To overcome these limitations, we introduce Lumos3D, a generalizable pose-free framework for 3D low-light scene restoration. Trained once on a single dataset, Lumos3D performs inference in a purely feed- forward manner, directly restoring illumination and structure from unposed, low-light multi-view images without any per- scene training or optimization. Built upon a geometry-grounded backbone, Lumos3D reconstructs a normal-light 3D Gaussian representation that restores illumination while faithfully pre- serving structural details. During training, a cross-illumination distillation scheme is employed, where the teacher network is distilled on normal-light ground truth to transfer accurate geometric information, such as depth, to the student model. A dedicated Lumos loss is further introduced to promote photomet- ric consistency within the reconstructed 3D space. Experiments on real-world datasets demonstrate that Lumos3D achieves high- fidelity low-light 3D scene restoration with accurate geometry and strong generalization to unseen cases. Furthermore, the framework naturally extends to handle over-exposure correction, highlighting its versatility for diverse lighting restoration tasks.",
    "arxiv_url": "https://arxiv.org/abs/2511.09818v1",
    "pdf_url": "https://arxiv.org/pdf/2511.09818v1",
    "published_date": "2025-11-12",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "dynamic",
      "geometry",
      "ar",
      "3d gaussian",
      "lighting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.09818v1",
      "pdf": "https://arxiv.org/pdf/2511.09818v1"
    },
    "bibtex": ""
  },
  {
    "title": "A Shared-Autonomy Construction Robotic System for Overhead Works",
    "authors": [
      "David Minkwan Kim",
      "K. M. Brian Lee",
      "Yong Hyeok Seo",
      "Nikola Raicevic",
      "Runfa Blark Li",
      "Kehan Long",
      "Chan Seon Yoon",
      "Dong Min Kang",
      "Byeong Jo Lim",
      "Young Pyoung Kim",
      "Nikolay Atanasov",
      "Truong Nguyen",
      "Se Woong Jun",
      "Young Wook Kim"
    ],
    "abstract": "We present the ongoing development of a robotic system for overhead work such as ceiling drilling. The hardware platform comprises a mobile base with a two-stage lift, on which a bimanual torso is mounted with a custom-designed drilling end effector and RGB-D cameras. To support teleoperation in dynamic environments with limited visibility, we use Gaussian splatting for online 3D reconstruction and introduce motion parameters to model moving objects. For safe operation around dynamic obstacles, we developed a neural configuration-space barrier approach for planning and control. Initial feasibility studies demonstrate the capability of the hardware in drilling, bolting, and anchoring, and the software in safe teleoperation in a dynamic environment.",
    "arxiv_url": "https://arxiv.org/abs/2511.09695v1",
    "pdf_url": "https://arxiv.org/pdf/2511.09695v1",
    "published_date": "2025-11-12",
    "categories": [
      "cs.RO",
      "eess.SY"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "dynamic",
      "3d reconstruction",
      "gaussian splatting",
      "ar",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.09695v1",
      "pdf": "https://arxiv.org/pdf/2511.09695v1"
    },
    "bibtex": ""
  },
  {
    "title": "OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS",
    "authors": [
      "Haiyi Li",
      "Qi Chen",
      "Denis Kalkofen",
      "Hsiang-Ting Chen"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have achieved state-of-the-art results for novel view synthesis. However, efficiently capturing high-fidelity reconstructions of specific objects within complex scenes remains a significant challenge. A key limitation of existing active reconstruction methods is their reliance on scene-level uncertainty metrics, which are often biased by irrelevant background clutter and lead to inefficient view selection for object-centric tasks. We present OUGS, a novel framework that addresses this challenge with a more principled, physically-grounded uncertainty formulation for 3DGS. Our core innovation is to derive uncertainty directly from the explicit physical parameters of the 3D Gaussian primitives (e.g., position, scale, rotation). By propagating the covariance of these parameters through the rendering Jacobian, we establish a highly interpretable uncertainty model. This foundation allows us to then seamlessly integrate semantic segmentation masks to produce a targeted, object-aware uncertainty score that effectively disentangles the object from its environment. This allows for a more effective active view selection strategy that prioritizes views critical to improving object fidelity. Experimental evaluations on public datasets demonstrate that our approach significantly improves the efficiency of the 3DGS reconstruction process and achieves higher quality for targeted objects compared to existing state-of-the-art methods, while also serving as a robust uncertainty estimator for the global scene.",
    "arxiv_url": "https://arxiv.org/abs/2511.09397v2",
    "pdf_url": "https://arxiv.org/pdf/2511.09397v2",
    "published_date": "2025-11-12",
    "categories": [
      "cs.CV",
      "cs.CG",
      "cs.GR",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "segmentation",
      "ar",
      "semantic",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.09397v2",
      "pdf": "https://arxiv.org/pdf/2511.09397v2"
    },
    "bibtex": ""
  },
  {
    "title": "SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering",
    "authors": [
      "Laura Bragagnolo",
      "Leonardo Barcellona",
      "Stefano Ghidoni"
    ],
    "abstract": "Accurate 3D human pose estimation is fundamental for applications such as augmented reality and human-robot interaction. State-of-the-art multi-view methods learn to fuse predictions across views by training on large annotated datasets, leading to poor generalization when the test scenario differs. To overcome these limitations, we propose SkelSplat, a novel framework for multi-view 3D human pose estimation based on differentiable Gaussian rendering. Human pose is modeled as a skeleton of 3D Gaussians, one per joint, optimized via differentiable rendering to enable seamless fusion of arbitrary camera views without 3D ground-truth supervision. Since Gaussian Splatting was originally designed for dense scene reconstruction, we propose a novel one-hot encoding scheme that enables independent optimization of human joints. SkelSplat outperforms approaches that do not rely on 3D ground truth in Human3.6M and CMU, while reducing the cross-dataset error up to 47.8% compared to learning-based methods. Experiments on Human3.6M-Occ and Occlusion-Person demonstrate robustness to occlusions, without scenario-specific fine-tuning. Our project page is available here: https://skelsplat.github.io.",
    "arxiv_url": "https://arxiv.org/abs/2511.08294v2",
    "pdf_url": "https://arxiv.org/pdf/2511.08294v2",
    "published_date": "2025-11-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "human",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.08294v2",
      "pdf": "https://arxiv.org/pdf/2511.08294v2",
      "project": "https://skelsplat.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "Perceptual Quality Assessment of 3D Gaussian Splatting: A Subjective Dataset and Prediction Metric",
    "authors": [
      "Zhaolin Wan",
      "Yining Diao",
      "Jingqi Xu",
      "Hao Wang",
      "Zhiyang Li",
      "Xiaopeng Fan",
      "Wangmeng Zuo",
      "Debin Zhao"
    ],
    "abstract": "With the rapid advancement of 3D visualization, 3D Gaussian Splatting (3DGS) has emerged as a leading technique for real-time, high-fidelity rendering. While prior research has emphasized algorithmic performance and visual fidelity, the perceptual quality of 3DGS-rendered content, especially under varying reconstruction conditions, remains largely underexplored. In practice, factors such as viewpoint sparsity, limited training iterations, point downsampling, noise, and color distortions can significantly degrade visual quality, yet their perceptual impact has not been systematically studied. To bridge this gap, we present 3DGS-QA, the first subjective quality assessment dataset for 3DGS. It comprises 225 degraded reconstructions across 15 object types, enabling a controlled investigation of common distortion factors. Based on this dataset, we introduce a no-reference quality prediction model that directly operates on native 3D Gaussian primitives, without requiring rendered images or ground-truth references. Our model extracts spatial and photometric cues from the Gaussian representation to estimate perceived quality in a structure-aware manner. We further benchmark existing quality assessment methods, spanning both traditional and learning-based approaches. Experimental results show that our method consistently achieves superior performance, highlighting its robustness and effectiveness for 3DGS content evaluation. The dataset and code are made publicly available at https://github.com/diaoyn/3DGSQA to facilitate future research in 3DGS quality assessment.",
    "arxiv_url": "https://arxiv.org/abs/2511.08032v1",
    "pdf_url": "https://arxiv.org/pdf/2511.08032v1",
    "published_date": "2025-11-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/diaoyn/3DGSQA",
    "keywords": [
      "high-fidelity",
      "ar",
      "3d gaussian",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.08032v1",
      "pdf": "https://arxiv.org/pdf/2511.08032v1",
      "github": "https://github.com/diaoyn/3DGSQA"
    },
    "bibtex": ""
  },
  {
    "title": "Is It Truly Necessary to Process and Fit Minutes-Long Reference Videos for Personalized Talking Face Generation?",
    "authors": [
      "Rui-Qing Sun",
      "Ang Li",
      "Zhijing Wu",
      "Tian Lan",
      "Qianyu Lu",
      "Xingshan Yao",
      "Chen Xu",
      "Xian-Ling Mao"
    ],
    "abstract": "Talking Face Generation (TFG) aims to produce realistic and dynamic talking portraits, with broad applications in fields such as digital education, film and television production, e-commerce live streaming, and other related areas. Currently, TFG methods based on Neural Radiated Field (NeRF) or 3D Gaussian sputtering (3DGS) are received widespread attention. They learn and store personalized features from reference videos of each target individual to generate realistic speaking videos. To ensure models can capture sufficient 3D information and successfully learns the lip-audio mapping, previous studies usually require meticulous processing and fitting several minutes of reference video, which always takes hours. The computational burden of processing and fitting long reference videos severely limits the practical application value of these methods.However, is it really necessary to fit such minutes of reference video? Our exploratory case studies show that using some informative reference video segments of just a few seconds can achieve performance comparable to or even better than the full reference video. This indicates that video informative quality is much more important than its length. Inspired by this observation, we propose the ISExplore (short for Informative Segment Explore), a simple-yet-effective segment selection strategy that automatically identifies the informative 5-second reference video segment based on three key data quality dimensions: audio feature diversity, lip movement amplitude, and number of camera views. Extensive experiments demonstrate that our approach increases data processing and training speed by more than 5x for NeRF and 3DGS methods, while maintaining high-fidelity output. Project resources are available at xx.",
    "arxiv_url": "https://arxiv.org/abs/2511.07940v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07940v1",
    "published_date": "2025-11-11",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "high-fidelity",
      "face",
      "ar",
      "nerf",
      "3d gaussian",
      "mapping"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.07940v1",
      "pdf": "https://arxiv.org/pdf/2511.07940v1"
    },
    "bibtex": ""
  },
  {
    "title": "UltraGS: Real-Time Physically-Decoupled Gaussian Splatting for Ultrasound Novel View Synthesis",
    "authors": [
      "Yuezhe Yang",
      "Qingqing Ruan",
      "Wenjie Cai",
      "Yudang Dong",
      "Dexin Yang",
      "Xingbo Dong",
      "Zhe Jin",
      "Yong Dai"
    ],
    "abstract": "Ultrasound imaging is a cornerstone of non-invasive clinical diagnostics, yet its limited field of view poses challenges for novel view synthesis. We present UltraGS, a real-time framework that adapts Gaussian Splatting to sensorless ultrasound imaging by integrating explicit radiance fields with lightweight, physics-inspired acoustic modeling. UltraGS employs depth-aware Gaussian primitives with learnable fields of view to improve geometric consistency under unconstrained probe motion, and introduces PD Rendering, a differentiable acoustic operator that combines low-order spherical harmonics with first-order wave effects for efficient intensity synthesis. We further present a clinical ultrasound dataset acquired under real-world scanning protocols. Extensive evaluations across three datasets demonstrate that UltraGS establishes a new performance-efficiency frontier, achieving state-of-the-art results in PSNR (up to 29.55) and SSIM (up to 0.89) while achieving real-time synthesis at 64.69 fps on a single GPU. The code and dataset are open-sourced at: https://github.com/Bean-Young/UltraGS.",
    "arxiv_url": "https://arxiv.org/abs/2511.07743v2",
    "pdf_url": "https://arxiv.org/pdf/2511.07743v2",
    "published_date": "2025-11-11",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "https://github.com/Bean-Young/UltraGS",
    "keywords": [
      "lightweight",
      "gaussian splatting",
      "ar",
      "efficient",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.07743v2",
      "pdf": "https://arxiv.org/pdf/2511.07743v2",
      "github": "https://github.com/Bean-Young/UltraGS"
    },
    "bibtex": ""
  },
  {
    "title": "DIMO: Diverse 3D Motion Generation for Arbitrary Objects",
    "authors": [
      "Linzhan Mou",
      "Jiahui Lei",
      "Chen Wang",
      "Lingjie Liu",
      "Kostas Daniilidis"
    ],
    "abstract": "We present DIMO, a generative approach capable of generating diverse 3D motions for arbitrary objects from a single image. The core idea of our work is to leverage the rich priors in well-trained video models to extract the common motion patterns and then embed them into a shared low-dimensional latent space. Specifically, we first generate multiple videos of the same object with diverse motions. We then embed each motion into a latent vector and train a shared motion decoder to learn the distribution of motions represented by a structured and compact motion representation, i.e., neural key point trajectories. The canonical 3D Gaussians are then driven by these key points and fused to model the geometry and appearance. During inference time with learned latent space, we can instantly sample diverse 3D motions in a single-forward pass and support several interesting applications including 3D motion interpolation and language-guided motion generation. Our project page is available at https://linzhanm.github.io/dimo.",
    "arxiv_url": "https://arxiv.org/abs/2511.07409v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07409v1",
    "published_date": "2025-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "geometry",
      "ar",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.07409v1",
      "pdf": "https://arxiv.org/pdf/2511.07409v1",
      "project": "https://linzhanm.github.io/dimo"
    },
    "bibtex": ""
  },
  {
    "title": "YoNoSplat: You Only Need One Model for Feedforward 3D Gaussian Splatting",
    "authors": [
      "Botao Ye",
      "Boqi Chen",
      "Haofei Xu",
      "Daniel Barath",
      "Marc Pollefeys"
    ],
    "abstract": "Fast and flexible 3D scene reconstruction from unstructured image collections remains a significant challenge. We present YoNoSplat, a feedforward model that reconstructs high-quality 3D Gaussian Splatting representations from an arbitrary number of images. Our model is highly versatile, operating effectively with both posed and unposed, calibrated and uncalibrated inputs. YoNoSplat predicts local Gaussians and camera poses for each view, which are aggregated into a global representation using either predicted or provided poses. To overcome the inherent difficulty of jointly learning 3D Gaussians and camera parameters, we introduce a novel mixing training strategy. This approach mitigates the entanglement between the two tasks by initially using ground-truth poses to aggregate local Gaussians and gradually transitioning to a mix of predicted and ground-truth poses, which prevents both training instability and exposure bias. We further resolve the scale ambiguity problem by a novel pairwise camera-distance normalization scheme and by embedding camera intrinsics into the network. Moreover, YoNoSplat also predicts intrinsic parameters, making it feasible for uncalibrated inputs. YoNoSplat demonstrates exceptional efficiency, reconstructing a scene from 100 views (at 280x518 resolution) in just 2.69 seconds on an NVIDIA GH200 GPU. It achieves state-of-the-art performance on standard benchmarks in both pose-free and pose-dependent settings. Our project page is at https://botaoye.github.io/yonosplat/.",
    "arxiv_url": "https://arxiv.org/abs/2511.07321v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07321v1",
    "published_date": "2025-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.07321v1",
      "pdf": "https://arxiv.org/pdf/2511.07321v1",
      "project": "https://botaoye.github.io/yonosplat"
    },
    "bibtex": ""
  },
  {
    "title": "4DSTR: Advancing Generative 4D Gaussians with Spatial-Temporal Rectification for High-Quality and Consistent 4D Generation",
    "authors": [
      "Mengmeng Liu",
      "Jiuming Liu",
      "Yunpeng Zhang",
      "Jiangtao Li",
      "Michael Ying Yang",
      "Francesco Nex",
      "Hao Cheng"
    ],
    "abstract": "Remarkable advances in recent 2D image and 3D shape generation have induced a significant focus on dynamic 4D content generation. However, previous 4D generation methods commonly struggle to maintain spatial-temporal consistency and adapt poorly to rapid temporal variations, due to the lack of effective spatial-temporal modeling. To address these problems, we propose a novel 4D generation network called 4DSTR, which modulates generative 4D Gaussian Splatting with spatial-temporal rectification. Specifically, temporal correlation across generated 4D sequences is designed to rectify deformable scales and rotations and guarantee temporal consistency. Furthermore, an adaptive spatial densification and pruning strategy is proposed to address significant temporal variations by dynamically adding or deleting Gaussian points with the awareness of their pre-frame movements. Extensive experiments demonstrate that our 4DSTR achieves state-of-the-art performance in video-to-4D generation, excelling in reconstruction quality, spatial-temporal consistency, and adaptation to rapid temporal movements.",
    "arxiv_url": "https://arxiv.org/abs/2511.07241v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07241v1",
    "published_date": "2025-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.07241v1",
      "pdf": "https://arxiv.org/pdf/2511.07241v1"
    },
    "bibtex": ""
  },
  {
    "title": "Sparse4DGS: 4D Gaussian Splatting for Sparse-Frame Dynamic Scene Reconstruction",
    "authors": [
      "Changyue Shi",
      "Chuxiao Yang",
      "Xinyuan Hu",
      "Minghao Chen",
      "Wenwen Pan",
      "Yan Yang",
      "Jiajun Ding",
      "Zhou Yu",
      "Jun Yu"
    ],
    "abstract": "Dynamic Gaussian Splatting approaches have achieved remarkable performance for 4D scene reconstruction. However, these approaches rely on dense-frame video sequences for photorealistic reconstruction. In real-world scenarios, due to equipment constraints, sometimes only sparse frames are accessible. In this paper, we propose Sparse4DGS, the first method for sparse-frame dynamic scene reconstruction. We observe that dynamic reconstruction methods fail in both canonical and deformed spaces under sparse-frame settings, especially in areas with high texture richness. Sparse4DGS tackles this challenge by focusing on texture-rich areas. For the deformation network, we propose Texture-Aware Deformation Regularization, which introduces a texture-based depth alignment loss to regulate Gaussian deformation. For the canonical Gaussian field, we introduce Texture-Aware Canonical Optimization, which incorporates texture-based noise into the gradient descent process of canonical Gaussians. Extensive experiments show that when taking sparse frames as inputs, our method outperforms existing dynamic or few-shot techniques on NeRF-Synthetic, HyperNeRF, NeRF-DS, and our iPhone-4D datasets.",
    "arxiv_url": "https://arxiv.org/abs/2511.07122v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07122v1",
    "published_date": "2025-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "few-shot",
      "deformation",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.07122v1",
      "pdf": "https://arxiv.org/pdf/2511.07122v1"
    },
    "bibtex": ""
  },
  {
    "title": "GFix: Perceptually Enhanced Gaussian Splatting Video Compression",
    "authors": [
      "Siyue Teng",
      "Ge Gao",
      "Duolikun Danier",
      "Yuxuan Jiang",
      "Fan Zhang",
      "Thomas Davis",
      "Zoe Liu",
      "David Bull"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) enhances 3D scene reconstruction through explicit representation and fast rendering, demonstrating potential benefits for various low-level vision tasks, including video compression. However, existing 3DGS-based video codecs generally exhibit more noticeable visual artifacts and relatively low compression ratios. In this paper, we specifically target the perceptual enhancement of 3DGS-based video compression, based on the assumption that artifacts from 3DGS rendering and quantization resemble noisy latents sampled during diffusion training. Building on this premise, we propose a content-adaptive framework, GFix, comprising a streamlined, single-step diffusion model that serves as an off-the-shelf neural enhancer. Moreover, to increase compression efficiency, We propose a modulated LoRA scheme that freezes the low-rank decompositions and modulates the intermediate hidden states, thereby achieving efficient adaptation of the diffusion backbone with highly compressible updates. Experimental results show that GFix delivers strong perceptual quality enhancement, outperforming GSVC with up to 72.1% BD-rate savings in LPIPS and 21.4% in FID.",
    "arxiv_url": "https://arxiv.org/abs/2511.06953v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06953v1",
    "published_date": "2025-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compression",
      "ar",
      "quality enhancement",
      "3d gaussian",
      "efficient",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.06953v1",
      "pdf": "https://arxiv.org/pdf/2511.06953v1"
    },
    "bibtex": ""
  },
  {
    "title": "MUGSQA: Novel Multi-Uncertainty-Based Gaussian Splatting Quality Assessment Method, Dataset, and Benchmarks",
    "authors": [
      "Tianang Chen",
      "Jian Jin",
      "Shilv Cai",
      "Zhuangzi Li",
      "Weisi Lin"
    ],
    "abstract": "Gaussian Splatting (GS) has recently emerged as a promising technique for 3D object reconstruction, delivering high-quality rendering results with significantly improved reconstruction speed. As variants continue to appear, assessing the perceptual quality of 3D objects reconstructed with different GS-based methods remains an open challenge. To address this issue, we first propose a unified multi-distance subjective quality assessment method that closely mimics human viewing behavior for objects reconstructed with GS-based methods in actual applications, thereby better collecting perceptual experiences. Based on it, we also construct a novel GS quality assessment dataset named MUGSQA, which is constructed considering multiple uncertainties of the input data. These uncertainties include the quantity and resolution of input views, the view distance, and the accuracy of the initial point cloud. Moreover, we construct two benchmarks: one to evaluate the robustness of various GS-based reconstruction methods under multiple uncertainties, and the other to evaluate the performance of existing quality assessment metrics. Our dataset and benchmark code will be released soon.",
    "arxiv_url": "https://arxiv.org/abs/2511.06830v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06830v1",
    "published_date": "2025-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "human",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.06830v1",
      "pdf": "https://arxiv.org/pdf/2511.06830v1"
    },
    "bibtex": ""
  },
  {
    "title": "ConeGS: Error-Guided Densification Using Pixel Cones for Improved Reconstruction With Fewer Primitives",
    "authors": [
      "Bartłomiej Baranowski",
      "Stefano Esposito",
      "Patricia Gschoßmann",
      "Anpei Chen",
      "Andreas Geiger"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) achieves state-of-the-art image quality and real-time performance in novel view synthesis but often suffers from a suboptimal spatial distribution of primitives. This issue stems from cloning-based densification, which propagates Gaussians along existing geometry, limiting exploration and requiring many primitives to adequately cover the scene. We present ConeGS, an image-space-informed densification framework that is independent of existing scene geometry state. ConeGS first creates a fast Instant Neural Graphics Primitives (iNGP) reconstruction as a geometric proxy to estimate per-pixel depth. During the subsequent 3DGS optimization, it identifies high-error pixels and inserts new Gaussians along the corresponding viewing cones at the predicted depth values, initializing their size according to the cone diameter. A pre-activation opacity penalty rapidly removes redundant Gaussians, while a primitive budgeting strategy controls the total number of primitives, either by a fixed budget or by adapting to scene complexity, ensuring high reconstruction quality. Experiments show that ConeGS consistently enhances reconstruction quality and rendering performance across Gaussian budgets, with especially strong gains under tight primitive constraints where efficient placement is crucial.",
    "arxiv_url": "https://arxiv.org/abs/2511.06810v2",
    "pdf_url": "https://arxiv.org/pdf/2511.06810v2",
    "published_date": "2025-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "geometry",
      "ar",
      "3d gaussian",
      "efficient",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.06810v2",
      "pdf": "https://arxiv.org/pdf/2511.06810v2"
    },
    "bibtex": ""
  },
  {
    "title": "Robust and High-Fidelity 3D Gaussian Splatting: Fusing Pose Priors and Geometry Constraints for Texture-Deficient Outdoor Scenes",
    "authors": [
      "Meijun Guo",
      "Yongliang Shi",
      "Caiyun Liu",
      "Yixiao Feng",
      "Ming Ma",
      "Tinghai Yan",
      "Weining Lu",
      "Bin Liang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a key rendering pipeline for digital asset creation due to its balance between efficiency and visual quality. To address the issues of unstable pose estimation and scene representation distortion caused by geometric texture inconsistency in large outdoor scenes with weak or repetitive textures, we approach the problem from two aspects: pose estimation and scene representation. For pose estimation, we leverage LiDAR-IMU Odometry to provide prior poses for cameras in large-scale environments. These prior pose constraints are incorporated into COLMAP's triangulation process, with pose optimization performed via bundle adjustment. Ensuring consistency between pixel data association and prior poses helps maintain both robustness and accuracy. For scene representation, we introduce normal vector constraints and effective rank regularization to enforce consistency in the direction and shape of Gaussian primitives. These constraints are jointly optimized with the existing photometric loss to enhance the map quality. We evaluate our approach using both public and self-collected datasets. In terms of pose optimization, our method requires only one-third of the time while maintaining accuracy and robustness across both datasets. In terms of scene representation, the results show that our method significantly outperforms conventional 3DGS pipelines. Notably, on self-collected datasets characterized by weak or repetitive textures, our approach demonstrates enhanced visualization capabilities and achieves superior overall performance. Codes and data will be publicly available at https://github.com/justinyeah/normal_shape.git.",
    "arxiv_url": "https://arxiv.org/abs/2511.06765v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06765v1",
    "published_date": "2025-11-10",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "https://github.com/justinyeah/normal_shape.git",
    "keywords": [
      "outdoor",
      "high-fidelity",
      "geometry",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.06765v1",
      "pdf": "https://arxiv.org/pdf/2511.06765v1",
      "github": "https://github.com/justinyeah/normal_shape.git"
    },
    "bibtex": ""
  },
  {
    "title": "Rethinking Rainy 3D Scene Reconstruction via Perspective Transforming and Brightness Tuning",
    "authors": [
      "Qianfeng Yang",
      "Xiang Chen",
      "Pengpeng Li",
      "Qiyuan Guan",
      "Guiyue Jin",
      "Jiyu Jin"
    ],
    "abstract": "Rain degrades the visual quality of multi-view images, which are essential for 3D scene reconstruction, resulting in inaccurate and incomplete reconstruction results. Existing datasets often overlook two critical characteristics of real rainy 3D scenes: the viewpoint-dependent variation in the appearance of rain streaks caused by their projection onto 2D images, and the reduction in ambient brightness resulting from cloud coverage during rainfall. To improve data realism, we construct a new dataset named OmniRain3D that incorporates perspective heterogeneity and brightness dynamicity, enabling more faithful simulation of rain degradation in 3D scenes. Based on this dataset, we propose an end-to-end reconstruction framework named REVR-GSNet (Rain Elimination and Visibility Recovery for 3D Gaussian Splatting). Specifically, REVR-GSNet integrates recursive brightness enhancement, Gaussian primitive optimization, and GS-guided rain elimination into a unified architecture through joint alternating optimization, achieving high-fidelity reconstruction of clean 3D scenes from rain-degraded inputs. Extensive experiments show the effectiveness of our dataset and method. Our dataset and method provide a foundation for future research on multi-view image deraining and rainy 3D scene reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2511.06734v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06734v1",
    "published_date": "2025-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "vr",
      "high-fidelity",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.06734v1",
      "pdf": "https://arxiv.org/pdf/2511.06734v1"
    },
    "bibtex": ""
  },
  {
    "title": "DIAL-GS: Dynamic Instance Aware Reconstruction for Label-free Street Scenes with 4D Gaussian Splatting",
    "authors": [
      "Chenpeng Su",
      "Wenhua Wu",
      "Chensheng Peng",
      "Tianchen Deng",
      "Zhe Liu",
      "Hesheng Wang"
    ],
    "abstract": "Urban scene reconstruction is critical for autonomous driving, enabling structured 3D representations for data synthesis and closed-loop testing. Supervised approaches rely on costly human annotations and lack scalability, while current self-supervised methods often confuse static and dynamic elements and fail to distinguish individual dynamic objects, limiting fine-grained editing. We propose DIAL-GS, a novel dynamic instance-aware reconstruction method for label-free street scenes with 4D Gaussian Splatting. We first accurately identify dynamic instances by exploiting appearance-position inconsistency between warped rendering and actual observation. Guided by instance-level dynamic perception, we employ instance-aware 4D Gaussians as the unified volumetric representation, realizing dynamic-adaptive and instance-aware reconstruction. Furthermore, we introduce a reciprocal mechanism through which identity and dynamics reinforce each other, enhancing both integrity and consistency. Experiments on urban driving scenarios show that DIAL-GS surpasses existing self-supervised baselines in reconstruction quality and instance-level editing, offering a concise yet powerful solution for urban scene modeling.",
    "arxiv_url": "https://arxiv.org/abs/2511.06632v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06632v1",
    "published_date": "2025-11-10",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "urban scene",
      "dynamic",
      "autonomous driving",
      "ar",
      "human",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.06632v1",
      "pdf": "https://arxiv.org/pdf/2511.06632v1"
    },
    "bibtex": ""
  },
  {
    "title": "Inpaint360GS: Efficient Object-Aware 3D Inpainting via Gaussian Splatting for 360° Scenes",
    "authors": [
      "Shaoxiang Wang",
      "Shihong Zhang",
      "Christen Millerdurai",
      "Rüdiger Westermann",
      "Didier Stricker",
      "Alain Pagani"
    ],
    "abstract": "Despite recent advances in single-object front-facing inpainting using NeRF and 3D Gaussian Splatting (3DGS), inpainting in complex 360° scenes remains largely underexplored. This is primarily due to three key challenges: (i) identifying target objects in the 3D field of 360° environments, (ii) dealing with severe occlusions in multi-object scenes, which makes it hard to define regions to inpaint, and (iii) maintaining consistent and high-quality appearance across views effectively. To tackle these challenges, we propose Inpaint360GS, a flexible 360° editing framework based on 3DGS that supports multi-object removal and high-fidelity inpainting in 3D space. By distilling 2D segmentation into 3D and leveraging virtual camera views for contextual guidance, our method enables accurate object-level editing and consistent scene completion. We further introduce a new dataset tailored for 360° inpainting, addressing the lack of ground truth object-free scenes. Experiments demonstrate that Inpaint360GS outperforms existing baselines and achieves state-of-the-art performance. Project page: https://dfki-av.github.io/inpaint360gs/",
    "arxiv_url": "https://arxiv.org/abs/2511.06457v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06457v1",
    "published_date": "2025-11-09",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "segmentation",
      "ar",
      "nerf",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.06457v1",
      "pdf": "https://arxiv.org/pdf/2511.06457v1",
      "project": "https://dfki-av.github.io/inpaint360gs"
    },
    "bibtex": ""
  },
  {
    "title": "Physics-Informed Deformable Gaussian Splatting: Towards Unified Constitutive Laws for Time-Evolving Material Field",
    "authors": [
      "Haoqin Hong",
      "Ding Fan",
      "Fubin Dou",
      "Zhi-Li Zhou",
      "Haoran Sun",
      "Congcong Zhu",
      "Jingrun Chen"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS), an explicit scene representation technique, has shown significant promise for dynamic novel-view synthesis from monocular video input. However, purely data-driven 3DGS often struggles to capture the diverse physics-driven motion patterns in dynamic scenes. To fill this gap, we propose Physics-Informed Deformable Gaussian Splatting (PIDG), which treats each Gaussian particle as a Lagrangian material point with time-varying constitutive parameters and is supervised by 2D optical flow via motion projection. Specifically, we adopt static-dynamic decoupled 4D decomposed hash encoding to reconstruct geometry and motion efficiently. Subsequently, we impose the Cauchy momentum residual as a physics constraint, enabling independent prediction of each particle's velocity and constitutive stress via a time-evolving material field. Finally, we further supervise data fitting by matching Lagrangian particle flow to camera-compensated optical flow, which accelerates convergence and improves generalization. Experiments on a custom physics-driven dataset as well as on standard synthetic and real-world datasets demonstrate significant gains in physical consistency and monocular dynamic reconstruction quality.",
    "arxiv_url": "https://arxiv.org/abs/2511.06299v3",
    "pdf_url": "https://arxiv.org/pdf/2511.06299v3",
    "published_date": "2025-11-09",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "motion",
      "geometry",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.06299v3",
      "pdf": "https://arxiv.org/pdf/2511.06299v3"
    },
    "bibtex": ""
  },
  {
    "title": "StreamSTGS: Streaming Spatial and Temporal Gaussian Grids for Real-Time Free-Viewpoint Video",
    "authors": [
      "Zhihui Ke",
      "Yuyang Liu",
      "Xiaobo Zhou",
      "Tie Qiu"
    ],
    "abstract": "Streaming free-viewpoint video~(FVV) in real-time still faces significant challenges, particularly in training, rendering, and transmission efficiency. Harnessing superior performance of 3D Gaussian Splatting~(3DGS), recent 3DGS-based FVV methods have achieved notable breakthroughs in both training and rendering. However, the storage requirements of these methods can reach up to $10$MB per frame, making stream FVV in real-time impossible. To address this problem, we propose a novel FVV representation, dubbed StreamSTGS, designed for real-time streaming. StreamSTGS represents a dynamic scene using canonical 3D Gaussians, temporal features, and a deformation field. For high compression efficiency, we encode canonical Gaussian attributes as 2D images and temporal features as a video. This design not only enables real-time streaming, but also inherently supports adaptive bitrate control based on network condition without any extra training. Moreover, we propose a sliding window scheme to aggregate adjacent temporal features to learn local motions, and then introduce a transformer-guided auxiliary training module to learn global motions. On diverse FVV benchmarks, StreamSTGS demonstrates competitive performance on all metrics compared to state-of-the-art methods. Notably, StreamSTGS increases the PSNR by an average of $1$dB while reducing the average frame size to just $170$KB. The code is publicly available on https://github.com/kkkzh/StreamSTGS.",
    "arxiv_url": "https://arxiv.org/abs/2511.06046v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06046v1",
    "published_date": "2025-11-08",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/kkkzh/StreamSTGS",
    "keywords": [
      "dynamic",
      "face",
      "deformation",
      "compression",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.06046v1",
      "pdf": "https://arxiv.org/pdf/2511.06046v1",
      "github": "https://github.com/kkkzh/StreamSTGS"
    },
    "bibtex": ""
  },
  {
    "title": "4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos",
    "authors": [
      "Mengqi Guo",
      "Bo Xu",
      "Yanyan Li",
      "Gim Hee Lee"
    ],
    "abstract": "Novel view synthesis from monocular videos of dynamic scenes with unknown camera poses remains a fundamental challenge in computer vision and graphics. While recent advances in 3D representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static scenes, they struggle with dynamic content and typically rely on pre-computed camera poses. We present 4D3R, a pose-free dynamic neural rendering framework that decouples static and dynamic components through a two-stage approach. Our method first leverages 3D foundational models for initial pose and geometry estimation, followed by motion-aware refinement. 4D3R introduces two key technical innovations: (1) a motion-aware bundle adjustment (MA-BA) module that combines transformer-based learned priors with SAM2 for robust dynamic object segmentation, enabling more accurate camera pose refinement; and (2) an efficient Motion-Aware Gaussian Splatting (MA-GS) representation that uses control points with a deformation field MLP and linear blend skinning to model dynamic motion, significantly reducing computational cost while maintaining high-quality reconstruction. Extensive experiments on real-world dynamic datasets demonstrate that our approach achieves up to 1.8dB PSNR improvement over state-of-the-art methods, particularly in challenging scenarios with large dynamic objects, while reducing computational requirements by 5x compared to previous dynamic scene representations.",
    "arxiv_url": "https://arxiv.org/abs/2511.05229v1",
    "pdf_url": "https://arxiv.org/pdf/2511.05229v1",
    "published_date": "2025-11-07",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "deformation",
      "motion",
      "geometry",
      "neural rendering",
      "segmentation",
      "ar",
      "nerf",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.05229v1",
      "pdf": "https://arxiv.org/pdf/2511.05229v1"
    },
    "bibtex": ""
  },
  {
    "title": "Splatography: Sparse multi-view dynamic Gaussian Splatting for filmmaking challenges",
    "authors": [
      "Adrian Azzarelli",
      "Nantheera Anantrasirichai",
      "David R Bull"
    ],
    "abstract": "Deformable Gaussian Splatting (GS) accomplishes photorealistic dynamic 3-D reconstruction from dense multi-view video (MVV) by learning to deform a canonical GS representation. However, in filmmaking, tight budgets can result in sparse camera configurations, which limits state-of-the-art (SotA) methods when capturing complex dynamic features. To address this issue, we introduce an approach that splits the canonical Gaussians and deformation field into foreground and background components using a sparse set of masks for frames at t=0. Each representation is separately trained on different loss functions during canonical pre-training. Then, during dynamic training, different parameters are modeled for each deformation field following common filmmaking practices. The foreground stage contains diverse dynamic features so changes in color, position and rotation are learned. While, the background containing film-crew and equipment, is typically dimmer and less dynamic so only changes in point position are learned. Experiments on 3-D and 2.5-D entertainment datasets show that our method produces SotA qualitative and quantitative results; up to 3 PSNR higher with half the model size on 3-D scenes. Unlike the SotA and without the need for dense mask supervision, our method also produces segmented dynamic reconstructions including transparent and dynamic textures. Code and video comparisons are available online: https://interims-git.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2511.05152v1",
    "pdf_url": "https://arxiv.org/pdf/2511.05152v1",
    "published_date": "2025-11-07",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "deformation",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.05152v1",
      "pdf": "https://arxiv.org/pdf/2511.05152v1",
      "project": "https://interims-git.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "Efficient representation of 3D spatial data for defense-related applications",
    "authors": [
      "Benjamin Kahl",
      "Marcus Hebel",
      "Michael Arens"
    ],
    "abstract": "Geospatial sensor data is essential for modern defense and security, offering indispensable 3D information for situational awareness. This data, gathered from sources like lidar sensors and optical cameras, allows for the creation of detailed models of operational environments. In this paper, we provide a comparative analysis of traditional representation methods, such as point clouds, voxel grids, and triangle meshes, alongside modern neural and implicit techniques like Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS). Our evaluation reveals a fundamental trade-off: traditional models offer robust geometric accuracy ideal for functional tasks like line-of-sight analysis and physics simulations, while modern methods excel at producing high-fidelity, photorealistic visuals but often lack geometric reliability. Based on these findings, we conclude that a hybrid approach is the most promising path forward. We propose a system architecture that combines a traditional mesh scaffold for geometric integrity with a neural representation like 3DGS for visual detail, managed within a hierarchical scene structure to ensure scalability and performance.",
    "arxiv_url": "https://arxiv.org/abs/2511.05109v1",
    "pdf_url": "https://arxiv.org/pdf/2511.05109v1",
    "published_date": "2025-11-07",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "nerf",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.05109v1",
      "pdf": "https://arxiv.org/pdf/2511.05109v1"
    },
    "bibtex": ""
  },
  {
    "title": "CLM: Removing the GPU Memory Barrier for 3D Gaussian Splatting",
    "authors": [
      "Hexu Zhao",
      "Xiwen Min",
      "Xiaoteng Liu",
      "Moonjun Gong",
      "Yiming Li",
      "Ang Li",
      "Saining Xie",
      "Jinyang Li",
      "Aurojit Panda"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is an increasingly popular novel view synthesis approach due to its fast rendering time, and high-quality output. However, scaling 3DGS to large (or intricate) scenes is challenging due to its large memory requirement, which exceed most GPU's memory capacity. In this paper, we describe CLM, a system that allows 3DGS to render large scenes using a single consumer-grade GPU, e.g., RTX4090. It does so by offloading Gaussians to CPU memory, and loading them into GPU memory only when necessary. To reduce performance and communication overheads, CLM uses a novel offloading strategy that exploits observations about 3DGS's memory access pattern for pipelining, and thus overlap GPU-to-CPU communication, GPU computation and CPU computation. Furthermore, we also exploit observation about the access pattern to reduce communication volume. Our evaluation shows that the resulting implementation can render a large scene that requires 100 million Gaussians on a single RTX4090 and achieve state-of-the-art reconstruction quality.",
    "arxiv_url": "https://arxiv.org/abs/2511.04951v1",
    "pdf_url": "https://arxiv.org/pdf/2511.04951v1",
    "published_date": "2025-11-07",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "ar",
      "3d gaussian",
      "large scene",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.04951v1",
      "pdf": "https://arxiv.org/pdf/2511.04951v1"
    },
    "bibtex": ""
  },
  {
    "title": "Channel Knowledge Map Construction: Recent Advances and Open Challenges",
    "authors": [
      "Zixiang Ren",
      "Juncong Zhou",
      "Jie Xu",
      "Ling Qiu",
      "Yong Zeng",
      "Han Hu",
      "Juyong Zhang",
      "Rui Zhang"
    ],
    "abstract": "Channel knowledge map (CKM) has emerged as a pivotal technology for environment-aware wireless communications and sensing, which provides a priori location-specific channel knowledge to facilitate network optimization. Efficient CKM construction is an important technical problem for its effective implementation. This article provides a comprehensive overview of recent advances in CKM construction. First, we examine classical interpolation-based CKM construction methods, highlighting their limitations in practical deployments. Next, we explore image processing and generative artificial intelligence (AI) techniques, which leverage feature extraction to construct CKMs based on environmental knowledge. Furthermore, we present emerging wireless radiance field (WRF) frameworks that exploit neural radiance fields or Gaussian splatting to construct high-fidelity CKMs from sparse measurement data. Finally, we outline various future research directions in real-time and cross-domain CKM construction, as well as cost-efficient deployment of CKMs.",
    "arxiv_url": "https://arxiv.org/abs/2511.04944v1",
    "pdf_url": "https://arxiv.org/pdf/2511.04944v1",
    "published_date": "2025-11-07",
    "categories": [
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "ar",
      "efficient",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.04944v1",
      "pdf": "https://arxiv.org/pdf/2511.04944v1"
    },
    "bibtex": ""
  },
  {
    "title": "3D Gaussian Point Encoders",
    "authors": [
      "Jim James",
      "Ben Wilson",
      "Simon Lucey",
      "James Hays"
    ],
    "abstract": "In this work, we introduce the 3D Gaussian Point Encoder, an explicit per-point embedding built on mixtures of learned 3D Gaussians. This explicit geometric representation for 3D recognition tasks is a departure from widely used implicit representations such as PointNet. However, it is difficult to learn 3D Gaussian encoders in end-to-end fashion with standard optimizers. We develop optimization techniques based on natural gradients and distillation from PointNets to find a Gaussian Basis that can reconstruct PointNet activations. The resulting 3D Gaussian Point Encoders are faster and more parameter efficient than traditional PointNets. As in the 3D reconstruction literature where there has been considerable interest in the move from implicit (e.g., NeRF) to explicit (e.g., Gaussian Splatting) representations, we can take advantage of computational geometry heuristics to accelerate 3D Gaussian Point Encoders further. We extend filtering techniques from 3D Gaussian Splatting to construct encoders that run 2.7 times faster as a comparable accuracy PointNet while using 46% less memory and 88% fewer FLOPs. Furthermore, we demonstrate the effectiveness of 3D Gaussian Point Encoders as a component in Mamba3D, running 1.27 times faster and achieving a reduction in memory and FLOPs by 42% and 54% respectively. 3D Gaussian Point Encoders are lightweight enough to achieve high framerates on CPU-only devices.",
    "arxiv_url": "https://arxiv.org/abs/2511.04797v1",
    "pdf_url": "https://arxiv.org/pdf/2511.04797v1",
    "published_date": "2025-11-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "recognition",
      "3d reconstruction",
      "geometry",
      "ar",
      "nerf",
      "3d gaussian",
      "efficient",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.04797v1",
      "pdf": "https://arxiv.org/pdf/2511.04797v1"
    },
    "bibtex": ""
  },
  {
    "title": "Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions",
    "authors": [
      "Kaifeng Zhang",
      "Shuo Sha",
      "Hanxiao Jiang",
      "Matthew Loper",
      "Hyunjong Song",
      "Guangyan Cai",
      "Zhuo Xu",
      "Xiaochen Hu",
      "Changxi Zheng",
      "Yunzhu Li"
    ],
    "abstract": "Robotic manipulation policies are advancing rapidly, but their direct evaluation in the real world remains costly, time-consuming, and difficult to reproduce, particularly for tasks involving deformable objects. Simulation provides a scalable and systematic alternative, yet existing simulators often fail to capture the coupled visual and physical complexity of soft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from real-world videos and renders robots, objects, and environments with photorealistic fidelity using 3D Gaussian Splatting. We validate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and T-block pushing, demonstrating that simulated rollouts correlate strongly with real-world execution performance and reveal key behavioral patterns of learned policies. Our results suggest that combining physics-informed reconstruction with high-quality rendering enables reproducible, scalable, and accurate evaluation of robotic manipulation policies. Website: https://real2sim-eval.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2511.04665v2",
    "pdf_url": "https://arxiv.org/pdf/2511.04665v2",
    "published_date": "2025-11-06",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "body",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.04665v2",
      "pdf": "https://arxiv.org/pdf/2511.04665v2",
      "project": "https://real2sim-eval.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "FastGS: Training 3D Gaussian Splatting in 100 Seconds",
    "authors": [
      "Shiwei Ren",
      "Tianci Wen",
      "Yongchun Fang",
      "Biao Lu"
    ],
    "abstract": "The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to properly regulate the number of Gaussians during training, causing redundant computational time overhead. In this paper, we propose FastGS, a novel, simple, and general acceleration framework that fully considers the importance of each Gaussian based on multi-view consistency, efficiently solving the trade-off between training time and rendering quality. We innovatively design a densification and pruning strategy based on multi-view consistency, dispensing with the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks & Temples, and Deep Blending datasets demonstrate that our method significantly outperforms the state-of-the-art methods in training speed, achieving a 3.32$\\times$ training acceleration and comparable rendering quality compared with DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\\times$ acceleration compared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that FastGS exhibits strong generality, delivering 2-7$\\times$ training acceleration across various tasks, including dynamic scene reconstruction, surface reconstruction, sparse-view reconstruction, large-scale reconstruction, and simultaneous localization and mapping. The project page is available at https://fastgs.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2511.04283v3",
    "pdf_url": "https://arxiv.org/pdf/2511.04283v3",
    "published_date": "2025-11-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "localization",
      "sparse-view",
      "dynamic",
      "acceleration",
      "face",
      "ar",
      "nerf",
      "mapping",
      "3d gaussian",
      "efficient",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.04283v3",
      "pdf": "https://arxiv.org/pdf/2511.04283v3",
      "project": "https://fastgs.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation",
    "authors": [
      "Yuwen Tao",
      "Kanglei Zhou",
      "Xin Tan",
      "Yuan Xie"
    ],
    "abstract": "Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret free-form language expressions and localize the corresponding 3D regions in Gaussian fields. While recent advances have introduced cross-modal alignment between language and 3D geometry, existing pipelines still struggle with cross-view consistency due to their reliance on 2D rendered pseudo supervision and view specific feature learning. In this work, we present Camera Aware Referring Field (CaRF), a fully differentiable framework that operates directly in the 3D Gaussian space and achieves multi view consistency. Specifically, CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates camera geometry into Gaussian text interactions to explicitly model view dependent variations and enhance geometric reasoning. Building on this, In Training Paired View Supervision (ITPVS) is proposed to align per Gaussian logits across calibrated views during training, effectively mitigating single view overfitting and exposing inter view discrepancies for optimization. Extensive experiments on three representative benchmarks demonstrate that CaRF achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively. Moreover, this work promotes more reliable and view consistent 3D scene understanding, with potential benefits for embodied AI, AR/VR interaction, and autonomous perception.",
    "arxiv_url": "https://arxiv.org/abs/2511.03992v1",
    "pdf_url": "https://arxiv.org/pdf/2511.03992v1",
    "published_date": "2025-11-06",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "geometry",
      "segmentation",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.03992v1",
      "pdf": "https://arxiv.org/pdf/2511.03992v1"
    },
    "bibtex": ""
  },
  {
    "title": "DentalSplat: Dental Occlusion Novel View Synthesis from Sparse Intra-Oral Photographs",
    "authors": [
      "Yiyi Miao",
      "Taoyu Wu",
      "Tong Chen",
      "Sihao Li",
      "Ji Jiang",
      "Youpeng Yang",
      "Angelos Stefanidis",
      "Limin Yu",
      "Jionglong Su"
    ],
    "abstract": "In orthodontic treatment, particularly within telemedicine contexts, observing patients' dental occlusion from multiple viewpoints facilitates timely clinical decision-making. Recent advances in 3D Gaussian Splatting (3DGS) have shown strong potential in 3D reconstruction and novel view synthesis. However, conventional 3DGS pipelines typically rely on densely captured multi-view inputs and precisely initialized camera poses, limiting their practicality. Orthodontic cases, in contrast, often comprise only three sparse images, specifically, the anterior view and bilateral buccal views, rendering the reconstruction task especially challenging. The extreme sparsity of input views severely degrades reconstruction quality, while the absence of camera pose information further complicates the process. To overcome these limitations, we propose DentalSplat, an effective framework for 3D reconstruction from sparse orthodontic imagery. Our method leverages a prior-guided dense stereo reconstruction model to initialize the point cloud, followed by a scale-adaptive pruning strategy to improve the training efficiency and reconstruction quality of 3DGS. In scenarios with extremely sparse viewpoints, we further incorporate optical flow as a geometric constraint, coupled with gradient regularization, to enhance rendering fidelity. We validate our approach on a large-scale dataset comprising 950 clinical cases and an additional video-based test set of 195 cases designed to simulate real-world remote orthodontic imaging conditions. Experimental results demonstrate that our method effectively handles sparse input scenarios and achieves superior novel view synthesis quality for dental occlusion visualization, outperforming state-of-the-art techniques.",
    "arxiv_url": "https://arxiv.org/abs/2511.03099v1",
    "pdf_url": "https://arxiv.org/pdf/2511.03099v1",
    "published_date": "2025-11-05",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "sparse view",
      "3d reconstruction",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.03099v1",
      "pdf": "https://arxiv.org/pdf/2511.03099v1"
    },
    "bibtex": ""
  },
  {
    "title": "PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction & Editing",
    "authors": [
      "Antonio Oroz",
      "Matthias Nießner",
      "Tobias Kirschstein"
    ],
    "abstract": "We present PercHead, a method for single-image 3D head reconstruction and semantic 3D editing - two tasks that are inherently challenging due to severe view occlusions, weak perceptual supervision, and the ambiguity of editing in 3D space. We develop a unified base model for reconstructing view-consistent 3D heads from a single input image. The model employs a dual-branch encoder followed by a ViT-based decoder that lifts 2D features into 3D space through iterative cross-attention. Rendering is performed using Gaussian Splatting. At the heart of our approach is a novel perceptual supervision strategy based on DINOv2 and SAM2.1, which provides rich, generalized signals for both geometric and appearance fidelity. Our model achieves state-of-the-art performance in novel-view synthesis and, furthermore, exhibits exceptional robustness to extreme viewing angles compared to established baselines. Furthermore, this base model can be seamlessly extended for semantic 3D editing by swapping the encoder and finetuning the network. In this variant, we disentangle geometry and style through two distinct input modalities: a segmentation map to control geometry and either a text prompt or a reference image to specify appearance. We highlight the intuitive and powerful 3D editing capabilities of our model through a lightweight, interactive GUI, where users can effortlessly sculpt geometry by drawing segmentation maps and stylize appearance via natural language or image prompts.   Project Page: https://antoniooroz.github.io/PercHead Video: https://www.youtube.com/watch?v=4hFybgTk4kE",
    "arxiv_url": "https://arxiv.org/abs/2511.02777v1",
    "pdf_url": "https://arxiv.org/pdf/2511.02777v1",
    "published_date": "2025-11-04",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "lightweight",
      "geometry",
      "segmentation",
      "ar",
      "semantic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.02777v1",
      "pdf": "https://arxiv.org/pdf/2511.02777v1",
      "project": "https://antoniooroz.github.io/PercHead",
      "video": "https://www.youtube.com/watch?v=4hFybgTk4kE"
    },
    "bibtex": ""
  },
  {
    "title": "Object-Centric 3D Gaussian Splatting for Strawberry Plant Reconstruction and Phenotyping",
    "authors": [
      "Jiajia Li",
      "Keyi Zhu",
      "Qianwen Zhang",
      "Dong Chen",
      "Qi Sun",
      "Zhaojian Li"
    ],
    "abstract": "Strawberries are among the most economically significant fruits in the United States, generating over $2 billion in annual farm-gate sales and accounting for approximately 13% of the total fruit production value. Plant phenotyping plays a vital role in selecting superior cultivars by characterizing plant traits such as morphology, canopy structure, and growth dynamics. However, traditional plant phenotyping methods are time-consuming, labor-intensive, and often destructive. Recently, neural rendering techniques, notably Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have emerged as powerful frameworks for high-fidelity 3D reconstruction. By capturing a sequence of multi-view images or videos around a target plant, these methods enable non-destructive reconstruction of complex plant architectures. Despite their promise, most current applications of 3DGS in agricultural domains reconstruct the entire scene, including background elements, which introduces noise, increases computational costs, and complicates downstream trait analysis. To address this limitation, we propose a novel object-centric 3D reconstruction framework incorporating a preprocessing pipeline that leverages the Segment Anything Model v2 (SAM-2) and alpha channel background masking to achieve clean strawberry plant reconstructions. This approach produces more accurate geometric representations while substantially reducing computational time. With a background-free reconstruction, our algorithm can automatically estimate important plant traits, such as plant height and canopy width, using DBSCAN clustering and Principal Component Analysis (PCA). Experimental results show that our method outperforms conventional pipelines in both accuracy and efficiency, offering a scalable and non-destructive solution for strawberry plant phenotyping.",
    "arxiv_url": "https://arxiv.org/abs/2511.02207v1",
    "pdf_url": "https://arxiv.org/pdf/2511.02207v1",
    "published_date": "2025-11-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "high-fidelity",
      "3d reconstruction",
      "neural rendering",
      "ar",
      "nerf",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.02207v1",
      "pdf": "https://arxiv.org/pdf/2511.02207v1"
    },
    "bibtex": ""
  },
  {
    "title": "3D Gaussian Radiation Field Modeling for Integrated RIS-FAS Systems: Analysis and Optimization",
    "authors": [
      "Kaining Wang",
      "Bo Yang",
      "Yusheng Lei",
      "Zhiwen Yu",
      "Xuelin Cao",
      "Liang Wang",
      "Bin Guo",
      "George C. Alexandropoulos",
      "Mérouane Debbah",
      "Zhu Han"
    ],
    "abstract": "The integration of reconfigurable intelligent surfaces (RIS) and fluid antenna systems (FAS) has attracted considerable attention due to its tremendous potential in enhancing wireless communication performance. However, under fast-fading channel conditions, rapidly and effectively performing joint optimization of the antenna positions in an FAS system and the RIS phase configuration remains a critical challenge. Traditional optimization methods typically rely on complex iterative computations, thus making it challenging to obtain optimal solutions in real time within dynamic channel environments. To address this issue, this paper introduces a field information-driven optimization method based on three-dimensional Gaussian radiation-field modeling for real-time optimization of integrated FAS-RIS systems. In the proposed approach, obstacles are treated as virtual transmitters and, by separately learning the amplitude and phase variations, the model can quickly generate high-precision channel information based on the transmitter's position. This design eliminates the need for extensive pilot overhead and cumbersome computations. On this framework, an alternating optimization scheme is presented to jointly optimize the FAS position and the RIS phase configuration. Simulation results demonstrate that the proposed method significantly outperforms existing approaches in terms of spectrum prediction accuracy, convergence speed, and minimum achievable rate, validating its effectiveness and practicality in fast-fading scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2511.01373v1",
    "pdf_url": "https://arxiv.org/pdf/2511.01373v1",
    "published_date": "2025-11-03",
    "categories": [
      "cs.NI"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "dynamic",
      "face",
      "ar",
      "3d gaussian",
      "fast"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.01373v1",
      "pdf": "https://arxiv.org/pdf/2511.01373v1"
    },
    "bibtex": ""
  },
  {
    "title": "GauDP: Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies",
    "authors": [
      "Ziye Wang",
      "Li Kang",
      "Yiran Qin",
      "Jiahua Ma",
      "Zhanglin Peng",
      "Lei Bai",
      "Ruimao Zhang"
    ],
    "abstract": "Recently, effective coordination in embodied multi-agent systems has remained a fundamental challenge, particularly in scenarios where agents must balance individual perspectives with global environmental awareness. Existing approaches often struggle to balance fine-grained local control with comprehensive scene understanding, resulting in limited scalability and compromised collaboration quality. In this paper, we present GauDP, a novel Gaussian-image synergistic representation that facilitates scalable, perception-aware imitation learning in multi-agent collaborative systems. Specifically, GauDP constructs a globally consistent 3D Gaussian field from decentralized RGB observations, then dynamically redistributes 3D Gaussian attributes to each agent's local perspective. This enables all agents to adaptively query task-critical features from the shared scene representation while maintaining their individual viewpoints. This design facilitates both fine-grained control and globally coherent behavior without requiring additional sensing modalities (e.g., 3D point cloud). We evaluate GauDP on the RoboFactory benchmark, which includes diverse multi-arm manipulation tasks. Our method achieves superior performance over existing image-based methods and approaches the effectiveness of point-cloud-driven methods, while maintaining strong scalability as the number of agents increases.",
    "arxiv_url": "https://arxiv.org/abs/2511.00998v1",
    "pdf_url": "https://arxiv.org/pdf/2511.00998v1",
    "published_date": "2025-11-02",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "dynamic",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.00998v1",
      "pdf": "https://arxiv.org/pdf/2511.00998v1"
    },
    "bibtex": ""
  },
  {
    "title": "4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting",
    "authors": [
      "Chun-Tin Wu",
      "Jun-Cheng Chen"
    ],
    "abstract": "Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel view synthesis, extending it to dynamic scenes still results in substantial memory overhead from replicating Gaussians across frames. To address this challenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines voxel-based representations with neural Gaussian splatting for efficient dynamic scene modeling. Instead of generating separate Gaussian sets per timestamp, our method employs a compact set of neural voxels with learned deformation fields to model temporal dynamics. The design greatly reduces memory consumption and accelerates training while preserving high image quality. We further introduce a novel view refinement stage that selectively improves challenging viewpoints through targeted optimization, maintaining global efficiency while enhancing rendering quality for difficult viewing angles. Experiments demonstrate that our method outperforms state-of-the-art approaches with significant memory reduction and faster training, enabling real-time rendering with superior visual fidelity.",
    "arxiv_url": "https://arxiv.org/abs/2511.00560v1",
    "pdf_url": "https://arxiv.org/pdf/2511.00560v1",
    "published_date": "2025-11-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "compact",
      "head",
      "dynamic",
      "deformation",
      "real-time rendering",
      "ar",
      "efficient rendering",
      "3d gaussian",
      "efficient",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.00560v1",
      "pdf": "https://arxiv.org/pdf/2511.00560v1"
    },
    "bibtex": ""
  },
  {
    "title": "Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models",
    "authors": [
      "Panwang Pan",
      "Chenguo Lin",
      "Jingjing Zhao",
      "Chenxin Li",
      "Yuchen Lin",
      "Haopeng Li",
      "Honglei Yan",
      "Kairun Wen",
      "Yunlong Lin",
      "Yixuan Yuan",
      "Yadong Mu"
    ],
    "abstract": "We introduce Diff4Splat, a feed-forward method that synthesizes controllable and explicit 4D scenes from a single image. Our approach unifies the generative priors of video diffusion models with geometry and motion constraints learned from large-scale 4D datasets. Given a single input image, a camera trajectory, and an optional text prompt, Diff4Splat directly predicts a deformable 3D Gaussian field that encodes appearance, geometry, and motion, all in a single forward pass, without test-time optimization or post-hoc refinement. At the core of our framework lies a video latent transformer, which augments video diffusion models to jointly capture spatio-temporal dependencies and predict time-varying 3D Gaussian primitives. Training is guided by objectives on appearance fidelity, geometric accuracy, and motion consistency, enabling Diff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate the effectiveness of Diff4Splatacross video generation, novel view synthesis, and geometry extraction, where it matches or surpasses optimization-based methods for dynamic scene synthesis while being significantly more efficient.",
    "arxiv_url": "https://arxiv.org/abs/2511.00503v1",
    "pdf_url": "https://arxiv.org/pdf/2511.00503v1",
    "published_date": "2025-11-01",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "geometry",
      "ar",
      "3d gaussian",
      "efficient",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.00503v1",
      "pdf": "https://arxiv.org/pdf/2511.00503v1"
    },
    "bibtex": ""
  },
  {
    "title": "Object-Aware 4D Human Motion Generation",
    "authors": [
      "Shurui Gui",
      "Deep Anil Patel",
      "Xiner Li",
      "Martin Renqiang Min"
    ],
    "abstract": "Recent advances in video diffusion models have enabled the generation of high-quality videos. However, these videos still suffer from unrealistic deformations, semantic violations, and physical inconsistencies that are largely rooted in the absence of 3D physical priors. To address these challenges, we propose an object-aware 4D human motion generation framework grounded in 3D Gaussian representations and motion diffusion priors. With pre-generated 3D humans and objects, our method, Motion Score Distilled Interaction (MSDI), employs the spatial and prompt semantic information in large language models (LLMs) and motion priors through the proposed Motion Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs enables our spatial-aware motion optimization, which distills score gradients from pre-trained motion diffusion models, to refine human motion while respecting object and semantic constraints. Unlike prior methods requiring joint training on limited interaction datasets, our zero-shot approach avoids retraining and generalizes to out-of-distribution object aware human motions. Experiments demonstrate that our framework produces natural and physically plausible human motions that respect 3D spatial context, offering a scalable solution for realistic 4D generation.",
    "arxiv_url": "https://arxiv.org/abs/2511.00248v1",
    "pdf_url": "https://arxiv.org/pdf/2511.00248v1",
    "published_date": "2025-10-31",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "deformation",
      "ar",
      "semantic",
      "human",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.00248v1",
      "pdf": "https://arxiv.org/pdf/2511.00248v1"
    },
    "bibtex": ""
  },
  {
    "title": "Detail Enhanced Gaussian Splatting for Large-Scale Volumetric Capture",
    "authors": [
      "Julien Philip",
      "Li Ma",
      "Pascal Clausen",
      "Wenqi Xian",
      "Ahmet Levent Taşel",
      "Mingming He",
      "Xueming Yu",
      "David M. George",
      "Ning Yu",
      "Oliver Pilarski",
      "Paul Debevec"
    ],
    "abstract": "We present a unique system for large-scale, multi-performer, high resolution 4D volumetric capture providing realistic free-viewpoint video up to and including 4K resolution facial closeups. To achieve this, we employ a novel volumetric capture, reconstruction and rendering pipeline based on Dynamic Gaussian Splatting and Diffusion-based Detail Enhancement. We design our pipeline specifically to meet the demands of high-end media production. We employ two capture rigs: the Scene Rig, which captures multi-actor performances at a resolution which falls short of 4K production quality, and the Face Rig, which records high-fidelity single-actor facial detail to serve as a reference for detail enhancement. We first reconstruct dynamic performances from the Scene Rig using 4D Gaussian Splatting, incorporating new model designs and training strategies to improve reconstruction, dynamic range, and rendering quality. Then to render high-quality images for facial closeups, we introduce a diffusion-based detail enhancement model. This model is fine-tuned with high-fidelity data from the same actors recorded in the Face Rig. We train on paired data generated from low- and high-quality Gaussian Splatting (GS) models, using the low-quality input to match the quality of the Scene Rig, with the high-quality GS as ground truth. Our results demonstrate the effectiveness of this pipeline in bridging the gap between the scalable performance capture of a large-scale rig and the high-resolution standards required for film and media production.",
    "arxiv_url": "https://arxiv.org/abs/2511.21697v1",
    "pdf_url": "https://arxiv.org/pdf/2511.21697v1",
    "published_date": "2025-10-31",
    "categories": [
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "high-fidelity",
      "face",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2511.21697v1",
      "pdf": "https://arxiv.org/pdf/2511.21697v1"
    },
    "bibtex": ""
  },
  {
    "title": "SAGS: Self-Adaptive Alias-Free Gaussian Splatting for Dynamic Surgical Endoscopic Reconstruction",
    "authors": [
      "Wenfeng Huang",
      "Xiangyun Liao",
      "Yinling Qian",
      "Hao Liu",
      "Yongming Yang",
      "Wenjing Jia",
      "Qiong Wang"
    ],
    "abstract": "Surgical reconstruction of dynamic tissues from endoscopic videos is a crucial technology in robot-assisted surgery. The development of Neural Radiance Fields (NeRFs) has greatly advanced deformable tissue reconstruction, achieving high-quality results from video and image sequences. However, reconstructing deformable endoscopic scenes remains challenging due to aliasing and artifacts caused by tissue movement, which can significantly degrade visualization quality. The introduction of 3D Gaussian Splatting (3DGS) has improved reconstruction efficiency by enabling a faster rendering pipeline. Nevertheless, existing 3DGS methods often prioritize rendering speed while neglecting these critical issues. To address these challenges, we propose SAGS, a self-adaptive alias-free Gaussian splatting framework. We introduce an attention-driven, dynamically weighted 4D deformation decoder, leveraging 3D smoothing filters and 2D Mip filters to mitigate artifacts in deformable tissue reconstruction and better capture the fine details of tissue movement. Experimental results on two public benchmarks, EndoNeRF and SCARED, demonstrate that our method achieves superior performance in all metrics of PSNR, SSIM, and LPIPS compared to the state of the art while also delivering better visualization quality.",
    "arxiv_url": "https://arxiv.org/abs/2510.27318v1",
    "pdf_url": "https://arxiv.org/pdf/2510.27318v1",
    "published_date": "2025-10-31",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "deformation",
      "ar",
      "nerf",
      "3d gaussian",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.27318v1",
      "pdf": "https://arxiv.org/pdf/2510.27318v1"
    },
    "bibtex": ""
  },
  {
    "title": "WildfireX-SLAM: A Large-scale Low-altitude RGB-D Dataset for Wildfire SLAM and Beyond",
    "authors": [
      "Zhicong Sun",
      "Jacqueline Lo",
      "Jinxing Hu"
    ],
    "abstract": "3D Gaussian splatting (3DGS) and its subsequent variants have led to remarkable progress in simultaneous localization and mapping (SLAM). While most recent 3DGS-based SLAM works focus on small-scale indoor scenes, developing 3DGS-based SLAM methods for large-scale forest scenes holds great potential for many real-world applications, especially for wildfire emergency response and forest management. However, this line of research is impeded by the absence of a comprehensive and high-quality dataset, and collecting such a dataset over real-world scenes is costly and technically infeasible. To this end, we have built a large-scale, comprehensive, and high-quality synthetic dataset for SLAM in wildfire and forest environments. Leveraging the Unreal Engine 5 Electric Dreams Environment Sample Project, we developed a pipeline to easily collect aerial and ground views, including ground-truth camera poses and a range of additional data modalities from unmanned aerial vehicle. Our pipeline also provides flexible controls on environmental factors such as light, weather, and types and conditions of wildfire, supporting the need for various tasks covering forest mapping, wildfire emergency response, and beyond. The resulting pilot dataset, WildfireX-SLAM, contains 5.5k low-altitude RGB-D aerial images from a large-scale forest map with a total size of 16 km2. On top of WildfireX-SLAM, a thorough benchmark is also conducted, which not only reveals the unique challenges of 3DGS-based SLAM in the forest but also highlights potential improvements for future works. The dataset and code will be publicly available. Project page: https://zhicongsun.github.io/wildfirexslam.",
    "arxiv_url": "https://arxiv.org/abs/2510.27133v1",
    "pdf_url": "https://arxiv.org/pdf/2510.27133v1",
    "published_date": "2025-10-31",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "ar",
      "3d gaussian",
      "slam",
      "mapping",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.27133v1",
      "pdf": "https://arxiv.org/pdf/2510.27133v1",
      "project": "https://zhicongsun.github.io/wildfirexslam"
    },
    "bibtex": ""
  },
  {
    "title": "DC4GS: Directional Consistency-Driven Adaptive Density Control for 3D Gaussian Splatting",
    "authors": [
      "Moonsoo Jeong",
      "Dongbeen Kim",
      "Minseong Kim",
      "Sungkil Lee"
    ],
    "abstract": "We present a Directional Consistency (DC)-driven Adaptive Density Control (ADC) for 3D Gaussian Splatting (DC4GS). Whereas the conventional ADC bases its primitive splitting on the magnitudes of positional gradients, we further incorporate the DC of the gradients into ADC, and realize it through the angular coherence of the gradients. Our DC better captures local structural complexities in ADC, avoiding redundant splitting. When splitting is required, we again utilize the DC to define optimal split positions so that sub-primitives best align with the local structures than the conventional random placement. As a consequence, our DC4GS greatly reduces the number of primitives (up to 30% in our experiments) than the existing ADC, and also enhances reconstruction fidelity greatly.",
    "arxiv_url": "https://arxiv.org/abs/2510.26921v1",
    "pdf_url": "https://arxiv.org/pdf/2510.26921v1",
    "published_date": "2025-10-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.26921v1",
      "pdf": "https://arxiv.org/pdf/2510.26921v1"
    },
    "bibtex": ""
  },
  {
    "title": "HEIR: Learning Graph-Based Motion Hierarchies",
    "authors": [
      "Cheng Zheng",
      "William Koch",
      "Baiang Li",
      "Felix Heide"
    ],
    "abstract": "Hierarchical structures of motion exist across research fields, including computer vision, graphics, and robotics, where complex dynamics typically arise from coordinated interactions among simpler motion components. Existing methods to model such dynamics typically rely on manually-defined or heuristic hierarchies with fixed motion primitives, limiting their generalizability across different tasks. In this work, we propose a general hierarchical motion modeling method that learns structured, interpretable motion relationships directly from data. Our method represents observed motions using graph-based hierarchies, explicitly decomposing global absolute motions into parent-inherited patterns and local motion residuals. We formulate hierarchy inference as a differentiable graph learning problem, where vertices represent elemental motions and directed edges capture learned parent-child dependencies through graph neural networks. We evaluate our hierarchical reconstruction approach on three examples: 1D translational motion, 2D rotational motion, and dynamic 3D scene deformation via Gaussian splatting. Experimental results show that our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases, and produces more realistic and interpretable deformations compared to the baseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable, data-driven hierarchical modeling paradigm, our method offers a formulation applicable to a broad range of motion-centric tasks. Project Page: https://light.princeton.edu/HEIR/",
    "arxiv_url": "https://arxiv.org/abs/2510.26786v1",
    "pdf_url": "https://arxiv.org/pdf/2510.26786v1",
    "published_date": "2025-10-30",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "dynamic",
      "deformation",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.26786v1",
      "pdf": "https://arxiv.org/pdf/2510.26786v1",
      "project": "https://light.princeton.edu/HEIR"
    },
    "bibtex": ""
  },
  {
    "title": "The Impact and Outlook of 3D Gaussian Splatting",
    "authors": [
      "Bernhard Kerbl"
    ],
    "abstract": "Since its introduction, 3D Gaussian Splatting (3DGS) has rapidly transformed the landscape of 3D scene representations, inspiring an extensive body of associated research. Follow-up work includes analyses and contributions that enhance the efficiency, scalability, and real-world applicability of 3DGS. In this summary, we present an overview of several key directions that have emerged in the wake of 3DGS. We highlight advances enabling resource-efficient training and rendering, the evolution toward dynamic (or four-dimensional, 4DGS) representations, and deeper exploration of the mathematical foundations underlying its appearance modeling and rendering process. Furthermore, we examine efforts to bring 3DGS to mobile and virtual reality platforms, its extension to massive-scale environments, and recent progress toward near-instant radiance field reconstruction via feed-forward or distributed computation. Collectively, these developments illustrate how 3DGS has evolved from a breakthrough representation into a versatile and foundational tool for 3D vision and graphics.",
    "arxiv_url": "https://arxiv.org/abs/2510.26694v1",
    "pdf_url": "https://arxiv.org/pdf/2510.26694v1",
    "published_date": "2025-10-30",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "body",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.26694v1",
      "pdf": "https://arxiv.org/pdf/2510.26694v1"
    },
    "bibtex": ""
  },
  {
    "title": "AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian Splatting SLAM",
    "authors": [
      "Mirko Usuelli",
      "David Rapado-Rincon",
      "Gert Kootstra",
      "Matteo Matteucci"
    ],
    "abstract": "Autonomous robots in orchards require real-time 3D scene understanding despite repetitive row geometry, seasonal appearance changes, and wind-driven foliage motion. We present AgriGS-SLAM, a Visual--LiDAR SLAM framework that couples direct LiDAR odometry and loop closures with multi-camera 3D Gaussian Splatting (3DGS) rendering. Batch rasterization across complementary viewpoints recovers orchard structure under occlusions, while a unified gradient-driven map lifecycle executed between keyframes preserves fine details and bounds memory. Pose refinement is guided by a probabilistic LiDAR-based depth consistency term, back-propagated through the camera projection to tighten geometry-appearance coupling. We deploy the system on a field platform in apple and pear orchards across dormancy, flowering, and harvesting, using a standardized trajectory protocol that evaluates both training-view and novel-view synthesis to reduce 3DGS overfitting in evaluation. Across seasons and sites, AgriGS-SLAM delivers sharper, more stable reconstructions and steadier trajectories than recent state-of-the-art 3DGS-SLAM baselines while maintaining real-time performance on-tractor. While demonstrated in orchard monitoring, the approach can be applied to other outdoor domains requiring robust multimodal perception.",
    "arxiv_url": "https://arxiv.org/abs/2510.26358v1",
    "pdf_url": "https://arxiv.org/pdf/2510.26358v1",
    "published_date": "2025-10-30",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "understanding",
      "geometry",
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "slam",
      "mapping",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.26358v1",
      "pdf": "https://arxiv.org/pdf/2510.26358v1"
    },
    "bibtex": ""
  },
  {
    "title": "6D Channel Knowledge Map Construction via Bidirectional Wireless Gaussian Splatting",
    "authors": [
      "Juncong Zhou",
      "Chao Hu",
      "Guanlin Wu",
      "Zixiang Ren",
      "Han Hu",
      "Juyong Zhang",
      "Rui Zhang",
      "Jie Xu"
    ],
    "abstract": "This paper investigates the construction of channel knowledge map (CKM) from sparse channel measurements. Dif ferent from conventional two-/three-dimensional (2D/3D) CKM approaches assuming fixed base station configurations, we present a six-dimensional (6D) CKM framework named bidirectional wireless Gaussian splatting (BiWGS), which is capable of mod eling wireless channels across dynamic transmitter (Tx) and receiver (Rx) positions in 3D space. BiWGS uses Gaussian el lipsoids to represent virtual scatterer clusters and environmental obstacles in the wireless environment. By properly learning the bidirectional scattering patterns and complex attenuation profiles based on channel measurements, these ellipsoids inherently cap ture the electromagnetic transmission characteristics of wireless environments, thereby accurately modeling signal transmission under varying transceiver configurations. Experiment results show that BiWGS significantly outperforms classic multi-layer perception (MLP) for the construction of 6D channel power gain map with varying Tx-Rx positions, and achieves spatial spectrum prediction accuracy comparable to the state-of-the art wireless radiation field Gaussian splatting (WRF-GS) for 3D CKM construction. This validates the capability of the proposed BiWGS in accomplishing dimensional expansion of 6D CKM construction, without compromising fidelity.",
    "arxiv_url": "https://arxiv.org/abs/2510.26166v1",
    "pdf_url": "https://arxiv.org/pdf/2510.26166v1",
    "published_date": "2025-10-30",
    "categories": [
      "eess.SP"
    ],
    "github_url": "",
    "keywords": [
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.26166v1",
      "pdf": "https://arxiv.org/pdf/2510.26166v1"
    },
    "bibtex": ""
  },
  {
    "title": "JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting",
    "authors": [
      "Xianben Yang",
      "Yuxuan Li",
      "Tao Wang",
      "Tao Wang",
      "Yi Jin",
      "Yidong Li",
      "Haibin Ling"
    ],
    "abstract": "Traditional novel view synthesis methods heavily rely on external camera pose estimation tools such as COLMAP, which often introduce computational bottlenecks and propagate errors. To address these challenges, we propose a unified framework that jointly optimizes 3D Gaussian points and camera poses without requiring pre-calibrated inputs. Our approach iteratively refines 3D Gaussian parameters and updates camera poses through a novel co-optimization strategy, ensuring simultaneous improvements in scene reconstruction fidelity and pose estimation accuracy. The key innovation lies in decoupling the joint optimization into two interleaved phases: first, updating 3D Gaussian parameters via differentiable rendering with fixed poses, and second, refining camera poses using a customized 3D optical flow algorithm that incorporates geometric and photometric constraints. This formulation progressively reduces projection errors, particularly in challenging scenarios with large viewpoint variations and sparse feature distributions, where traditional methods struggle. Extensive evaluations on multiple datasets demonstrate that our approach significantly outperforms existing COLMAP-free techniques in reconstruction quality, and also surpasses the standard COLMAP-based baseline in general.",
    "arxiv_url": "https://arxiv.org/abs/2510.26117v2",
    "pdf_url": "https://arxiv.org/pdf/2510.26117v2",
    "published_date": "2025-10-30",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.26117v2",
      "pdf": "https://arxiv.org/pdf/2510.26117v2"
    },
    "bibtex": ""
  },
  {
    "title": "Learning Disentangled Speech- and Expression-Driven Blendshapes for 3D Talking Face Animation",
    "authors": [
      "Yuxiang Mao",
      "Zhijie Zhang",
      "Zhiheng Zhang",
      "Jiawei Liu",
      "Chen Zeng",
      "Shihong Xia"
    ],
    "abstract": "Expressions are fundamental to conveying human emotions. With the rapid advancement of AI-generated content (AIGC), realistic and expressive 3D facial animation has become increasingly crucial. Despite recent progress in speech-driven lip-sync for talking-face animation, generating emotionally expressive talking faces remains underexplored. A major obstacle is the scarcity of real emotional 3D talking-face datasets due to the high cost of data capture. To address this, we model facial animation driven by both speech and emotion as a linear additive problem. Leveraging a 3D talking-face dataset with neutral expressions (VOCAset) and a dataset of 3D expression sequences (Florence4D), we jointly learn a set of blendshapes driven by speech and emotion. We introduce a sparsity constraint loss to encourage disentanglement between the two types of blendshapes while allowing the model to capture inherent secondary cross-domain deformations present in the training data. The learned blendshapes can be further mapped to the expression and jaw pose parameters of the FLAME model, enabling the animation of 3D Gaussian avatars. Qualitative and quantitative experiments demonstrate that our method naturally generates talking faces with specified expressions while maintaining accurate lip synchronization. Perceptual studies further show that our approach achieves superior emotional expressivity compared to existing methods, without compromising lip-sync quality.",
    "arxiv_url": "https://arxiv.org/abs/2510.25234v1",
    "pdf_url": "https://arxiv.org/pdf/2510.25234v1",
    "published_date": "2025-10-29",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "face",
      "deformation",
      "animation",
      "avatar",
      "ar",
      "human",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.25234v1",
      "pdf": "https://arxiv.org/pdf/2510.25234v1"
    },
    "bibtex": ""
  },
  {
    "title": "D$^2$GS: Dense Depth Regularization for LiDAR-free Urban Scene Reconstruction",
    "authors": [
      "Kejing Xia",
      "Jidong Jia",
      "Ke Jin",
      "Yucai Bai",
      "Li Sun",
      "Dacheng Tao",
      "Youjian Zhang"
    ],
    "abstract": "Recently, Gaussian Splatting (GS) has shown great potential for urban scene reconstruction in the field of autonomous driving. However, current urban scene reconstruction methods often depend on multimodal sensors as inputs, \\textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDAR point clouds can largely mitigate ill-posedness in reconstruction, acquiring such accurate LiDAR data is still challenging in practice: i) precise spatiotemporal calibration between LiDAR and other sensors is required, as they may not capture data simultaneously; ii) reprojection errors arise from spatial misalignment when LiDAR and cameras are mounted at different locations. To avoid the difficulty of acquiring accurate LiDAR depth, we propose D$^2$GS, a LiDAR-free urban scene reconstruction framework. In this work, we obtain geometry priors that are as effective as LiDAR while being denser and more accurate. $\\textbf{First}$, we initialize a dense point cloud by back-projecting multi-view metric depth predictions. This point cloud is then optimized by a Progressive Pruning strategy to improve the global consistency. $\\textbf{Second}$, we jointly refine Gaussian geometry and predicted dense metric depth via a Depth Enhancer. Specifically, we leverage diffusion priors from a depth foundation model to enhance the depth maps rendered by Gaussians. In turn, the enhanced depths provide stronger geometric constraints during Gaussian training. $\\textbf{Finally}$, we improve the accuracy of ground geometry by constraining the shape and normal attributes of Gaussians within road regions. Extensive experiments on the Waymo dataset demonstrate that our method consistently outperforms state-of-the-art methods, producing more accurate geometry even when compared with those using ground-truth LiDAR data.",
    "arxiv_url": "https://arxiv.org/abs/2510.25173v2",
    "pdf_url": "https://arxiv.org/pdf/2510.25173v2",
    "published_date": "2025-10-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "urban scene",
      "geometry",
      "autonomous driving",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.25173v2",
      "pdf": "https://arxiv.org/pdf/2510.25173v2"
    },
    "bibtex": ""
  },
  {
    "title": "AtlasGS: Atlanta-world Guided Surface Reconstruction with Implicit Structured Gaussians",
    "authors": [
      "Xiyu Zhang",
      "Chong Bao",
      "Yipeng Chen",
      "Hongjia Zhai",
      "Yitong Dong",
      "Hujun Bao",
      "Zhaopeng Cui",
      "Guofeng Zhang"
    ],
    "abstract": "3D reconstruction of indoor and urban environments is a prominent research topic with various downstream applications. However, existing geometric priors for addressing low-texture regions in indoor and urban settings often lack global consistency. Moreover, Gaussian Splatting and implicit SDF fields often suffer from discontinuities or exhibit computational inefficiencies, resulting in a loss of detail. To address these issues, we propose an Atlanta-world guided implicit-structured Gaussian Splatting that achieves smooth indoor and urban scene reconstruction while preserving high-frequency details and rendering efficiency. By leveraging the Atlanta-world model, we ensure the accurate surface reconstruction for low-texture regions, while the proposed novel implicit-structured GS representations provide smoothness without sacrificing efficiency and high-frequency details. Specifically, we propose a semantic GS representation to predict the probability of all semantic regions and deploy a structure plane regularization with learnable plane indicators for global accurate surface reconstruction. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches in both indoor and urban scenes, delivering superior surface reconstruction quality.",
    "arxiv_url": "https://arxiv.org/abs/2510.25129v1",
    "pdf_url": "https://arxiv.org/pdf/2510.25129v1",
    "published_date": "2025-10-29",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "urban scene",
      "face",
      "3d reconstruction",
      "ar",
      "semantic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.25129v1",
      "pdf": "https://arxiv.org/pdf/2510.25129v1"
    },
    "bibtex": ""
  },
  {
    "title": "NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation",
    "authors": [
      "Mingyu Jeong",
      "Eunsung Kim",
      "Sehun Park",
      "Andrew Jaeyong Choi"
    ],
    "abstract": "We present NVSim, a framework that automatically constructs large-scale, navigable indoor simulators from only common image sequences, overcoming the cost and scalability limitations of traditional 3D scanning. Our approach adapts 3D Gaussian Splatting to address visual artifacts on sparsely observed floors a common issue in robotic traversal data. We introduce Floor-Aware Gaussian Splatting to ensure a clean, navigable ground plane, and a novel mesh-free traversability checking algorithm that constructs a topological graph by directly analyzing rendered views. We demonstrate our system's ability to generate valid, large-scale navigation graphs from real-world data. A video demonstration is avilable at https://youtu.be/tTiIQt6nXC8",
    "arxiv_url": "https://arxiv.org/abs/2510.24335v1",
    "pdf_url": "https://arxiv.org/pdf/2510.24335v1",
    "published_date": "2025-10-28",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.24335v1",
      "pdf": "https://arxiv.org/pdf/2510.24335v1",
      "video": "https://youtu.be/tTiIQt6nXC8"
    },
    "bibtex": ""
  },
  {
    "title": "LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal Open-vocabulary Multi-goal Visual Navigation",
    "authors": [
      "Haotian Zhou",
      "Xiaole Wang",
      "He Li",
      "Fusheng Sun",
      "Shengyu Guo",
      "Guolei Qi",
      "Jianghuan Xu",
      "Huijing Zhao"
    ],
    "abstract": "Navigating to a designated goal using visual information is a fundamental capability for intelligent robots. Most classical visual navigation methods are restricted to single-goal, single-modality, and closed set goal settings. To address the practical demands of multi-modal, open-vocabulary goal queries and multi-goal visual navigation, we propose LagMemo, a navigation system that leverages a language 3D Gaussian Splatting memory. During exploration, LagMemo constructs a unified 3D language memory. With incoming task goals, the system queries the memory, predicts candidate goal locations, and integrates a local perception-based verification mechanism to dynamically match and validate goals during navigation. For fair and rigorous evaluation, we curate GOAT-Core, a high-quality core split distilled from GOAT-Bench tailored to multi-modal open-vocabulary multi-goal visual navigation. Experimental results show that LagMemo's memory module enables effective multi-modal open-vocabulary goal localization, and that LagMemo outperforms state-of-the-art methods in multi-goal visual navigation. Project page: https://weekgoodday.github.io/lagmemo",
    "arxiv_url": "https://arxiv.org/abs/2510.24118v1",
    "pdf_url": "https://arxiv.org/pdf/2510.24118v1",
    "published_date": "2025-10-28",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "localization",
      "dynamic",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.24118v1",
      "pdf": "https://arxiv.org/pdf/2510.24118v1",
      "project": "https://weekgoodday.github.io/lagmemo"
    },
    "bibtex": ""
  },
  {
    "title": "A Survey on Collaborative SLAM with 3D Gaussian Splatting",
    "authors": [
      "Phuc Nguyen Xuan",
      "Thanh Nguyen Canh",
      "Huu-Hung Nguyen",
      "Nak Young Chong",
      "Xiem HoangVan"
    ],
    "abstract": "This survey comprehensively reviews the evolving field of multi-robot collaborative Simultaneous Localization and Mapping (SLAM) using 3D Gaussian Splatting (3DGS). As an explicit scene representation, 3DGS has enabled unprecedented real-time, high-fidelity rendering, ideal for robotics. However, its use in multi-robot systems introduces significant challenges in maintaining global consistency, managing communication, and fusing data from heterogeneous sources. We systematically categorize approaches by their architecture -- centralized, distributed -- and analyze core components like multi-agent consistency and alignment, communication-efficient, Gaussian representation, semantic distillation, fusion and pose optimization, and real-time scalability. In addition, a summary of critical datasets and evaluation metrics is provided to contextualize performance. Finally, we identify key open challenges and chart future research directions, including lifelong mapping, semantic association and mapping, multi-model for robustness, and bridging the Sim2Real gap.",
    "arxiv_url": "https://arxiv.org/abs/2510.23988v1",
    "pdf_url": "https://arxiv.org/pdf/2510.23988v1",
    "published_date": "2025-10-28",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "localization",
      "high-fidelity",
      "survey",
      "ar",
      "semantic",
      "mapping",
      "3d gaussian",
      "slam",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.23988v1",
      "pdf": "https://arxiv.org/pdf/2510.23988v1"
    },
    "bibtex": ""
  },
  {
    "title": "PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors",
    "authors": [
      "Xirui Jin",
      "Renbiao Jin",
      "Boying Li",
      "Danping Zou",
      "Wenxian Yu"
    ],
    "abstract": "Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an efficient representation for novel-view synthesis, achieving impressive visual quality. However, in scenes dominated by large and low-texture regions, common in indoor environments, the photometric loss used to optimize 3DGS yields ambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome this limitation, we introduce PlanarGS, a 3DGS-based framework tailored for indoor scene reconstruction. Specifically, we design a pipeline for Language-Prompted Planar Priors (LP3) that employs a pretrained vision-language segmentation model and refines its region proposals via cross-view fusion and inspection with geometric priors. 3D Gaussians in our framework are optimized with two additional terms: a planar prior supervision term that enforces planar consistency, and a geometric prior supervision term that steers the Gaussians toward the depth and normal cues. We have conducted extensive experiments on standard indoor benchmarks. The results show that PlanarGS reconstructs accurate and detailed 3D surfaces, consistently outperforming state-of-the-art methods by a large margin. Project page: https://planargs.github.io",
    "arxiv_url": "https://arxiv.org/abs/2510.23930v1",
    "pdf_url": "https://arxiv.org/pdf/2510.23930v1",
    "published_date": "2025-10-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "geometry",
      "segmentation",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.23930v1",
      "pdf": "https://arxiv.org/pdf/2510.23930v1",
      "project": "https://planargs.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "Adaptive Keyframe Selection for Scalable 3D Scene Reconstruction in Dynamic Environments",
    "authors": [
      "Raman Jha",
      "Yang Zhou",
      "Giuseppe Loianno"
    ],
    "abstract": "In this paper, we propose an adaptive keyframe selection method for improved 3D scene reconstruction in dynamic environments. The proposed method integrates two complementary modules: an error-based selection module utilizing photometric and structural similarity (SSIM) errors, and a momentum-based update module that dynamically adjusts keyframe selection thresholds according to scene motion dynamics. By dynamically curating the most informative frames, our approach addresses a key data bottleneck in real-time perception. This allows for the creation of high-quality 3D world representations from a compressed data stream, a critical step towards scalable robot learning and deployment in complex, dynamic environments. Experimental results demonstrate significant improvements over traditional static keyframe selection strategies, such as fixed temporal intervals or uniform frame skipping. These findings highlight a meaningful advancement toward adaptive perception systems that can dynamically respond to complex and evolving visual scenes. We evaluate our proposed adaptive keyframe selection module on two recent state-of-the-art 3D reconstruction networks, Spann3r and CUT3R, and observe consistent improvements in reconstruction quality across both frameworks. Furthermore, an extensive ablation study confirms the effectiveness of each individual component in our method, underlining their contribution to the overall performance gains.",
    "arxiv_url": "https://arxiv.org/abs/2510.23928v3",
    "pdf_url": "https://arxiv.org/pdf/2510.23928v3",
    "published_date": "2025-10-27",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "ar",
      "dynamic",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.23928v3",
      "pdf": "https://arxiv.org/pdf/2510.23928v3"
    },
    "bibtex": ""
  },
  {
    "title": "The Compressed 3D Lyman-Alpha Forest Bispectrum",
    "authors": [
      "Roger de Belsunce",
      "James M. Sullivan",
      "Patrick McDonald"
    ],
    "abstract": "Cosmological studies of the Lyman-Alpha (Lya) forest typically constrain parameters using two-point statistics. However, higher-order statistics, such as the three-point function (or its Fourier counterpart, the bispectrum) offer additional information and help break the degeneracy between the mean flux and power spectrum amplitude, albeit at a significant computational cost. To address this, we extend an existing highly informative compression of the bispectrum, the skew spectra, to the Lya forest. We derive the tree-level bispectrum of Lya forest fluctuations in the framework of effective field theory (EFT) directly in redshift space and validate our methodology on synthetic Lya forest data. We measure the anisotropic cross-spectra between the transmitted flux fraction and all quadratic operators arising in the bispectrum, yielding a set of 26 skew spectra. Using idealized 3D Gaussian smoothing (R=10 Mpc/h), we find good agreement (1-2 sigma level based on the statistical errors of the mocks) with the theoretical tree-level bispectrum prediction for monopole and quadrupole up to k <= 0.17 h/Mpc. To enable the cosmological analysis of Lya forest data from the currently observing Dark Energy Spectroscopic Instrument (DESI), where we cannot do 3D smoothing, we use a line-of-sight smoothing and introduce a new statistic, the shifted skew spectra. These probe non-squeezed bispectrum triangles and avoid locally applying quadratic operators to the field by displacing one copy of the field in the radial direction. Using a fixed displacement of 40 Mpc/h (and line-of-sight smoothing of 10 Mpc/h) yields a similar agreement with the theory prediction. For the special case of correlating the squared (and displaced) field with the original one, we analytically forward model the window function making this approach readily applicable to DESI data.",
    "arxiv_url": "https://arxiv.org/abs/2510.23597v1",
    "pdf_url": "https://arxiv.org/pdf/2510.23597v1",
    "published_date": "2025-10-27",
    "categories": [
      "astro-ph.CO",
      "astro-ph.GA",
      "hep-th"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "compression"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.23597v1",
      "pdf": "https://arxiv.org/pdf/2510.23597v1"
    },
    "bibtex": ""
  },
  {
    "title": "Explicit Memory through Online 3D Gaussian Splatting Improves Class-Agnostic Video Segmentation",
    "authors": [
      "Anthony Opipari",
      "Aravindhan K Krishnan",
      "Shreekant Gayaka",
      "Min Sun",
      "Cheng-Hao Kuo",
      "Arnie Sen",
      "Odest Chadwicke Jenkins"
    ],
    "abstract": "Remembering where object segments were predicted in the past is useful for improving the accuracy and consistency of class-agnostic video segmentation algorithms. Existing video segmentation algorithms typically use either no object-level memory (e.g. FastSAM) or they use implicit memories in the form of recurrent neural network features (e.g. SAM2). In this paper, we augment both types of segmentation models using an explicit 3D memory and show that the resulting models have more accurate and consistent predictions. For this, we develop an online 3D Gaussian Splatting (3DGS) technique to store predicted object-level segments generated throughout the duration of a video. Based on this 3DGS representation, a set of fusion techniques are developed, named FastSAM-Splat and SAM2-Splat, that use the explicit 3DGS memory to improve their respective foundation models' predictions. Ablation experiments are used to validate the proposed techniques' design and hyperparameter settings. Results from both real-world and simulated benchmarking experiments show that models which use explicit 3D memories result in more accurate and consistent predictions than those which use no memory or only implicit neural network memories. Project Page: https://topipari.com/projects/FastSAM-Splat/",
    "arxiv_url": "https://arxiv.org/abs/2510.23521v1",
    "pdf_url": "https://arxiv.org/pdf/2510.23521v1",
    "published_date": "2025-10-27",
    "categories": [
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "segmentation",
      "ar",
      "3d gaussian",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.23521v1",
      "pdf": "https://arxiv.org/pdf/2510.23521v1",
      "project": "https://topipari.com/projects/FastSAM-Splat"
    },
    "bibtex": ""
  },
  {
    "title": "VR-Drive: Viewpoint-Robust End-to-End Driving with Feed-Forward 3D Gaussian Splatting",
    "authors": [
      "Hoonhee Cho",
      "Jae-Young Kang",
      "Giwon Lee",
      "Hyemin Yang",
      "Heejun Park",
      "Seokwoo Jung",
      "Kuk-Jin Yoon"
    ],
    "abstract": "End-to-end autonomous driving (E2E-AD) has emerged as a promising paradigm that unifies perception, prediction, and planning into a holistic, data-driven framework. However, achieving robustness to varying camera viewpoints, a common real-world challenge due to diverse vehicle configurations, remains an open problem. In this work, we propose VR-Drive, a novel E2E-AD framework that addresses viewpoint generalization by jointly learning 3D scene reconstruction as an auxiliary task to enable planning-aware view synthesis. Unlike prior scene-specific synthesis approaches, VR-Drive adopts a feed-forward inference strategy that supports online training-time augmentation from sparse views without additional annotations. To further improve viewpoint consistency, we introduce a viewpoint-mixed memory bank that facilitates temporal interaction across multiple viewpoints and a viewpoint-consistent distillation strategy that transfers knowledge from original to synthesized views. Trained in a fully end-to-end manner, VR-Drive effectively mitigates synthesis-induced noise and improves planning under viewpoint shifts. In addition, we release a new benchmark dataset to evaluate E2E-AD performance under novel camera viewpoints, enabling comprehensive analysis. Our results demonstrate that VR-Drive is a scalable and robust solution for the real-world deployment of end-to-end autonomous driving systems.",
    "arxiv_url": "https://arxiv.org/abs/2510.23205v1",
    "pdf_url": "https://arxiv.org/pdf/2510.23205v1",
    "published_date": "2025-10-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "vr",
      "sparse view",
      "autonomous driving",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.23205v1",
      "pdf": "https://arxiv.org/pdf/2510.23205v1"
    },
    "bibtex": ""
  },
  {
    "title": "EndoWave: Rational-Wavelet 4D Gaussian Splatting for Endoscopic Reconstruction",
    "authors": [
      "Taoyu Wu",
      "Yiyi Miao",
      "Jiaxin Guo",
      "Ziyan Chen",
      "Sihang Zhao",
      "Zhuoxiao Li",
      "Zhe Tang",
      "Baoru Huang",
      "Limin Yu"
    ],
    "abstract": "In robot-assisted minimally invasive surgery, accurate 3D reconstruction from endoscopic video is vital for downstream tasks and improved outcomes. However, endoscopic scenarios present unique challenges, including photometric inconsistencies, non-rigid tissue motion, and view-dependent highlights. Most 3DGS-based methods that rely solely on appearance constraints for optimizing 3DGS are often insufficient in this context, as these dynamic visual artifacts can mislead the optimization process and lead to inaccurate reconstructions. To address these limitations, we present EndoWave, a unified spatio-temporal Gaussian Splatting framework by incorporating an optical flow-based geometric constraint and a multi-resolution rational wavelet supervision. First, we adopt a unified spatio-temporal Gaussian representation that directly optimizes primitives in a 4D domain. Second, we propose a geometric constraint derived from optical flow to enhance temporal coherence and effectively constrain the 3D structure of the scene. Third, we propose a multi-resolution rational orthogonal wavelet as a constraint, which can effectively separate the details of the endoscope and enhance the rendering performance. Extensive evaluations on two real surgical datasets, EndoNeRF and StereoMIS, demonstrate that our method EndoWave achieves state-of-the-art reconstruction quality and visual accuracy compared to the baseline method.",
    "arxiv_url": "https://arxiv.org/abs/2510.23087v1",
    "pdf_url": "https://arxiv.org/pdf/2510.23087v1",
    "published_date": "2025-10-27",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "motion",
      "3d reconstruction",
      "ar",
      "nerf",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.23087v1",
      "pdf": "https://arxiv.org/pdf/2510.23087v1"
    },
    "bibtex": ""
  },
  {
    "title": "Equivariant Neural Networks for General Linear Symmetries on Lie Algebras",
    "authors": [
      "Chankyo Kim",
      "Sicheng Zhao",
      "Minghan Zhu",
      "Tzu-Yuan Lin",
      "Maani Ghaffari"
    ],
    "abstract": "Many scientific and geometric problems exhibit general linear symmetries, yet most equivariant neural networks are built for compact groups or simple vector features, limiting their reuse on matrix-valued data such as covariances, inertias, or shape tensors. We introduce Reductive Lie Neurons (ReLNs), an exactly GL(n)-equivariant architecture that natively supports matrix-valued and Lie-algebraic features. ReLNs resolve a central stability issue for reductive Lie algebras by introducing a non-degenerate adjoint (conjugation)-invariant bilinear form, enabling principled nonlinear interactions and invariant feature construction in a single architecture that transfers across subgroups without redesign. We demonstrate ReLNs on algebraic tasks with sl(3) and sp(4) symmetries, Lorentz-equivariant particle physics, uncertainty-aware drone state estimation via joint velocity-covariance processing, learning from 3D Gaussian-splat representations, and EMLP double-pendulum benchmark spanning multiple symmetry groups. ReLNs consistently match or outperform strong equivariant and self-supervised baselines while using substantially fewer parameters and compute, improving the accuracy-efficiency trade-off and providing a practical, reusable backbone for learning with broad linear symmetries. Project page: https://reductive-lie-neuron.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2510.22984v2",
    "pdf_url": "https://arxiv.org/pdf/2510.22984v2",
    "published_date": "2025-10-27",
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "compact",
      "ar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.22984v2",
      "pdf": "https://arxiv.org/pdf/2510.22984v2",
      "project": "https://reductive-lie-neuron.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "Scaling Up Occupancy-centric Driving Scene Generation: Dataset and Method",
    "authors": [
      "Bohan Li",
      "Xin Jin",
      "Hu Zhu",
      "Hongsi Liu",
      "Ruikai Li",
      "Jiazhe Guo",
      "Kaiwen Cai",
      "Chao Ma",
      "Yueming Jin",
      "Hao Zhao",
      "Xiaokang Yang",
      "Wenjun Zeng"
    ],
    "abstract": "Driving scene generation is a critical domain for autonomous driving, enabling downstream applications, including perception and planning evaluation. Occupancy-centric methods have recently achieved state-of-the-art results by offering consistent conditioning across frames and modalities; however, their performance heavily depends on annotated occupancy data, which still remains scarce. To overcome this limitation, we curate Nuplan-Occ, the largest semantic occupancy dataset to date, constructed from the widely used Nuplan benchmark. Its scale and diversity facilitate not only large-scale generative modeling but also autonomous driving downstream applications. Based on this dataset, we develop a unified framework that jointly synthesizes high-quality semantic occupancy, multi-view videos, and LiDAR point clouds. Our approach incorporates a spatio-temporal disentangled architecture to support high-fidelity spatial expansion and temporal forecasting of 4D dynamic occupancy. To bridge modal gaps, we further propose two novel techniques: a Gaussian splatting-based sparse point map rendering strategy that enhances multi-view video generation, and a sensor-aware embedding strategy that explicitly models LiDAR sensor properties for realistic multi-LiDAR simulation. Extensive experiments demonstrate that our method achieves superior generation fidelity and scalability compared to existing approaches, and validates its practical value in downstream tasks. Repo: https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2",
    "arxiv_url": "https://arxiv.org/abs/2510.22973v1",
    "pdf_url": "https://arxiv.org/pdf/2510.22973v1",
    "published_date": "2025-10-27",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation",
    "keywords": [
      "4d",
      "dynamic",
      "high-fidelity",
      "autonomous driving",
      "ar",
      "semantic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.22973v1",
      "pdf": "https://arxiv.org/pdf/2510.22973v1",
      "github": "https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation"
    },
    "bibtex": ""
  },
  {
    "title": "Gen-LangSplat: Generalized Language Gaussian Splatting with Pre-Trained Feature Compression",
    "authors": [
      "Pranav Saxena"
    ],
    "abstract": "Modeling open-vocabulary language fields in 3D is essential for intuitive human-AI interaction and querying within physical environments. State-of-the-art approaches, such as LangSplat, leverage 3D Gaussian Splatting to efficiently construct these language fields, encoding features distilled from high-dimensional models like CLIP. However, this efficiency is currently offset by the requirement to train a scene-specific language autoencoder for feature compression, introducing a costly, per-scene optimization bottleneck that hinders deployment scalability. In this work, we introduce Gen-LangSplat, that eliminates this requirement by replacing the scene-wise autoencoder with a generalized autoencoder, pre-trained extensively on the large-scale ScanNet dataset. This architectural shift enables the use of a fixed, compact latent space for language features across any new scene without any scene-specific training. By removing this dependency, our entire language field construction process achieves a efficiency boost while delivering querying performance comparable to, or exceeding, the original LangSplat method. To validate our design choice, we perform a thorough ablation study empirically determining the optimal latent embedding dimension and quantifying representational fidelity using Mean Squared Error and cosine similarity between the original and reprojected 512-dimensional CLIP embeddings. Our results demonstrate that generalized embeddings can efficiently and accurately support open-vocabulary querying in novel 3D scenes, paving the way for scalable, real-time interactive 3D AI applications.",
    "arxiv_url": "https://arxiv.org/abs/2510.22930v1",
    "pdf_url": "https://arxiv.org/pdf/2510.22930v1",
    "published_date": "2025-10-27",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "compression",
      "ar",
      "human",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.22930v1",
      "pdf": "https://arxiv.org/pdf/2510.22930v1"
    },
    "bibtex": ""
  },
  {
    "title": "Region-Adaptive Learned Hierarchical Encoding for 3D Gaussian Splatting Data",
    "authors": [
      "Shashank N. Sridhara",
      "Birendra Kathariya",
      "Fangjun Pu",
      "Peng Yin",
      "Eduardo Pavez",
      "Antonio Ortega"
    ],
    "abstract": "We introduce Region-Adaptive Learned Hierarchical Encoding (RALHE) for 3D Gaussian Splatting (3DGS) data. While 3DGS has recently become popular for novel view synthesis, the size of trained models limits its deployment in bandwidth-constrained applications such as volumetric media streaming. To address this, we propose a learned hierarchical latent representation that builds upon the principles of \"overfitted\" learned image compression (e.g., Cool-Chic and C3) to efficiently encode 3DGS attributes. Unlike images, 3DGS data have irregular spatial distributions of Gaussians (geometry) and consist of multiple attributes (signals) defined on the irregular geometry. Our codec is designed to account for these differences between images and 3DGS. Specifically, we leverage the octree structure of the voxelized 3DGS geometry to obtain a hierarchical multi-resolution representation. Our approach overfits latents to each Gaussian attribute under a global rate constraint. These latents are decoded independently through a lightweight decoder network. To estimate the bitrate during training, we employ an autoregressive probability model that leverages octree-derived contexts from the 3D point structure. The multi-resolution latents, decoder, and autoregressive entropy coding networks are jointly optimized for each Gaussian attribute. Experiments demonstrate that the proposed RALHE compression framework achieves a rendering PSNR gain of up to 2dB at low bitrates (less than 1 MB) compared to the baseline 3DGS compression methods.",
    "arxiv_url": "https://arxiv.org/abs/2510.22812v1",
    "pdf_url": "https://arxiv.org/pdf/2510.22812v1",
    "published_date": "2025-10-26",
    "categories": [
      "eess.IV",
      "cs.MM"
    ],
    "github_url": "",
    "keywords": [
      "lightweight",
      "compression",
      "geometry",
      "ar",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.22812v1",
      "pdf": "https://arxiv.org/pdf/2510.22812v1"
    },
    "bibtex": ""
  },
  {
    "title": "Edge Collaborative Gaussian Splatting with Integrated Rendering and Communication",
    "authors": [
      "Yujie Wan",
      "Chenxuan Liu",
      "Shuai Wang",
      "Tong Zhang",
      "James Jianqiao Yu",
      "Kejiang Ye",
      "Dusit Niyato",
      "Chengzhong Xu"
    ],
    "abstract": "Gaussian splatting (GS) struggles with degraded rendering quality on low-cost devices. To address this issue, we present edge collaborative GS (ECO-GS), where each user can switch between a local small GS model to guarantee timeliness and a remote large GS model to guarantee fidelity. However, deciding how to engage the large GS model is nontrivial, due to the interdependency between rendering requirements and resource conditions. To this end, we propose integrated rendering and communication (IRAC), which jointly optimizes collaboration status (i.e., deciding whether to engage large GS) and edge power allocation (i.e., enabling remote rendering) under communication constraints across different users by minimizing a newly-derived GS switching function. Despite the nonconvexity of the problem, we propose an efficient penalty majorization minimization (PMM) algorithm to obtain the critical point solution. Furthermore, we develop an imitation learning optimization (ILO) algorithm, which reduces the computational time by over 100x compared to PMM. Experiments demonstrate the superiority of PMM and the real-time execution capability of ILO.",
    "arxiv_url": "https://arxiv.org/abs/2510.22718v2",
    "pdf_url": "https://arxiv.org/pdf/2510.22718v2",
    "published_date": "2025-10-26",
    "categories": [
      "cs.IT",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "efficient",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.22718v2",
      "pdf": "https://arxiv.org/pdf/2510.22718v2"
    },
    "bibtex": ""
  },
  {
    "title": "LVD-GS: Gaussian Splatting SLAM for Dynamic Scenes via Hierarchical Explicit-Implicit Representation Collaboration Rendering",
    "authors": [
      "Wenkai Zhu",
      "Xu Li",
      "Qimin Xu",
      "Benwu Wang",
      "Kun Wei",
      "Yiming Peng",
      "Zihang Wang"
    ],
    "abstract": "3D Gaussian Splatting SLAM has emerged as a widely used technique for high-fidelity mapping in spatial intelligence. However, existing methods often rely on a single representation scheme, which limits their performance in large-scale dynamic outdoor scenes and leads to cumulative pose errors and scale ambiguity. To address these challenges, we propose \\textbf{LVD-GS}, a novel LiDAR-Visual 3D Gaussian Splatting SLAM system. Motivated by the human chain-of-thought process for information seeking, we introduce a hierarchical collaborative representation module that facilitates mutual reinforcement for mapping optimization, effectively mitigating scale drift and enhancing reconstruction robustness. Furthermore, to effectively eliminate the influence of dynamic objects, we propose a joint dynamic modeling module that generates fine-grained dynamic masks by fusing open-world segmentation with implicit residual constraints, guided by uncertainty estimates from DINO-Depth features. Extensive evaluations on KITTI, nuScenes, and self-collected datasets demonstrate that our approach achieves state-of-the-art performance compared to existing methods.",
    "arxiv_url": "https://arxiv.org/abs/2510.22669v1",
    "pdf_url": "https://arxiv.org/pdf/2510.22669v1",
    "published_date": "2025-10-26",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "outdoor",
      "dynamic",
      "high-fidelity",
      "segmentation",
      "ar",
      "human",
      "3d gaussian",
      "slam",
      "mapping",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.22669v1",
      "pdf": "https://arxiv.org/pdf/2510.22669v1"
    },
    "bibtex": ""
  },
  {
    "title": "RoGER-SLAM: A Robust Gaussian Splatting SLAM System for Noisy and Low-light Environment Resilience",
    "authors": [
      "Huilin Yin",
      "Zhaolin Yang",
      "Linchuan Zhang",
      "Gerhard Rigoll",
      "Johannes Betz"
    ],
    "abstract": "The reliability of Simultaneous Localization and Mapping (SLAM) is severely constrained in environments where visual inputs suffer from noise and low illumination. Although recent 3D Gaussian Splatting (3DGS) based SLAM frameworks achieve high-fidelity mapping under clean conditions, they remain vulnerable to compounded degradations that degrade mapping and tracking performance. A key observation underlying our work is that the original 3DGS rendering pipeline inherently behaves as an implicit low-pass filter, attenuating high-frequency noise but also risking over-smoothing. Building on this insight, we propose RoGER-SLAM, a robust 3DGS SLAM system tailored for noise and low-light resilience. The framework integrates three innovations: a Structure-Preserving Robust Fusion (SP-RoFusion) mechanism that couples rendered appearance, depth, and edge cues; an adaptive tracking objective with residual balancing regularization; and a Contrastive Language-Image Pretraining (CLIP)-based enhancement module, selectively activated under compounded degradations to restore semantic and structural fidelity. Comprehensive experiments on Replica, TUM, and real-world sequences show that RoGER-SLAM consistently improves trajectory accuracy and reconstruction quality compared with other 3DGS-SLAM systems, especially under adverse imaging conditions.",
    "arxiv_url": "https://arxiv.org/abs/2510.22600v1",
    "pdf_url": "https://arxiv.org/pdf/2510.22600v1",
    "published_date": "2025-10-26",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "localization",
      "high-fidelity",
      "tracking",
      "ar",
      "semantic",
      "3d gaussian",
      "slam",
      "mapping",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.22600v1",
      "pdf": "https://arxiv.org/pdf/2510.22600v1"
    },
    "bibtex": ""
  },
  {
    "title": "DynaPose4D: High-Quality 4D Dynamic Content Generation via Pose Alignment Loss",
    "authors": [
      "Jing Yang",
      "Yufeng Yang"
    ],
    "abstract": "Recent advancements in 2D and 3D generative models have expanded the capabilities of computer vision. However, generating high-quality 4D dynamic content from a single static image remains a significant challenge. Traditional methods have limitations in modeling temporal dependencies and accurately capturing dynamic geometry changes, especially when considering variations in camera perspective. To address this issue, we propose DynaPose4D, an innovative solution that integrates 4D Gaussian Splatting (4DGS) techniques with Category-Agnostic Pose Estimation (CAPE) technology. This framework uses 3D Gaussian Splatting to construct a 3D model from single images, then predicts multi-view pose keypoints based on one-shot support from a chosen view, leveraging supervisory signals to enhance motion consistency. Experimental results show that DynaPose4D achieves excellent coherence, consistency, and fluidity in dynamic motion generation. These findings not only validate the efficacy of the DynaPose4D framework but also indicate its potential applications in the domains of computer vision and animation production.",
    "arxiv_url": "https://arxiv.org/abs/2510.22473v1",
    "pdf_url": "https://arxiv.org/pdf/2510.22473v1",
    "published_date": "2025-10-26",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "dynamic",
      "animation",
      "motion",
      "geometry",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.22473v1",
      "pdf": "https://arxiv.org/pdf/2510.22473v1"
    },
    "bibtex": ""
  },
  {
    "title": "DynamicTree: Interactive Real Tree Animation via Sparse Voxel Spectrum",
    "authors": [
      "Yaokun Li",
      "Lihe Ding",
      "Xiao Chen",
      "Guang Tan",
      "Tianfan Xue"
    ],
    "abstract": "Generating dynamic and interactive 3D trees has wide applications in virtual reality, games, and world simulation. However, existing methods still face various challenges in generating structurally consistent and realistic 4D motion for complex real trees. In this paper, we propose DynamicTree, the first framework that can generate long-term, interactive 3D motion for 3DGS reconstructions of real trees. Unlike prior optimization-based methods, our approach generates dynamics in a fast feed-forward manner. The key success of our approach is the use of a compact sparse voxel spectrum to represent the tree movement. Given a 3D tree from Gaussian Splatting reconstruction, our pipeline first generates mesh motion using the sparse voxel spectrum and then binds Gaussians to deform the mesh. Additionally, the proposed sparse voxel spectrum can also serve as a basis for fast modal analysis under external forces, allowing real-time interactive responses. To train our model, we also introduce 4DTree, the first large-scale synthetic 4D tree dataset containing 8,786 animated tree meshes with 100-frame motion sequences. Extensive experiments demonstrate that our method achieves realistic and responsive tree animations, significantly outperforming existing approaches in both visual quality and computational efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2510.22213v2",
    "pdf_url": "https://arxiv.org/pdf/2510.22213v2",
    "published_date": "2025-10-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "compact",
      "dynamic",
      "face",
      "animation",
      "gaussian splatting",
      "ar",
      "fast",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.22213v2",
      "pdf": "https://arxiv.org/pdf/2510.22213v2"
    },
    "bibtex": ""
  },
  {
    "title": "STG-Avatar: Animatable Human Avatars via Spacetime Gaussian",
    "authors": [
      "Guangan Jiang",
      "Tianzi Zhang",
      "Dong Li",
      "Zhenjun Zhao",
      "Haoang Li",
      "Mingrui Li",
      "Hongyu Wang"
    ],
    "abstract": "Realistic animatable human avatars from monocular videos are crucial for advancing human-robot interaction and enhancing immersive virtual experiences. While recent research on 3DGS-based human avatars has made progress, it still struggles with accurately representing detailed features of non-rigid objects (e.g., clothing deformations) and dynamic regions (e.g., rapidly moving limbs). To address these challenges, we present STG-Avatar, a 3DGS-based framework for high-fidelity animatable human avatar reconstruction. Specifically, our framework introduces a rigid-nonrigid coupled deformation framework that synergistically integrates Spacetime Gaussians (STG) with linear blend skinning (LBS). In this hybrid design, LBS enables real-time skeletal control by driving global pose transformations, while STG complements it through spacetime adaptive optimization of 3D Gaussians. Furthermore, we employ optical flow to identify high-dynamic regions and guide the adaptive densification of 3D Gaussians in these regions. Experimental results demonstrate that our method consistently outperforms state-of-the-art baselines in both reconstruction quality and operational efficiency, achieving superior quantitative metrics while retaining real-time rendering capabilities. Our code is available at https://github.com/jiangguangan/STG-Avatar",
    "arxiv_url": "https://arxiv.org/abs/2510.22140v1",
    "pdf_url": "https://arxiv.org/pdf/2510.22140v1",
    "published_date": "2025-10-25",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/jiangguangan/STG-Avatar",
    "keywords": [
      "dynamic",
      "high-fidelity",
      "deformation",
      "real-time rendering",
      "avatar",
      "ar",
      "human",
      "3d gaussian"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.22140v1",
      "pdf": "https://arxiv.org/pdf/2510.22140v1",
      "github": "https://github.com/jiangguangan/STG-Avatar"
    },
    "bibtex": ""
  },
  {
    "title": "Towards Physically Executable 3D Gaussian for Embodied Navigation",
    "authors": [
      "Bingchen Miao",
      "Rong Wei",
      "Zhiqi Ge",
      "Xiaoquan sun",
      "Shiqi Gao",
      "Jingzhe Zhu",
      "Renhan Wang",
      "Siliang Tang",
      "Jun Xiao",
      "Rui Tang",
      "Juncheng Li"
    ],
    "abstract": "3D Gaussian Splatting (3DGS), a 3D representation method with photorealistic real-time rendering capabilities, is regarded as an effective tool for narrowing the sim-to-real gap. However, it lacks fine-grained semantics and physical executability for Visual-Language Navigation (VLN). To address this, we propose SAGE-3D (Semantically and Physically Aligned Gaussian Environments for 3D Navigation), a new paradigm that upgrades 3DGS into an executable, semantically and physically aligned environment. It comprises two components: (1) Object-Centric Semantic Grounding, which adds object-level fine-grained annotations to 3DGS; and (2) Physics-Aware Execution Jointing, which embeds collision objects into 3DGS and constructs rich physical interfaces. We release InteriorGS, containing 1K object-annotated 3DGS indoor scene data, and introduce SAGE-Bench, the first 3DGS-based VLN benchmark with 2M VLN data. Experiments show that 3DGS scene data is more difficult to converge, while exhibiting strong generalizability, improving baseline performance by 31% on the VLN-CE Unseen task. Our data and code are available at: https://sage-3d.github.io.",
    "arxiv_url": "https://arxiv.org/abs/2510.21307v2",
    "pdf_url": "https://arxiv.org/pdf/2510.21307v2",
    "published_date": "2025-10-24",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "real-time rendering",
      "ar",
      "semantic",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.21307v2",
      "pdf": "https://arxiv.org/pdf/2510.21307v2",
      "project": "https://sage-3d.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation",
    "authors": [
      "Guangqi Jiang",
      "Haoran Chang",
      "Ri-Zhao Qiu",
      "Yutong Liang",
      "Mazeyu Ji",
      "Jiyue Zhu",
      "Zhao Dong",
      "Xueyan Zou",
      "Xiaolong Wang"
    ],
    "abstract": "This paper presents GSWorld, a robust, photo-realistic simulator for robotics manipulation that combines 3D Gaussian Splatting with physics engines. Our framework advocates \"closing the loop\" of developing manipulation policies with reproducible evaluation of policies learned from real-robot data and sim2real policy training without using real robots. To enable photo-realistic rendering of diverse scenes, we propose a new asset format, which we term GSDF (Gaussian Scene Description File), that infuses Gaussian-on-Mesh representation with robot URDF and other objects. With a streamlined reconstruction pipeline, we curate a database of GSDF that contains 3 robot embodiments for single-arm and bimanual manipulation, as well as more than 40 objects. Combining GSDF with physics engines, we demonstrate several immediate interesting applications: (1) learning zero-shot sim2real pixel-to-action manipulation policy with photo-realistic rendering, (2) automated high-quality DAgger data collection for adapting policies to deployment environments, (3) reproducible benchmarking of real-robot manipulation policies in simulation, (4) simulation data collection by virtual teleoperation, and (5) zero-shot sim2real visual reinforcement learning. Website: https://3dgsworld.github.io/.",
    "arxiv_url": "https://arxiv.org/abs/2510.20813v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20813v1",
    "published_date": "2025-10-23",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "robotics",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.20813v1",
      "pdf": "https://arxiv.org/pdf/2510.20813v1",
      "project": "https://3dgsworld.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "OnlineSplatter: Pose-Free Online 3D Reconstruction for Free-Moving Objects",
    "authors": [
      "Mark He Huang",
      "Lin Geng Foo",
      "Christian Theobalt",
      "Ying Sun",
      "De Wen Soh"
    ],
    "abstract": "Free-moving object reconstruction from monocular video remains challenging, particularly without reliable pose or depth cues and under arbitrary object motion. We introduce OnlineSplatter, a novel online feed-forward framework generating high-quality, object-centric 3D Gaussians directly from RGB frames without requiring camera pose, depth priors, or bundle optimization. Our approach anchors reconstruction using the first frame and progressively refines the object representation through a dense Gaussian primitive field, maintaining constant computational cost regardless of video sequence length. Our core contribution is a dual-key memory module combining latent appearance-geometry keys with explicit directional keys, robustly fusing current frame features with temporally aggregated object states. This design enables effective handling of free-moving objects via spatial-guided memory readout and an efficient sparsification mechanism, ensuring comprehensive yet compact object coverage. Evaluations on real-world datasets demonstrate that OnlineSplatter significantly outperforms state-of-the-art pose-free reconstruction baselines, consistently improving with more observations while maintaining constant memory and runtime.",
    "arxiv_url": "https://arxiv.org/abs/2510.20605v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20605v1",
    "published_date": "2025-10-23",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "3d reconstruction",
      "geometry",
      "ar",
      "3d gaussian",
      "efficient",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.20605v1",
      "pdf": "https://arxiv.org/pdf/2510.20605v1"
    },
    "bibtex": ""
  },
  {
    "title": "From Far and Near: Perceptual Evaluation of Crowd Representations Across Levels of Detail",
    "authors": [
      "Xiaohan Sun",
      "Carol O'Sullivan"
    ],
    "abstract": "In this paper, we investigate how users perceive the visual quality of crowd character representations at different levels of detail (LoD) and viewing distances. Each representation: geometric meshes, image-based impostors, Neural Radiance Fields (NeRFs), and 3D Gaussians, exhibits distinct trade-offs between visual fidelity and computational performance. Our qualitative and quantitative results provide insights to guide the design of perceptually optimized LoD strategies for crowd rendering.",
    "arxiv_url": "https://arxiv.org/abs/2510.20558v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20558v1",
    "published_date": "2025-10-23",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.HC"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "nerf"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.20558v1",
      "pdf": "https://arxiv.org/pdf/2510.20558v1"
    },
    "bibtex": ""
  },
  {
    "title": "Dino-Diffusion Modular Designs Bridge the Cross-Domain Gap in Autonomous Parking",
    "authors": [
      "Zixuan Wu",
      "Hengyuan Zhang",
      "Ting-Hsuan Chen",
      "Yuliang Guo",
      "David Paz",
      "Xinyu Huang",
      "Liu Ren"
    ],
    "abstract": "Parking is a critical pillar of driving safety. While recent end-to-end (E2E) approaches have achieved promising in-domain results, robustness under domain shifts (e.g., weather and lighting changes) remains a key challenge. Rather than relying on additional data, in this paper, we propose Dino-Diffusion Parking (DDP), a domain-agnostic autonomous parking pipeline that integrates visual foundation models with diffusion-based planning to enable generalized perception and robust motion planning under distribution shifts. We train our pipeline in CARLA at regular setting and transfer it to more adversarial settings in a zero-shot fashion. Our model consistently achieves a parking success rate above 90% across all tested out-of-distribution (OOD) scenarios, with ablation studies confirming that both the network architecture and algorithmic design significantly enhance cross-domain performance over existing baselines. Furthermore, testing in a 3D Gaussian splatting (3DGS) environment reconstructed from a real-world parking lot demonstrates promising sim-to-real transfer.",
    "arxiv_url": "https://arxiv.org/abs/2510.20335v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20335v1",
    "published_date": "2025-10-23",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "gaussian splatting",
      "ar",
      "3d gaussian",
      "lighting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.20335v1",
      "pdf": "https://arxiv.org/pdf/2510.20335v1"
    },
    "bibtex": ""
  },
  {
    "title": "COS3D: Collaborative Open-Vocabulary 3D Segmentation",
    "authors": [
      "Runsong Zhu",
      "Ka-Hei Hui",
      "Zhengzhe Liu",
      "Qianyi Wu",
      "Weiliang Tang",
      "Shi Qiu",
      "Pheng-Ann Heng",
      "Chi-Wing Fu"
    ],
    "abstract": "Open-vocabulary 3D segmentation is a fundamental yet challenging task, requiring a mutual understanding of both segmentation and language. However, existing Gaussian-splatting-based methods rely either on a single 3D language field, leading to inferior segmentation, or on pre-computed class-agnostic segmentations, suffering from error accumulation. To address these limitations, we present COS3D, a new collaborative prompt-segmentation framework that contributes to effectively integrating complementary language and segmentation cues throughout its entire pipeline. We first introduce the new concept of collaborative field, comprising an instance field and a language field, as the cornerstone for collaboration. During training, to effectively construct the collaborative field, our key idea is to capture the intrinsic relationship between the instance field and language field, through a novel instance-to-language feature mapping and designing an efficient two-stage training strategy. During inference, to bridge distinct characteristics of the two fields, we further design an adaptive language-to-instance prompt refinement, promoting high-quality prompt-segmentation inference. Extensive experiments not only demonstrate COS3D's leading performance over existing methods on two widely-used benchmarks but also show its high potential to various applications,~\\ie, novel image-based 3D segmentation, hierarchical segmentation, and robotics. The code is publicly available at \\href{https://github.com/Runsong123/COS3D}{https://github.com/Runsong123/COS3D}.",
    "arxiv_url": "https://arxiv.org/abs/2510.20238v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20238v1",
    "published_date": "2025-10-23",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/Runsong123/COS3D",
    "keywords": [
      "robotics",
      "segmentation",
      "ar",
      "mapping",
      "efficient",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.20238v1",
      "pdf": "https://arxiv.org/pdf/2510.20238v1",
      "github": "https://github.com/Runsong123/COS3D"
    },
    "bibtex": ""
  },
  {
    "title": "Extreme Views: 3DGS Filter for Novel View Synthesis from Out-of-Distribution Camera Poses",
    "authors": [
      "Damian Bowness",
      "Charalambos Poullis"
    ],
    "abstract": "When viewing a 3D Gaussian Splatting (3DGS) model from camera positions significantly outside the training data distribution, substantial visual noise commonly occurs. These artifacts result from the lack of training data in these extrapolated regions, leading to uncertain density, color, and geometry predictions from the model.   To address this issue, we propose a novel real-time render-aware filtering method. Our approach leverages sensitivity scores derived from intermediate gradients, explicitly targeting instabilities caused by anisotropic orientations rather than isotropic variance. This filtering method directly addresses the core issue of generative uncertainty, allowing 3D reconstruction systems to maintain high visual fidelity even when users freely navigate outside the original training viewpoints.   Experimental evaluation demonstrates that our method substantially improves visual quality, realism, and consistency compared to existing Neural Radiance Field (NeRF)-based approaches such as BayesRays. Critically, our filter seamlessly integrates into existing 3DGS rendering pipelines in real-time, unlike methods that require extensive post-hoc retraining or fine-tuning.   Code and results at https://damian-bowness.github.io/EV3DGS",
    "arxiv_url": "https://arxiv.org/abs/2510.20027v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20027v1",
    "published_date": "2025-10-22",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "geometry",
      "ar",
      "nerf",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.20027v1",
      "pdf": "https://arxiv.org/pdf/2510.20027v1",
      "project": "https://damian-bowness.github.io/EV3DGS"
    },
    "bibtex": ""
  },
  {
    "title": "VGD: Visual Geometry Gaussian Splatting for Feed-Forward Surround-view Driving Reconstruction",
    "authors": [
      "Junhong Lin",
      "Kangli Wang",
      "Shunzhou Wang",
      "Songlin Fan",
      "Ge Li",
      "Wei Gao"
    ],
    "abstract": "Feed-forward surround-view autonomous driving scene reconstruction offers fast, generalizable inference ability, which faces the core challenge of ensuring generalization while elevating novel view quality. Due to the surround-view with minimal overlap regions, existing methods typically fail to ensure geometric consistency and reconstruction quality for novel views. To tackle this tension, we claim that geometric information must be learned explicitly, and the resulting features should be leveraged to guide the elevating of semantic quality in novel views. In this paper, we introduce \\textbf{Visual Gaussian Driving (VGD)}, a novel feed-forward end-to-end learning framework designed to address this challenge. To achieve generalizable geometric estimation, we design a lightweight variant of the VGGT architecture to efficiently distill its geometric priors from the pre-trained VGGT to the geometry branch. Furthermore, we design a Gaussian Head that fuses multi-scale geometry tokens to predict Gaussian parameters for novel view rendering, which shares the same patch backbone as the geometry branch. Finally, we integrate multi-scale features from both geometry and Gaussian head branches to jointly supervise a semantic refinement model, optimizing rendering quality through feature-consistent learning. Experiments on nuScenes demonstrate that our approach significantly outperforms state-of-the-art methods in both objective metrics and subjective quality under various settings, which validates VGD's scalability and high-fidelity surround-view reconstruction.",
    "arxiv_url": "https://arxiv.org/abs/2510.19578v1",
    "pdf_url": "https://arxiv.org/pdf/2510.19578v1",
    "published_date": "2025-10-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "lightweight",
      "face",
      "geometry",
      "autonomous driving",
      "ar",
      "semantic",
      "efficient",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.19578v1",
      "pdf": "https://arxiv.org/pdf/2510.19578v1"
    },
    "bibtex": ""
  },
  {
    "title": "Advances in 4D Representation: Geometry, Motion, and Interaction",
    "authors": [
      "Mingrui Zhao",
      "Sauradip Nag",
      "Kai Wang",
      "Aditya Vora",
      "Guangda Ji",
      "Peter Chun",
      "Ali Mahdavi-Amiri",
      "Hao Zhang"
    ],
    "abstract": "We present a survey on 4D generation and reconstruction, a fast-evolving subfield of computer graphics whose developments have been propelled by recent advances in neural fields, geometric and motion deep learning, as well 3D generative artificial intelligence (GenAI). While our survey is not the first of its kind, we build our coverage of the domain from a unique and distinctive perspective of 4D representations\\/}, to model 3D geometry evolving over time while exhibiting motion and interaction. Specifically, instead of offering an exhaustive enumeration of many works, we take a more selective approach by focusing on representative works to highlight both the desirable properties and ensuing challenges of each representation under different computation, application, and data scenarios. The main take-away message we aim to convey to the readers is on how to select and then customize the appropriate 4D representations for their tasks. Organizationally, we separate the 4D representations based on three key pillars: geometry, motion, and interaction. Our discourse will not only encompass the most popular representations of today, such as neural radiance fields (NeRFs) and 3D Gaussian Splatting (3DGS), but also bring attention to relatively under-explored representations in the 4D context, such as structured models and long-range motions. Throughout our survey, we will reprise the role of large language models (LLMs) and video foundational models (VFMs) in a variety of 4D applications, while steering our discussion towards their current limitations and how they can be addressed. We also provide a dedicated coverage on what 4D datasets are currently available, as well as what is lacking, in driving the subfield forward. Project page:https://mingrui-zhao.github.io/4DRep-GMI/",
    "arxiv_url": "https://arxiv.org/abs/2510.19255v1",
    "pdf_url": "https://arxiv.org/pdf/2510.19255v1",
    "published_date": "2025-10-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "survey",
      "motion",
      "geometry",
      "ar",
      "nerf",
      "3d gaussian",
      "fast",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.19255v1",
      "pdf": "https://arxiv.org/pdf/2510.19255v1",
      "project": "https://mingrui-zhao.github.io/4DRep-GMI"
    },
    "bibtex": ""
  },
  {
    "title": "MoE-GS: Mixture of Experts for Dynamic Gaussian Splatting",
    "authors": [
      "In-Hwan Jin",
      "Hyeongju Mun",
      "Joonsoo Kim",
      "Kugjin Yun",
      "Kyeongbo Kong"
    ],
    "abstract": "Recent advances in dynamic scene reconstruction have significantly benefited from 3D Gaussian Splatting, yet existing methods show inconsistent performance across diverse scenes, indicating no single approach effectively handles all dynamic challenges. To overcome these limitations, we propose Mixture of Experts for Dynamic Gaussian Splatting (MoE-GS), a unified framework integrating multiple specialized experts via a novel Volume-aware Pixel Router. Our router adaptively blends expert outputs by projecting volumetric Gaussian-level weights into pixel space through differentiable weight splatting, ensuring spatially and temporally coherent results. Although MoE-GS improves rendering quality, the increased model capacity and reduced FPS are inherent to the MoE architecture. To mitigate this, we explore two complementary directions: (1) single-pass multi-expert rendering and gate-aware Gaussian pruning, which improve efficiency within the MoE framework, and (2) a distillation strategy that transfers MoE performance to individual experts, enabling lightweight deployment without architectural changes. To the best of our knowledge, MoE-GS is the first approach incorporating Mixture-of-Experts techniques into dynamic Gaussian splatting. Extensive experiments on the N3V and Technicolor datasets demonstrate that MoE-GS consistently outperforms state-of-the-art methods with improved efficiency. Video demonstrations are available at https://anonymous.4open.science/w/MoE-GS-68BA/.",
    "arxiv_url": "https://arxiv.org/abs/2510.19210v1",
    "pdf_url": "https://arxiv.org/pdf/2510.19210v1",
    "published_date": "2025-10-22",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "lightweight",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.19210v1",
      "pdf": "https://arxiv.org/pdf/2510.19210v1",
      "project": "https://anonymous.4open.science/w/MoE-GS-68BA"
    },
    "bibtex": ""
  },
  {
    "title": "GRASPLAT: Enabling dexterous grasping through novel view synthesis",
    "authors": [
      "Matteo Bortolon",
      "Nuno Ferreira Duarte",
      "Plinio Moreno",
      "Fabio Poiesi",
      "José Santos-Victor",
      "Alessio Del Bue"
    ],
    "abstract": "Achieving dexterous robotic grasping with multi-fingered hands remains a significant challenge. While existing methods rely on complete 3D scans to predict grasp poses, these approaches face limitations due to the difficulty of acquiring high-quality 3D data in real-world scenarios. In this paper, we introduce GRASPLAT, a novel grasping framework that leverages consistent 3D information while being trained solely on RGB images. Our key insight is that by synthesizing physically plausible images of a hand grasping an object, we can regress the corresponding hand joints for a successful grasp. To achieve this, we utilize 3D Gaussian Splatting to generate high-fidelity novel views of real hand-object interactions, enabling end-to-end training with RGB data. Unlike prior methods, our approach incorporates a photometric loss that refines grasp predictions by minimizing discrepancies between rendered and real images. We conduct extensive experiments on both synthetic and real-world grasping datasets, demonstrating that GRASPLAT improves grasp success rates up to 36.9% over existing image-based methods. Project page: https://mbortolon97.github.io/grasplat/",
    "arxiv_url": "https://arxiv.org/abs/2510.19200v1",
    "pdf_url": "https://arxiv.org/pdf/2510.19200v1",
    "published_date": "2025-10-22",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.19200v1",
      "pdf": "https://arxiv.org/pdf/2510.19200v1",
      "project": "https://mbortolon97.github.io/grasplat"
    },
    "bibtex": ""
  },
  {
    "title": "Moving Light Adaptive Colonoscopy Reconstruction via Illumination-Attenuation-Aware 3D Gaussian Splatting",
    "authors": [
      "Hao Wang",
      "Ying Zhou",
      "Haoyu Zhao",
      "Rui Wang",
      "Qiang Hu",
      "Xing Zhang",
      "Qiang Li",
      "Zhiwei Wang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a pivotal technique for real-time view synthesis in colonoscopy, enabling critical applications such as virtual colonoscopy and lesion tracking. However, the vanilla 3DGS assumes static illumination and that observed appearance depends solely on viewing angle, which causes incompatibility with the photometric variations in colonoscopic scenes induced by dynamic light source/camera. This mismatch forces most 3DGS methods to introduce structure-violating vaporous Gaussian blobs between the camera and tissues to compensate for illumination attenuation, ultimately degrading the quality of 3D reconstructions. Previous works only consider the illumination attenuation caused by light distance, ignoring the physical characters of light source and camera. In this paper, we propose ColIAGS, an improved 3DGS framework tailored for colonoscopy. To mimic realistic appearance under varying illumination, we introduce an Improved Appearance Modeling with two types of illumination attenuation factors, which enables Gaussians to adapt to photometric variations while preserving geometry accuracy. To ensure the geometry approximation condition of appearance modeling, we propose an Improved Geometry Modeling using high-dimensional view embedding to enhance Gaussian geometry attribute prediction. Furthermore, another cosine embedding input is leveraged to generate illumination attenuation solutions in an implicit manner. Comprehensive experimental results on standard benchmarks demonstrate that our proposed ColIAGS achieves the dual capabilities of novel view synthesis and accurate geometric reconstruction. It notably outperforms other state-of-the-art methods by achieving superior rendering fidelity while significantly reducing Depth MSE. Code will be available.",
    "arxiv_url": "https://arxiv.org/abs/2510.18739v1",
    "pdf_url": "https://arxiv.org/pdf/2510.18739v1",
    "published_date": "2025-10-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "dynamic",
      "tracking",
      "3d reconstruction",
      "geometry",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.18739v1",
      "pdf": "https://arxiv.org/pdf/2510.18739v1"
    },
    "bibtex": ""
  },
  {
    "title": "Re-Activating Frozen Primitives for 3D Gaussian Splatting",
    "authors": [
      "Yuxin Cheng",
      "Binxiao Huang",
      "Wenyong Zhou",
      "Taiqiang Wu",
      "Zhengwu Liu",
      "Graziano Chesi",
      "Ngai Wong"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) achieves real-time photorealistic novel view synthesis, yet struggles with complex scenes due to over-reconstruction artifacts, manifesting as local blurring and needle-shape distortions. While recent approaches attribute these issues to insufficient splitting of large-scale Gaussians, we identify two fundamental limitations: gradient magnitude dilution during densification and the primitive frozen phenomenon, where essential Gaussian densification is inhibited in complex regions while suboptimally scaled Gaussians become trapped in local optima. To address these challenges, we introduce ReAct-GS, a method founded on the principle of re-activation. Our approach features: (1) an importance-aware densification criterion incorporating $α$-blending weights from multiple viewpoints to re-activate stalled primitive growth in complex regions, and (2) a re-activation mechanism that revitalizes frozen primitives through adaptive parameter perturbations. Comprehensive experiments across diverse real-world datasets demonstrate that ReAct-GS effectively eliminates over-reconstruction artifacts and achieves state-of-the-art performance on standard novel view synthesis metrics while preserving intricate geometric details. Additionally, our re-activation mechanism yields consistent improvements when integrated with other 3D-GS variants such as Pixel-GS, demonstrating its broad applicability.",
    "arxiv_url": "https://arxiv.org/abs/2510.19653v1",
    "pdf_url": "https://arxiv.org/pdf/2510.19653v1",
    "published_date": "2025-10-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.19653v1",
      "pdf": "https://arxiv.org/pdf/2510.19653v1"
    },
    "bibtex": ""
  },
  {
    "title": "Mono4DGS-HDR: High Dynamic Range 4D Gaussian Splatting from Alternating-exposure Monocular Videos",
    "authors": [
      "Jinfeng Liu",
      "Lingtong Kong",
      "Mi Zhou",
      "Jinwen Chen",
      "Dan Xu"
    ],
    "abstract": "We introduce Mono4DGS-HDR, the first system for reconstructing renderable 4D high dynamic range (HDR) scenes from unposed monocular low dynamic range (LDR) videos captured with alternating exposures. To tackle such a challenging problem, we present a unified framework with two-stage optimization approach based on Gaussian Splatting. The first stage learns a video HDR Gaussian representation in orthographic camera coordinate space, eliminating the need for camera poses and enabling robust initial HDR video reconstruction. The second stage transforms video Gaussians into world space and jointly refines the world Gaussians with camera poses. Furthermore, we propose a temporal luminance regularization strategy to enhance the temporal consistency of the HDR appearance. Since our task has not been studied before, we construct a new evaluation benchmark using publicly available datasets for HDR video reconstruction. Extensive experiments demonstrate that Mono4DGS-HDR significantly outperforms alternative solutions adapted from state-of-the-art methods in both rendering quality and speed.",
    "arxiv_url": "https://arxiv.org/abs/2510.18489v1",
    "pdf_url": "https://arxiv.org/pdf/2510.18489v1",
    "published_date": "2025-10-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "4d",
      "ar",
      "dynamic",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.18489v1",
      "pdf": "https://arxiv.org/pdf/2510.18489v1"
    },
    "bibtex": ""
  },
  {
    "title": "OpenInsGaussian: Open-vocabulary Instance Gaussian Segmentation with Context-aware Cross-view Fusion",
    "authors": [
      "Tianyu Huang",
      "Runnan Chen",
      "Dongting Hu",
      "Fengming Huang",
      "Mingming Gong",
      "Tongliang Liu"
    ],
    "abstract": "Understanding 3D scenes is pivotal for autonomous driving, robotics, and augmented reality. Recent semantic Gaussian Splatting approaches leverage large-scale 2D vision models to project 2D semantic features onto 3D scenes. However, they suffer from two major limitations: (1) insufficient contextual cues for individual masks during preprocessing and (2) inconsistencies and missing details when fusing multi-view features from these 2D models. In this paper, we introduce \\textbf{OpenInsGaussian}, an \\textbf{Open}-vocabulary \\textbf{Ins}tance \\textbf{Gaussian} segmentation framework with Context-aware Cross-view Fusion. Our method consists of two modules: Context-Aware Feature Extraction, which augments each mask with rich semantic context, and Attention-Driven Feature Aggregation, which selectively fuses multi-view features to mitigate alignment errors and incompleteness. Through extensive experiments on benchmark datasets, OpenInsGaussian achieves state-of-the-art results in open-vocabulary 3D Gaussian segmentation, outperforming existing baselines by a large margin. These findings underscore the robustness and generality of our proposed approach, marking a significant step forward in 3D scene understanding and its practical deployment across diverse real-world scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2510.18253v1",
    "pdf_url": "https://arxiv.org/pdf/2510.18253v1",
    "published_date": "2025-10-21",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "segmentation",
      "gaussian splatting",
      "semantic",
      "autonomous driving",
      "ar",
      "3d gaussian",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.18253v1",
      "pdf": "https://arxiv.org/pdf/2510.18253v1"
    },
    "bibtex": ""
  },
  {
    "title": "From Volume Rendering to 3D Gaussian Splatting: Theory and Applications",
    "authors": [
      "Vitor Pereira Matias",
      "Daniel Perazzo",
      "Vinicius Silva",
      "Alberto Raposo",
      "Luiz Velho",
      "Afonso Paiva",
      "Tiago Novello"
    ],
    "abstract": "The problem of 3D reconstruction from posed images is undergoing a fundamental transformation, driven by continuous advances in 3D Gaussian Splatting (3DGS). By modeling scenes explicitly as collections of 3D Gaussians, 3DGS enables efficient rasterization through volumetric splatting, offering thus a seamless integration with common graphics pipelines. Despite its real-time rendering capabilities for novel view synthesis, 3DGS suffers from a high memory footprint, the tendency to bake lighting effects directly into its representation, and limited support for secondary-ray effects. This tutorial provides a concise yet comprehensive overview of the 3DGS pipeline, starting from its splatting formulation and then exploring the main efforts in addressing its limitations. Finally, we survey a range of applications that leverage 3DGS for surface reconstruction, avatar modeling, animation, and content generation-highlighting its efficient rendering and suitability for feed-forward pipelines.",
    "arxiv_url": "https://arxiv.org/abs/2510.18101v1",
    "pdf_url": "https://arxiv.org/pdf/2510.18101v1",
    "published_date": "2025-10-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "face",
      "survey",
      "real-time rendering",
      "3d reconstruction",
      "animation",
      "avatar",
      "efficient rendering",
      "efficient",
      "ar",
      "3d gaussian",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.18101v1",
      "pdf": "https://arxiv.org/pdf/2510.18101v1"
    },
    "bibtex": ""
  },
  {
    "title": "HouseTour: A Virtual Real Estate A(I)gent",
    "authors": [
      "Ata Çelen",
      "Marc Pollefeys",
      "Daniel Barath",
      "Iro Armeni"
    ],
    "abstract": "We introduce HouseTour, a method for spatially-aware 3D camera trajectory and natural language summary generation from a collection of images depicting an existing 3D space. Unlike existing vision-language models (VLMs), which struggle with geometric reasoning, our approach generates smooth video trajectories via a diffusion process constrained by known camera poses and integrates this information into the VLM for 3D-grounded descriptions. We synthesize the final video using 3D Gaussian splatting to render novel views along the trajectory. To support this task, we present the HouseTour dataset, which includes over 1,200 house-tour videos with camera poses, 3D reconstructions, and real estate descriptions. Experiments demonstrate that incorporating 3D camera trajectories into the text generation process improves performance over methods handling each task independently. We evaluate both individual and end-to-end performance, introducing a new joint metric. Our work enables automated, professional-quality video creation for real estate and touristic applications without requiring specialized expertise or equipment.",
    "arxiv_url": "https://arxiv.org/abs/2510.18054v1",
    "pdf_url": "https://arxiv.org/pdf/2510.18054v1",
    "published_date": "2025-10-20",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.18054v1",
      "pdf": "https://arxiv.org/pdf/2510.18054v1"
    },
    "bibtex": ""
  },
  {
    "title": "Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats",
    "authors": [
      "Simeon Adebola",
      "Chung Min Kim",
      "Justin Kerr",
      "Shuangyu Xie",
      "Prithvi Akella",
      "Jose Luis Susa Rincon",
      "Eugen Solowjow",
      "Ken Goldberg"
    ],
    "abstract": "Commercial plant phenotyping systems using fixed cameras cannot perceive many plant details due to leaf occlusion. In this paper, we present Botany-Bot, a system for building detailed \"annotated digital twins\" of living plants using two stereo cameras, a digital turntable inside a lightbox, an industrial robot arm, and 3D segmentated Gaussian Splat models. We also present robot algorithms for manipulating leaves to take high-resolution indexable images of occluded details such as stem buds and the underside/topside of leaves. Results from experiments suggest that Botany-Bot can segment leaves with 90.8% accuracy, detect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and take detailed overside/underside images with 77.3% accuracy. Code, videos, and datasets are available at https://berkeleyautomation.github.io/Botany-Bot/.",
    "arxiv_url": "https://arxiv.org/abs/2510.17783v1",
    "pdf_url": "https://arxiv.org/pdf/2510.17783v1",
    "published_date": "2025-10-20",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "ar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.17783v1",
      "pdf": "https://arxiv.org/pdf/2510.17783v1",
      "project": "https://berkeleyautomation.github.io/Botany-Bot"
    },
    "bibtex": ""
  },
  {
    "title": "RaindropGS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions",
    "authors": [
      "Zhiqiang Teng",
      "Tingting Chen",
      "Beibei Lin",
      "Zifeng Yuan",
      "Xuanyi Li",
      "Xuanyu Zhang",
      "Shunli Zhang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe occlusions and optical distortions caused by raindrop contamination on the camera lens, substantially degrading reconstruction quality. Existing benchmarks typically evaluate 3DGS using synthetic raindrop images with known camera poses (constrained images), assuming ideal conditions. However, in real-world scenarios, raindrops often interfere with accurate camera pose estimation and point cloud initialization. Moreover, a significant domain gap between synthetic and real raindrops further impairs generalization. To tackle these issues, we introduce RaindropGS, a comprehensive benchmark designed to evaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images to clear 3DGS reconstructions. Specifically, the whole benchmark pipeline consists of three parts: data preparation, data processing, and raindrop-aware 3DGS evaluation, including types of raindrop interference, camera pose estimation and point cloud initialization, single image rain removal comparison, and 3D Gaussian training comparison. First, we collect a real-world raindrop reconstruction dataset, in which each scene contains three aligned image sets: raindrop-focused, background-focused, and rain-free ground truth, enabling a comprehensive evaluation of reconstruction quality under different focus conditions. Through comprehensive experiments and analyses, we reveal critical insights into the performance limitations of existing 3DGS methods on unconstrained raindrop images and the varying impact of different pipeline components: the impact of camera focus position on 3DGS reconstruction performance, and the interference caused by inaccurate pose and point cloud initialization on reconstruction. These insights establish clear directions for developing more robust 3DGS methods under raindrop conditions.",
    "arxiv_url": "https://arxiv.org/abs/2510.17719v2",
    "pdf_url": "https://arxiv.org/pdf/2510.17719v2",
    "published_date": "2025-10-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.17719v2",
      "pdf": "https://arxiv.org/pdf/2510.17719v2"
    },
    "bibtex": ""
  },
  {
    "title": "Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS",
    "authors": [
      "Feng Zhou",
      "Wenkai Guo",
      "Pu Cao",
      "Zhicheng Zhang",
      "Jianqin Yin"
    ],
    "abstract": "Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training views, leading to artifacts like blurring in novel view rendering. Prior work addresses it either by enhancing the initialization (\\emph{i.e.}, the point cloud from Structure-from-Motion (SfM)) or by adding training-time constraints (regularization) to the 3DGS optimization. Yet our controlled ablations reveal that initialization is the decisive factor: it determines the attainable performance band in sparse-view 3DGS, while training-time constraints yield only modest within-band improvements at extra cost. Given initialization's primacy, we focus our design there. Although SfM performs poorly under sparse views due to its reliance on feature matching, it still provides reliable seed points. Thus, building on SfM, our effort aims to supplement the regions it fails to cover as comprehensively as possible. Specifically, we design: (i) frequency-aware SfM that improves low-texture coverage via low-frequency view augmentation and relaxed multi-view correspondences; (ii) 3DGS self-initialization that lifts photometric supervision into additional points, compensating SfM-sparse regions with learned Gaussian centers; and (iii) point-cloud regularization that enforces multi-view consistency and uniform spatial coverage through simple geometric/visibility priors, yielding a clean and reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate consistent gains in sparse-view settings, establishing our approach as a stronger initialization strategy. Code is available at https://github.com/zss171999645/ItG-GS.",
    "arxiv_url": "https://arxiv.org/abs/2510.17479v1",
    "pdf_url": "https://arxiv.org/pdf/2510.17479v1",
    "published_date": "2025-10-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "https://github.com/zss171999645/ItG-GS",
    "keywords": [
      "sparse-view",
      "sparse view",
      "gaussian splatting",
      "ar",
      "nerf",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.17479v1",
      "pdf": "https://arxiv.org/pdf/2510.17479v1",
      "github": "https://github.com/zss171999645/ItG-GS"
    },
    "bibtex": ""
  },
  {
    "title": "GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation",
    "authors": [
      "Ruitong Gan",
      "Junran Peng",
      "Yang Liu",
      "Chuanchen Luo",
      "Qing Li",
      "Zhaoxiang Zhang"
    ],
    "abstract": "Planes are fundamental primitives of 3D sences, especially in man-made environments such as indoor spaces and urban streets. Representing these planes in a structured and parameterized format facilitates scene editing and physical simulations in downstream applications. Recently, Gaussian Splatting (GS) has demonstrated remarkable effectiveness in the Novel View Synthesis task, with extensions showing great potential in accurate surface reconstruction. However, even state-of-the-art GS representations often struggle to reconstruct planar regions with sufficient smoothness and precision. To address this issue, we propose GSPlane, which recovers accurate geometry and produces clean and well-structured mesh connectivity for plane regions in the reconstructed scene. By leveraging off-the-shelf segmentation and normal prediction models, GSPlane extracts robust planar priors to establish structured representations for planar Gaussian coordinates, which help guide the training process by enforcing geometric consistency. To further enhance training robustness, a Dynamic Gaussian Re-classifier is introduced to adaptively reclassify planar Gaussians with persistently high gradients as non-planar, ensuring more reliable optimization. Furthermore, we utilize the optimized planar priors to refine the mesh layouts, significantly improving topological structure while reducing the number of vertices and faces. We also explore applications of the structured planar representation, which enable decoupling and flexible manipulation of objects on supportive planes. Extensive experiments demonstrate that, with no sacrifice in rendering quality, the introduction of planar priors significantly improves the geometric accuracy of the extracted meshes across various baselines.",
    "arxiv_url": "https://arxiv.org/abs/2510.17095v1",
    "pdf_url": "https://arxiv.org/pdf/2510.17095v1",
    "published_date": "2025-10-20",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "face",
      "geometry",
      "segmentation",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.17095v1",
      "pdf": "https://arxiv.org/pdf/2510.17095v1"
    },
    "bibtex": ""
  },
  {
    "title": "2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting",
    "authors": [
      "Haofan Ren",
      "Qingsong Yan",
      "Ming Lu",
      "Rongfeng Lu",
      "Zunjie Zhu"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced neural fields, as it enables high-fidelity rendering with impressive visual quality. However, 3DGS has difficulty accurately representing surfaces. In contrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian disks. Despite advancements in geometric fidelity, rendering quality remains compromised, highlighting the challenge of achieving both high-quality rendering and precise geometric structures. This indicates that optimizing both geometric and rendering quality in a single training stage is currently unfeasible. To overcome this limitation, we present 2DGS-R, a new method that uses a hierarchical training approach to improve rendering quality while maintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians with the normal consistency regularization. Then 2DGS-R selects the 2D Gaussians with inadequate rendering quality and applies a novel in-place cloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R model with opacity frozen. Experimental results show that compared to the original 2DGS, our method requires only 1\\% more storage and minimal additional training time. Despite this negligible overhead, it achieves high-quality rendering results while preserving fine geometric structures. These findings indicate that our approach effectively balances efficiency with performance, leading to improvements in both visual fidelity and geometric reconstruction accuracy.",
    "arxiv_url": "https://arxiv.org/abs/2510.16837v1",
    "pdf_url": "https://arxiv.org/pdf/2510.16837v1",
    "published_date": "2025-10-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "high-fidelity",
      "face",
      "ar",
      "3d gaussian",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.16837v1",
      "pdf": "https://arxiv.org/pdf/2510.16837v1"
    },
    "bibtex": ""
  },
  {
    "title": "GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation",
    "authors": [
      "Junbo Li",
      "Weimin Yuan",
      "Yinuo Wang",
      "Yue Zeng",
      "Shihao Shu",
      "Cai Meng",
      "Xiangzhi Bai"
    ],
    "abstract": "Accurate 6D pose estimation of 3D objects is a fundamental task in computer vision, and current research typically predicts the 6D pose by establishing correspondences between 2D image features and 3D model features. However, these methods often face difficulties with textureless objects and varying illumination conditions. To overcome these limitations, we propose GS2POSE, a novel approach for 6D object pose estimation. GS2POSE formulates a pose regression algorithm inspired by the principles of Bundle Adjustment (BA). By leveraging Lie algebra, we extend the capabilities of 3DGS to develop a pose-differentiable rendering pipeline, which iteratively optimizes the pose by comparing the input image to the rendered image. Additionally, GS2POSE updates color parameters within the 3DGS model, enhancing its adaptability to changes in illumination. Compared to previous models, GS2POSE demonstrates accuracy improvements of 1.4\\%, 2.8\\% and 2.5\\% on the T-LESS, LineMod-Occlusion and LineMod datasets, respectively.",
    "arxiv_url": "https://arxiv.org/abs/2510.16777v1",
    "pdf_url": "https://arxiv.org/pdf/2510.16777v1",
    "published_date": "2025-10-19",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "illumination",
      "face",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.16777v1",
      "pdf": "https://arxiv.org/pdf/2510.16777v1"
    },
    "bibtex": ""
  },
  {
    "title": "HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D Avatars",
    "authors": [
      "Haocheng Tang",
      "Ruoke Yan",
      "Xinhui Yin",
      "Qi Zhang",
      "Xinfeng Zhang",
      "Siwei Ma",
      "Wen Gao",
      "Chuanmin Jia"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled fast, photorealistic rendering of dynamic 3D scenes, showing strong potential in immersive communication. However, in digital human encoding and transmission, the compression methods based on general 3DGS representations are limited by the lack of human priors, resulting in suboptimal bitrate efficiency and reconstruction quality at the decoder side, which hinders their application in streamable 3D avatar systems. We propose HGC-Avatar, a novel Hierarchical Gaussian Compression framework designed for efficient transmission and high-quality rendering of dynamic avatars. Our method disentangles the Gaussian representation into a structural layer, which maps poses to Gaussians via a StyleUNet-based generator, and a motion layer, which leverages the SMPL-X model to represent temporal pose variations compactly and semantically. This hierarchical design supports layer-wise compression, progressive decoding, and controllable rendering from diverse pose inputs such as video sequences or text. Since people are most concerned with facial realism, we incorporate a facial attention mechanism during StyleUNet training to preserve identity and expression details under low-bitrate constraints. Experimental results demonstrate that HGC-Avatar provides a streamable solution for rapid 3D avatar rendering, while significantly outperforming prior methods in both visual quality and compression efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2510.16463v1",
    "pdf_url": "https://arxiv.org/pdf/2510.16463v1",
    "published_date": "2025-10-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "dynamic",
      "compression",
      "avatar",
      "gaussian splatting",
      "ar",
      "semantic",
      "human",
      "3d gaussian",
      "efficient",
      "fast",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.16463v1",
      "pdf": "https://arxiv.org/pdf/2510.16463v1"
    },
    "bibtex": ""
  },
  {
    "title": "REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting",
    "authors": [
      "Changyue Shi",
      "Minghao Chen",
      "Yiping Mao",
      "Chuxiao Yang",
      "Xinyuan Hu",
      "Jiajun Ding",
      "Zhou Yu"
    ],
    "abstract": "Bridging the gap between complex human instructions and precise 3D object grounding remains a significant challenge in vision and robotics. Existing 3D segmentation methods often struggle to interpret ambiguous, reasoning-based instructions, while 2D vision-language models that excel at such reasoning lack intrinsic 3D spatial understanding. In this paper, we introduce REALM, an innovative MLLM-agent framework that enables open-world reasoning-based segmentation without requiring extensive 3D-specific post-training. We perform segmentation directly on 3D Gaussian Splatting representations, capitalizing on their ability to render photorealistic novel views that are highly suitable for MLLM comprehension. As directly feeding one or more rendered views to the MLLM can lead to high sensitivity to viewpoint selection, we propose a novel Global-to-Local Spatial Grounding strategy. Specifically, multiple global views are first fed into the MLLM agent in parallel for coarse-level localization, aggregating responses to robustly identify the target object. Then, several close-up novel views of the object are synthesized to perform fine-grained local segmentation, yielding accurate and consistent 3D masks. Extensive experiments show that REALM achieves remarkable performance in interpreting both explicit and implicit instructions across LERF, 3D-OVS, and our newly introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly supports a range of 3D interaction tasks, including object removal, replacement, and style transfer, demonstrating its practical utility and versatility. Project page: https://ChangyueShi.github.io/REALM.",
    "arxiv_url": "https://arxiv.org/abs/2510.16410v2",
    "pdf_url": "https://arxiv.org/pdf/2510.16410v2",
    "published_date": "2025-10-18",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "localization",
      "segmentation",
      "gaussian splatting",
      "ar",
      "human",
      "3d gaussian",
      "understanding"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.16410v2",
      "pdf": "https://arxiv.org/pdf/2510.16410v2",
      "project": "https://ChangyueShi.github.io/REALM"
    },
    "bibtex": ""
  },
  {
    "title": "Proactive Scene Decomposition and Reconstruction",
    "authors": [
      "Baicheng Li",
      "Zike Yan",
      "Dong Wu",
      "Hongbin Zha"
    ],
    "abstract": "Human behaviors are the major causes of scene dynamics and inherently contain rich cues regarding the dynamics. This paper formalizes a new task of proactive scene decomposition and reconstruction, an online approach that leverages human-object interactions to iteratively disassemble and reconstruct the environment. By observing these intentional interactions, we can dynamically refine the decomposition and reconstruction process, addressing inherent ambiguities in static object-level reconstruction. The proposed system effectively integrates multiple tasks in dynamic environments such as accurate camera and object pose estimation, instance decomposition, and online map updating, capitalizing on cues from human-object interactions in egocentric live streams for a flexible, progressive alternative to conventional object-level reconstruction methods. Aided by the Gaussian splatting technique, accurate and consistent dynamic scene modeling is achieved with photorealistic and efficient rendering. The efficacy is validated in multiple real-world scenarios with promising advantages.",
    "arxiv_url": "https://arxiv.org/abs/2510.16272v1",
    "pdf_url": "https://arxiv.org/pdf/2510.16272v1",
    "published_date": "2025-10-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "efficient rendering",
      "human",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.16272v1",
      "pdf": "https://arxiv.org/pdf/2510.16272v1"
    },
    "bibtex": ""
  },
  {
    "title": "Fix False Transparency by Noise Guided Splatting",
    "authors": [
      "Aly El Hakie",
      "Yiren Lu",
      "Yu Yin",
      "Michael Jenkins",
      "Yehe Liu"
    ],
    "abstract": "Opaque objects reconstructed by 3DGS often exhibit a falsely transparent surface, leading to inconsistent background and internal patterns under camera motion in interactive viewing. This issue stems from the ill-posed optimization in 3DGS. During training, background and foreground Gaussians are blended via alpha-compositing and optimized solely against the input RGB images using a photometric loss. As this process lacks an explicit constraint on surface opacity, the optimization may incorrectly assign transparency to opaque regions, resulting in view-inconsistent and falsely transparent. This issue is difficult to detect in standard evaluation settings but becomes particularly evident in object-centric reconstructions under interactive viewing. Although other causes of view-inconsistency have been explored recently, false transparency has not been explicitly identified. To the best of our knowledge, we are the first to identify, characterize, and develop solutions for this artifact, an underreported artifact in 3DGS. Our strategy, NGS, encourages surface Gaussians to adopt higher opacity by injecting opaque noise Gaussians in the object volume during training, requiring only minimal modifications to the existing splatting process. To quantitatively evaluate false transparency in static renderings, we propose a transmittance-based metric that measures the severity of this artifact. In addition, we introduce a customized, high-quality object-centric scan dataset exhibiting pronounced transparency issues, and we augment popular existing datasets with complementary infill noise specifically designed to assess the robustness of 3D reconstruction methods to false transparency. Experiments across multiple datasets show that NGS substantially reduces false transparency while maintaining competitive performance on standard rendering metrics, demonstrating its overall effectiveness.",
    "arxiv_url": "https://arxiv.org/abs/2510.15736v1",
    "pdf_url": "https://arxiv.org/pdf/2510.15736v1",
    "published_date": "2025-10-17",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "face",
      "ar",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.15736v1",
      "pdf": "https://arxiv.org/pdf/2510.15736v1"
    },
    "bibtex": ""
  },
  {
    "title": "The impact of dimensionality on universality of 2D quantum Hall transitions",
    "authors": [
      "Qiwei Wan",
      "Yi Zhang"
    ],
    "abstract": "Regardless of model and platform details, the critical phenomena exhibit universal behaviors that are remarkably consistent across various experiments and theories, resulting in a significant scientific success of condensed matter physics. One widely known and commonly used example is the 2D quantum Hall transition; yet, its universal exponents still somewhat conflict between experiments, theoretical models, and numerical ansatzes. We study critical behaviors of quasi-2D Weyl semimetal systems with a finite thickness $L_z>1$, disorder, and external magnetic field $B_z$. By analyzing the scaling behaviors of the localization lengths and local density of states using recursive methods, we find that the finite thickness yields a deviation from the 2D quantum Hall universality ($L_z=1$ case) and a crossover toward the 3D Gaussian Unitary Ensemble ($L_z\\rightarrow \\infty$ limit), potentially offering another cause of the discrepancy. Our work demonstrates the often-overlooked importance of auxiliary degrees of freedom, such as thickness, and that 3D quantum Hall physics is not merely a trivial finite-thickness extension of its 2D counterpart.",
    "arxiv_url": "https://arxiv.org/abs/2510.15671v1",
    "pdf_url": "https://arxiv.org/pdf/2510.15671v1",
    "published_date": "2025-10-17",
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.dis-nn"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "localization",
      "ar"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.15671v1",
      "pdf": "https://arxiv.org/pdf/2510.15671v1"
    },
    "bibtex": ""
  },
  {
    "title": "PFGS: Pose-Fused 3D Gaussian Splatting for Complete Multi-Pose Object Reconstruction",
    "authors": [
      "Ting-Yu Yen",
      "Yu-Sheng Chiu",
      "Shih-Hsuan Hung",
      "Peter Wonka",
      "Hung-Kuo Chu"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled high-quality, real-time novel-view synthesis from multi-view images. However, most existing methods assume the object is captured in a single, static pose, resulting in incomplete reconstructions that miss occluded or self-occluded regions. We introduce PFGS, a pose-aware 3DGS framework that addresses the practical challenge of reconstructing complete objects from multi-pose image captures. Given images of an object in one main pose and several auxiliary poses, PFGS iteratively fuses each auxiliary set into a unified 3DGS representation of the main pose. Our pose-aware fusion strategy combines global and local registration to merge views effectively and refine the 3DGS model. While recent advances in 3D foundation models have improved registration robustness and efficiency, they remain limited by high memory demands and suboptimal accuracy. PFGS overcomes these challenges by incorporating them more intelligently into the registration process: it leverages background features for per-pose camera pose estimation and employs foundation models for cross-pose registration. This design captures the best of both approaches while resolving background inconsistency issues. Experimental results demonstrate that PFGS consistently outperforms strong baselines in both qualitative and quantitative evaluations, producing more complete reconstructions and higher-fidelity 3DGS models.",
    "arxiv_url": "https://arxiv.org/abs/2510.15386v1",
    "pdf_url": "https://arxiv.org/pdf/2510.15386v1",
    "published_date": "2025-10-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.15386v1",
      "pdf": "https://arxiv.org/pdf/2510.15386v1"
    },
    "bibtex": ""
  },
  {
    "title": "GaussGym: An open-source real-to-sim framework for learning locomotion from pixels",
    "authors": [
      "Alejandro Escontrela",
      "Justin Kerr",
      "Arthur Allshire",
      "Jonas Frey",
      "Rocky Duan",
      "Carmelo Sferrazza",
      "Pieter Abbeel"
    ],
    "abstract": "We present a novel approach for photorealistic robot simulation that integrates 3D Gaussian Splatting as a drop-in renderer within vectorized physics simulators such as IsaacGym. This enables unprecedented speed -- exceeding 100,000 steps per second on consumer GPUs -- while maintaining high visual fidelity, which we showcase across diverse tasks. We additionally demonstrate its applicability in a sim-to-real robotics setting. Beyond depth-based sensing, our results highlight how rich visual semantics improve navigation and decision-making, such as avoiding undesirable regions. We further showcase the ease of incorporating thousands of environments from iPhone scans, large-scale scene datasets (e.g., GrandTour, ARKit), and outputs from generative video models like Veo, enabling rapid creation of realistic training worlds. This work bridges high-throughput simulation and high-fidelity perception, advancing scalable and generalizable robot learning. All code and data will be open-sourced for the community to build upon. Videos, code, and data available at https://escontrela.me/gauss_gym/.",
    "arxiv_url": "https://arxiv.org/abs/2510.15352v1",
    "pdf_url": "https://arxiv.org/pdf/2510.15352v1",
    "published_date": "2025-10-17",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "robotics",
      "high-fidelity",
      "gaussian splatting",
      "ar",
      "semantic",
      "3d gaussian",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.15352v1",
      "pdf": "https://arxiv.org/pdf/2510.15352v1",
      "project": "https://escontrela.me/gauss_gym"
    },
    "bibtex": ""
  },
  {
    "title": "DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion",
    "authors": [
      "Weijie Wang",
      "Jiagang Zhu",
      "Zeyu Zhang",
      "Xiaofeng Wang",
      "Zheng Zhu",
      "Guosheng Zhao",
      "Chaojun Ni",
      "Haoxiao Wang",
      "Guan Huang",
      "Xinze Chen",
      "Yukun Zhou",
      "Wenkang Qin",
      "Duochao Shi",
      "Haoyun Li",
      "Yicheng Xiao",
      "Donny Y. Chen",
      "Jiwen Lu"
    ],
    "abstract": "We present DriveGen3D, a novel framework for generating high-quality and highly controllable dynamic 3D driving scenes that addresses critical limitations in existing methodologies. Current approaches to driving scene synthesis either suffer from prohibitive computational demands for extended temporal generation, focus exclusively on prolonged video synthesis without 3D representation, or restrict themselves to static single-scene reconstruction. Our work bridges this methodological gap by integrating accelerated long-term video generation with large-scale dynamic scene reconstruction through multimodal conditional control. DriveGen3D introduces a unified pipeline consisting of two specialized components: FastDrive-DiT, an efficient video diffusion transformer for high-resolution, temporally coherent video synthesis under text and Bird's-Eye-View (BEV) layout guidance; and FastRecon3D, a feed-forward module that rapidly builds 3D Gaussian representations across time, ensuring spatial-temporal consistency. DriveGen3D enable the generation of long driving videos (up to $800\\times424$ at $12$ FPS) and corresponding 3D scenes, achieving state-of-the-art results while maintaining efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2510.15264v2",
    "pdf_url": "https://arxiv.org/pdf/2510.15264v2",
    "published_date": "2025-10-17",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "ar",
      "3d gaussian",
      "efficient",
      "fast"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.15264v2",
      "pdf": "https://arxiv.org/pdf/2510.15264v2"
    },
    "bibtex": ""
  },
  {
    "title": "SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images",
    "authors": [
      "Jiaxin Guo",
      "Tongfan Guan",
      "Wenzhen Dong",
      "Wenzhao Zheng",
      "Wenting Wang",
      "Yue Wang",
      "Yeung Yam",
      "Yun-Hui Liu"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled generalizable, on-the-fly reconstruction of sequential input views. However, existing methods often predict per-pixel Gaussians and combine Gaussians from all views as the scene representation, leading to substantial redundancies and geometric inconsistencies in long-duration video sequences. To address this, we propose SaLon3R, a novel framework for Structure-aware, Long-term 3DGS Reconstruction. To our best knowledge, SaLon3R is the first online generalizable GS method capable of reconstructing over 50 views in over 10 FPS, with 50% to 90% redundancy removal. Our method introduces compact anchor primitives to eliminate redundancy through differentiable saliency-aware Gaussian quantization, coupled with a 3D Point Transformer that refines anchor attributes and saliency to resolve cross-frame geometric and photometric inconsistencies. Specifically, we first leverage a 3D reconstruction backbone to predict dense per-pixel Gaussians and a saliency map encoding regional geometric complexity. Redundant Gaussians are compressed into compact anchors by prioritizing high-complexity regions. The 3D Point Transformer then learns spatial structural priors in 3D space from training data to refine anchor attributes and saliency, enabling regionally adaptive Gaussian decoding for geometric fidelity. Without known camera parameters or test-time optimization, our approach effectively resolves artifacts and prunes the redundant 3DGS in a single feed-forward pass. Experiments on multiple datasets demonstrate our state-of-the-art performance on both novel view synthesis and depth estimation, demonstrating superior efficiency, robustness, and generalization ability for long-term generalizable 3D reconstruction. Project Page: https://wrld.github.io/SaLon3R/.",
    "arxiv_url": "https://arxiv.org/abs/2510.15072v1",
    "pdf_url": "https://arxiv.org/pdf/2510.15072v1",
    "published_date": "2025-10-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "compact",
      "3d reconstruction",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.15072v1",
      "pdf": "https://arxiv.org/pdf/2510.15072v1",
      "project": "https://wrld.github.io/SaLon3R"
    },
    "bibtex": ""
  },
  {
    "title": "Terra: Explorable Native 3D World Model with Point Latents",
    "authors": [
      "Yuanhui Huang",
      "Weiliang Chen",
      "Wenzhao Zheng",
      "Xin Tao",
      "Pengfei Wan",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "World models have garnered increasing attention for comprehensive modeling of the real world. However, most existing methods still rely on pixel-aligned representations as the basis for world evolution, neglecting the inherent 3D nature of the physical world. This could undermine the 3D consistency and diminish the modeling efficiency of world models. In this paper, we present Terra, a native 3D world model that represents and generates explorable environments in an intrinsic 3D latent space. Specifically, we propose a novel point-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into a latent point representation, which is subsequently decoded as 3D Gaussian primitives to jointly model geometry and appearance. We then introduce a sparse point flow matching network (SPFlow) for generating the latent point representation, which simultaneously denoises the positions and features of the point latents. Our Terra enables exact multi-view consistency with native 3D representation and architecture, and supports flexible rendering from any viewpoint with only a single generation process. Furthermore, Terra achieves explorable world modeling through progressive generation in the point latent space. We conduct extensive experiments on the challenging indoor scenes from ScanNet v2. Terra achieves state-of-the-art performance in both reconstruction and generation with high 3D consistency.",
    "arxiv_url": "https://arxiv.org/abs/2510.14977v1",
    "pdf_url": "https://arxiv.org/pdf/2510.14977v1",
    "published_date": "2025-10-16",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "github_url": "",
    "keywords": [
      "3d gaussian",
      "ar",
      "geometry"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.14977v1",
      "pdf": "https://arxiv.org/pdf/2510.14977v1"
    },
    "bibtex": ""
  },
  {
    "title": "Leveraging Learned Image Prior for 3D Gaussian Compression",
    "authors": [
      "Seungjoo Shin",
      "Jaesik Park",
      "Sunghyun Cho"
    ],
    "abstract": "Compression techniques for 3D Gaussian Splatting (3DGS) have recently achieved considerable success in minimizing storage overhead for 3D Gaussians while preserving high rendering quality. Despite the impressive storage reduction, the lack of learned priors restricts further advances in the rate-distortion trade-off for 3DGS compression tasks. To address this, we introduce a novel 3DGS compression framework that leverages the powerful representational capacity of learned image priors to recover compression-induced quality degradation. Built upon initially compressed Gaussians, our restoration network effectively models the compression artifacts in the image space between degraded and original Gaussians. To enhance the rate-distortion performance, we provide coarse rendering residuals into the restoration network as side information. By leveraging the supervision of restored images, the compressed Gaussians are refined, resulting in a highly compact representation with enhanced rendering performance. Our framework is designed to be compatible with existing Gaussian compression methods, making it broadly applicable across different baselines. Extensive experiments validate the effectiveness of our framework, demonstrating superior rate-distortion performance and outperforming the rendering quality of state-of-the-art 3DGS compression methods while requiring substantially less storage.",
    "arxiv_url": "https://arxiv.org/abs/2510.14705v1",
    "pdf_url": "https://arxiv.org/pdf/2510.14705v1",
    "published_date": "2025-10-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "head",
      "compact",
      "compression",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.14705v1",
      "pdf": "https://arxiv.org/pdf/2510.14705v1"
    },
    "bibtex": ""
  },
  {
    "title": "BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian Splatting Training on GPU",
    "authors": [
      "Junyi Wu",
      "Jiaming Xu",
      "Jinhao Li",
      "Yongkang Zhou",
      "Jiayi Pan",
      "Xingyang Li",
      "Guohao Dai"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction technique. The traditional 3DGS training pipeline follows three sequential steps: Gaussian densification, Gaussian projection, and color splatting. Despite its promising reconstruction quality, this conventional approach suffers from three critical inefficiencies: (1) Skewed density allocation during Gaussian densification, (2) Imbalanced computation workload during Gaussian projection and (3) Fragmented memory access during color splatting.   To tackle the above challenges, we introduce BalanceGS, the algorithm-system co-design for efficient training in 3DGS. (1) At the algorithm level, we propose heuristic workload-sensitive Gaussian density control to automatically balance point distributions - removing 80% redundant Gaussians in dense regions while filling gaps in sparse areas. (2) At the system level, we propose Similarity-based Gaussian sampling and merging, which replaces the static one-to-one thread-pixel mapping with adaptive workload distribution - threads now dynamically process variable numbers of Gaussians based on local cluster density. (3) At the mapping level, we propose reordering-based memory access mapping strategy that restructures RGB storage and enables batch loading in shared memory.   Extensive experiments demonstrate that compared with 3DGS, our approach achieves a 1.44$\\times$ training speedup on a NVIDIA A100 GPU with negligible quality degradation.",
    "arxiv_url": "https://arxiv.org/abs/2510.14564v1",
    "pdf_url": "https://arxiv.org/pdf/2510.14564v1",
    "published_date": "2025-10-16",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "3d reconstruction",
      "ar",
      "mapping",
      "3d gaussian",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.14564v1",
      "pdf": "https://arxiv.org/pdf/2510.14564v1"
    },
    "bibtex": ""
  },
  {
    "title": "GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering",
    "authors": [
      "Alexander Valverde",
      "Brian Xu",
      "Yuyin Zhou",
      "Meng Xu",
      "Hongyun Wang"
    ],
    "abstract": "Scene reconstruction has emerged as a central challenge in computer vision, with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting achieving remarkable progress. While Gaussian Splatting demonstrates strong performance on large-scale datasets, it often struggles to capture fine details or maintain realism in regions with sparse coverage, largely due to the inherent limitations of sparse 3D training data.   In this work, we propose GauSSmart, a hybrid method that effectively bridges 2D foundational models and 3D Gaussian Splatting reconstruction. Our approach integrates established 2D computer vision techniques, including convex filtering and semantic feature supervision from foundational models such as DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D segmentation priors and high-dimensional feature embeddings, our method guides the densification and refinement of Gaussian splats, improving coverage in underrepresented areas and preserving intricate structural details.   We validate our approach across three datasets, where GauSSmart consistently outperforms existing Gaussian Splatting in the majority of evaluated scenes. Our results demonstrate the significant potential of hybrid 2D-3D approaches, highlighting how the thoughtful combination of 2D foundational models with 3D reconstruction pipelines can overcome the limitations inherent in either approach alone.",
    "arxiv_url": "https://arxiv.org/abs/2510.14270v3",
    "pdf_url": "https://arxiv.org/pdf/2510.14270v3",
    "published_date": "2025-10-16",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "segmentation",
      "ar",
      "semantic",
      "nerf",
      "3d gaussian",
      "lighting",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.14270v3",
      "pdf": "https://arxiv.org/pdf/2510.14270v3"
    },
    "bibtex": ""
  },
  {
    "title": "Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures",
    "authors": [
      "Yuancheng Xu",
      "Wenqi Xian",
      "Li Ma",
      "Julien Philip",
      "Ahmet Levent Taşel",
      "Yiwei Zhao",
      "Ryan Burgert",
      "Mingming He",
      "Oliver Hermann",
      "Oliver Pilarski",
      "Rahul Garg",
      "Paul Debevec",
      "Ning Yu"
    ],
    "abstract": "We introduce a framework that enables both multi-view character consistency and 3D camera control in video diffusion models through a novel customization data pipeline. We train the character consistency component with recorded volumetric capture performances re-rendered with diverse camera trajectories via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video relighting model. We fine-tune state-of-the-art open-source video diffusion models on this data to provide strong multi-view identity preservation, precise camera control, and lighting adaptability. Our framework also supports core capabilities for virtual production, including multi-subject generation using two approaches: joint training and noise blending, the latter enabling efficient composition of independently customized models at inference time; it also achieves scene and real-life video customization as well as control over motion and spatial layout during customization. Extensive experiments show improved video quality, higher personalization accuracy, and enhanced camera control and lighting adaptability, advancing the integration of video generation into virtual production. Our project page is available at: https://eyeline-labs.github.io/Virtually-Being.",
    "arxiv_url": "https://arxiv.org/abs/2510.14179v1",
    "pdf_url": "https://arxiv.org/pdf/2510.14179v1",
    "published_date": "2025-10-16",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "github_url": "",
    "keywords": [
      "relighting",
      "4d",
      "gaussian splatting",
      "ar",
      "efficient",
      "lighting",
      "motion"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.14179v1",
      "pdf": "https://arxiv.org/pdf/2510.14179v1",
      "project": "https://eyeline-labs.github.io/Virtually-Being"
    },
    "bibtex": ""
  },
  {
    "title": "InsideOut: Integrated RGB-Radiative Gaussian Splatting for Comprehensive 3D Object Representation",
    "authors": [
      "Jungmin Lee",
      "Seonghyuk Hong",
      "Juyong Lee",
      "Jaeyoon Lee",
      "Jongwon Choi"
    ],
    "abstract": "We introduce InsideOut, an extension of 3D Gaussian splatting (3DGS) that bridges the gap between high-fidelity RGB surface details and subsurface X-ray structures. The fusion of RGB and X-ray imaging is invaluable in fields such as medical diagnostics, cultural heritage restoration, and manufacturing. We collect new paired RGB and X-ray data, perform hierarchical fitting to align RGB and X-ray radiative Gaussian splats, and propose an X-ray reference loss to ensure consistent internal structures. InsideOut effectively addresses the challenges posed by disparate data representations between the two modalities and limited paired datasets. This approach significantly extends the applicability of 3DGS, enhancing visualization, simulation, and non-destructive testing capabilities across various domains.",
    "arxiv_url": "https://arxiv.org/abs/2510.17864v1",
    "pdf_url": "https://arxiv.org/pdf/2510.17864v1",
    "published_date": "2025-10-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "medical",
      "face",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.17864v1",
      "pdf": "https://arxiv.org/pdf/2510.17864v1"
    },
    "bibtex": ""
  },
  {
    "title": "Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images",
    "authors": [
      "Emanuel Garbin",
      "Guy Adam",
      "Oded Krams",
      "Zohar Barzelay",
      "Eran Guendelman",
      "Michael Schwarz",
      "Matteo Presutto",
      "Moran Vatelmacher",
      "Yigal Shenkman",
      "Eli Peker",
      "Itai Druker",
      "Uri Patish",
      "Yoav Blum",
      "Max Bluvstein",
      "Junxuan Li",
      "Rawal Khirodkar",
      "Shunsuke Saito"
    ],
    "abstract": "We present a novel, zero-shot pipeline for creating hyperrealistic, identity-preserving 3D avatars from a few unstructured phone images. Existing methods face several challenges: single-view approaches suffer from geometric inconsistencies and hallucinations, degrading identity preservation, while models trained on synthetic data fail to capture high-frequency details like skin wrinkles and fine hair, limiting realism. Our method introduces two key contributions: (1) a generative canonicalization module that processes multiple unstructured views into a standardized, consistent representation, and (2) a transformer-based model trained on a new, large-scale dataset of high-fidelity Gaussian splatting avatars derived from dome captures of real people. This \"Capture, Canonicalize, Splat\" pipeline produces static quarter-body avatars with compelling realism and robust identity preservation from unstructured photos.",
    "arxiv_url": "https://arxiv.org/abs/2510.14081v3",
    "pdf_url": "https://arxiv.org/pdf/2510.14081v3",
    "published_date": "2025-10-15",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "github_url": "",
    "keywords": [
      "high-fidelity",
      "face",
      "body",
      "avatar",
      "ar",
      "3d gaussian",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.14081v3",
      "pdf": "https://arxiv.org/pdf/2510.14081v3"
    },
    "bibtex": ""
  },
  {
    "title": "Instant Skinned Gaussian Avatars for Web, Mobile and VR Applications",
    "authors": [
      "Naruya Kondo",
      "Yuto Asano",
      "Yoichi Ochiai"
    ],
    "abstract": "We present Instant Skinned Gaussian Avatars, a real-time and cross-platform 3D avatar system. Many approaches have been proposed to animate Gaussian Splatting, but they often require camera arrays, long preprocessing times, or high-end GPUs. Some methods attempt to convert Gaussian Splatting into mesh-based representations, achieving lightweight performance but sacrificing visual fidelity. In contrast, our system efficiently animates Gaussian Splatting by leveraging parallel splat-wise processing to dynamically follow the underlying skinned mesh in real time while preserving high visual fidelity. From smartphone-based 3D scanning to on-device preprocessing, the entire process takes just around five minutes, with the avatar generation step itself completed in only about 30 seconds. Our system enables users to instantly transform their real-world appearance into a 3D avatar, making it ideal for seamless integration with social media and metaverse applications. Website: https://gaussian-vrm.github.io/",
    "arxiv_url": "https://arxiv.org/abs/2510.13978v2",
    "pdf_url": "https://arxiv.org/pdf/2510.13978v2",
    "published_date": "2025-10-15",
    "categories": [
      "cs.CG"
    ],
    "github_url": "",
    "keywords": [
      "dynamic",
      "vr",
      "lightweight",
      "avatar",
      "ar",
      "efficient",
      "gaussian splatting"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.13978v2",
      "pdf": "https://arxiv.org/pdf/2510.13978v2",
      "project": "https://gaussian-vrm.github.io"
    },
    "bibtex": ""
  },
  {
    "title": "FlashWorld: High-quality 3D Scene Generation within Seconds",
    "authors": [
      "Xinyang Li",
      "Tengfei Wang",
      "Zixiao Gu",
      "Shengchuan Zhang",
      "Chunchao Guo",
      "Liujuan Cao"
    ],
    "abstract": "We propose FlashWorld, a generative model that produces 3D scenes from a single image or text prompt in seconds, 10~100$\\times$ faster than previous works while possessing superior rendering quality. Our approach shifts from the conventional multi-view-oriented (MV-oriented) paradigm, which generates multi-view images for subsequent 3D reconstruction, to a 3D-oriented approach where the model directly produces 3D Gaussian representations during multi-view generation. While ensuring 3D consistency, 3D-oriented method typically suffers poor visual quality. FlashWorld includes a dual-mode pre-training phase followed by a cross-mode post-training phase, effectively integrating the strengths of both paradigms. Specifically, leveraging the prior from a video diffusion model, we first pre-train a dual-mode multi-view diffusion model, which jointly supports MV-oriented and 3D-oriented generation modes. To bridge the quality gap in 3D-oriented generation, we further propose a cross-mode post-training distillation by matching distribution from consistent 3D-oriented mode to high-quality MV-oriented mode. This not only enhances visual quality while maintaining 3D consistency, but also reduces the required denoising steps for inference. Also, we propose a strategy to leverage massive single-view images and text prompts during this process to enhance the model's generalization to out-of-distribution inputs. Extensive experiments demonstrate the superiority and efficiency of our method.",
    "arxiv_url": "https://arxiv.org/abs/2510.13678v1",
    "pdf_url": "https://arxiv.org/pdf/2510.13678v1",
    "published_date": "2025-10-15",
    "categories": [
      "cs.CV"
    ],
    "github_url": "",
    "keywords": [
      "3d reconstruction",
      "3d gaussian",
      "ar",
      "fast"
    ],
    "citations": 0,
    "semantic_url": "",
    "links": {
      "arxiv": "https://arxiv.org/abs/2510.13678v1",
      "pdf": "https://arxiv.org/pdf/2510.13678v1"
    },
    "bibtex": ""
  }
]